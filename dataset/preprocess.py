"""
ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ

ëª©ì : scan ê²°ê³¼ë¥¼ ì •ê·œí™”í•˜ì—¬ CSV ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë¥¼ ìƒì„±
ì…ë ¥:
    - df: ìŠ¤ìº” ê²°ê³¼ DataFrame (scan.pyì˜ ì¶œë ¥)
    - cfg: ì„¤ì • ê°ì²´ (manifest_filename, quarantine_dirname ì†ì„± í¬í•¨)
    - artifacts_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
ì¶œë ¥:
    - ì •ê·œí™”ëœ DataFrame
    - artifacts/{manifest_filename} CSV íŒŒì¼ ìƒì„±
ê²€ì¦ í¬ì¸íŠ¸:
    - ê¸°ë³¸ ì»¬ëŸ¼ ì¡´ì¬ ë³´ì¥ ë° TypeError ì²˜ë¦¬
    - ê²½ë¡œ ì •ê·œí™” (paths.norm() ì‚¬ìš©í•˜ì—¬ ì ˆëŒ€ê²½ë¡œ ë³€í™˜)
    - íŒŒì¼ ì¡´ì¬ì„± ì¬ê²€ì¦ ë° ëˆ„ë½ íŒŒì¼ ì œê±°
    - code ì¤‘ë³µ ì œê±° (ë§ˆì§€ë§‰ í•­ëª© ìœ ì§€)
    - CSV ì €ì¥ê³¼ DataFrame í–‰ ìˆ˜ ì¼ì¹˜ í™•ì¸
"""

import os
import pandas as pd
from pathlib import Path
from typing import Union, Dict, Any
import logging

import paths

logger = logging.getLogger(__name__)


def preprocess(
    df: pd.DataFrame,
    cfg: Any,
    *,
    artifacts_dir: Union[str, Path] = "artifacts"
) -> pd.DataFrame:
    """
    ìŠ¤ìº” ê²°ê³¼ DataFrameì„ ì •ê·œí™”í•˜ì—¬ CSV ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
    
    Args:
        df: ìŠ¤ìº” ê²°ê³¼ DataFrame
        cfg: ì„¤ì • ê°ì²´ (manifest_filename, quarantine_dirname ì†ì„± í•„ìš”)
        artifacts_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        
    Returns:
        pd.DataFrame: ì •ê·œí™”ëœ DataFrame
        
    Raises:
        ValueError: í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ëœ ê²½ìš°
    """
    print(f"ğŸ”„ Starting preprocessing with {len(df)} rows...")
    
    # 1) ê¸°ë³¸ ì»¬ëŸ¼ ì¡´ì¬ ë³´ì¥
    required_columns = ["image_path", "label_path", "code", "is_pair"]
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        raise ValueError(f"Missing required columns: {missing_columns}")
    
    print(f"âœ… Required columns verified: {required_columns}")
    
    # ì‘ì—…ìš© DataFrame ë³µì‚¬
    working_df = df.copy()
    summary = {}
    
    # 2) ê²½ë¡œ ì •ê·œí™”
    print("ğŸ“ Normalizing file paths...")
    
    def normalize_path(path_str):
        """ê²½ë¡œ ì •ê·œí™” í—¬í¼ í•¨ìˆ˜"""
        if path_str is None or pd.isna(path_str):
            return None
        try:
            return str(paths.norm(path_str))
        except Exception as e:
            logger.warning(f"Failed to normalize path '{path_str}': {e}")
            return None
    
    working_df["image_path"] = working_df["image_path"].apply(normalize_path)
    working_df["label_path"] = working_df["label_path"].apply(normalize_path)
    
    print(f"ğŸ“ Path normalization completed")
    
    # 3) íŒŒì¼ ì¡´ì¬ ì¬ê²€ì¦
    print("ğŸ” Re-validating file existence...")
    
    initial_count = len(working_df)
    
    # ì´ë¯¸ì§€ íŒŒì¼ ì¡´ì¬ í™•ì¸
    def file_exists(path_str):
        """íŒŒì¼ ì¡´ì¬ í™•ì¸ í—¬í¼ í•¨ìˆ˜"""
        if path_str is None or pd.isna(path_str):
            return False
        return Path(path_str).exists()
    
    # ì´ë¯¸ì§€ íŒŒì¼ ëˆ„ë½ ì œê±° (Noneì´ê±°ë‚˜ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°)
    def should_keep_image_row(row):
        if row["image_path"] is None or pd.isna(row["image_path"]):
            return False
        return Path(row["image_path"]).exists()
    
    def should_keep_label_row(row):
        if row["label_path"] is None or pd.isna(row["label_path"]):
            return False
        return Path(row["label_path"]).exists()
    
    # ì´ë¯¸ì§€ íŒŒì¼ í•„í„°ë§
    before_image_filter = len(working_df)
    working_df = working_df[working_df.apply(should_keep_image_row, axis=1)]
    missing_image_count = before_image_filter - len(working_df)
    
    # ë¼ë²¨ íŒŒì¼ í•„í„°ë§
    before_label_filter = len(working_df)
    working_df = working_df[working_df.apply(should_keep_label_row, axis=1)]
    missing_label_count = before_label_filter - len(working_df)
    
    summary["missing_image"] = missing_image_count
    summary["missing_label"] = missing_label_count
    
    print(f"ğŸ—‘ï¸  Removed {missing_image_count} rows with missing images")
    print(f"ğŸ—‘ï¸  Removed {missing_label_count} rows with missing labels")
    
    # 4) code ì¤‘ë³µ ì œê±°
    print("ğŸ”„ Removing duplicate codes...")
    
    before_dedup_count = len(working_df)
    
    # ë§ˆì§€ë§‰ í•­ëª© ìœ ì§€ (keep='last')
    working_df = working_df.drop_duplicates(subset=["code"], keep="last")
    
    after_dedup_count = len(working_df)
    duplicate_count = before_dedup_count - after_dedup_count
    
    summary["duplicate_code"] = duplicate_count
    
    print(f"ğŸ—‘ï¸  Removed {duplicate_count} duplicate codes")
    
    # ìŠ¤í‚¤ë§ˆ ë³´ì¡´ - ë¹ˆ ê²°ê³¼ë¼ë„ í•„ìˆ˜ ì»¬ëŸ¼ êµ¬ì¡° ìœ ì§€
    required_cols = ["image_path", "label_path", "code", "is_pair"]
    
    # í•„ìˆ˜ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ì¶”ê°€
    for col in required_cols:
        if col not in working_df.columns:
            working_df[col] = pd.Series(dtype="object" if col != "is_pair" else "bool")
    
    # ì»¬ëŸ¼ ìˆœì„œ ê³ ì •
    working_df = working_df[required_cols]
    
    # ë¹ˆ DataFrameì´ë©´ ìŠ¤í‚¤ë§ˆë§Œ ìœ ì§€ (0,4)
    if len(working_df) == 0:
        working_df = pd.DataFrame(columns=required_cols)
        print("ğŸ“‹ Empty result - preserving schema (0,4)")
    else:
        # 5) ì¬í˜„ì„±ì„ ìœ„í•œ ì •ë ¬ (CSV ì €ì¥ ì „ í•„ìˆ˜)
        print("ğŸ”„ Sorting DataFrame for reproducibility...")
        working_df = working_df.sort_values(
            ["code", "image_path", "label_path"]
        ).reset_index(drop=True)
        print(f"âœ… Sorted by code, image_path, label_path")
    
    # 6) artifacts_dir ìƒì„±
    artifacts_path = Path(artifacts_dir)
    artifacts_path.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“ Artifacts directory ensured: {artifacts_path}")
    
    # 7) CSV ì €ì¥ (index=False ê³ ì •)
    manifest_path = artifacts_path / cfg.manifest_filename
    working_df.to_csv(manifest_path, index=False)
    
    print(f"ğŸ’¾ Manifest saved: {manifest_path}")
    print(f"ğŸ“Š Final DataFrame: {len(working_df)} rows")
    
    # 8) ìš”ì•½ ì¶œë ¥
    total_removed = initial_count - len(working_df)
    print(f"ğŸ“ˆ Preprocessing summary:")
    print(f"   Initial rows: {initial_count}")
    print(f"   Final rows: {len(working_df)}")
    print(f"   Total removed: {total_removed}")
    print(f"   Removal breakdown: {summary}")
    
    # ì €ì¥ëœ CSVì™€ DataFrame í–‰ ìˆ˜ ì¼ì¹˜ í™•ì¸
    try:
        saved_df = pd.read_csv(manifest_path)
        if len(saved_df) != len(working_df):
            logger.warning(f"Row count mismatch: DataFrame={len(working_df)}, CSV={len(saved_df)}")
        else:
            print(f"âœ… CSV verification passed: {len(saved_df)} rows")
    except pd.errors.EmptyDataError:
        # ë¹ˆ DataFrameì¸ ê²½ìš° CSVì— í—¤ë”ë§Œ ìˆì„ ìˆ˜ ìˆìŒ
        if len(working_df) == 0:
            print(f"âœ… CSV verification passed: empty DataFrame saved correctly")
        else:
            logger.warning(f"CSV is empty but DataFrame has {len(working_df)} rows")
    
    return working_df


def build_summary_dict(
    initial_count: int,
    final_count: int,
    missing_image: int = 0,
    missing_label: int = 0,
    duplicate_code: int = 0,
    **kwargs
) -> Dict[str, Any]:
    """
    ì „ì²˜ë¦¬ ìš”ì•½ ì •ë³´ ìƒì„±
    
    Args:
        initial_count: ì´ˆê¸° í–‰ ìˆ˜
        final_count: ìµœì¢… í–‰ ìˆ˜
        missing_image: ëˆ„ë½ëœ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜
        missing_label: ëˆ„ë½ëœ ë¼ë²¨ íŒŒì¼ ìˆ˜
        duplicate_code: ì¤‘ë³µ ì½”ë“œ ìˆ˜
        **kwargs: ì¶”ê°€ í†µê³„ ì •ë³´
        
    Returns:
        Dict[str, Any]: ìš”ì•½ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    total_removed = initial_count - final_count
    removal_rate = (total_removed / initial_count * 100) if initial_count > 0 else 0
    
    summary = {
        "initial_count": initial_count,
        "final_count": final_count,
        "total_removed": total_removed,
        "removal_rate_percent": round(removal_rate, 2),
        "breakdown": {
            "missing_image": missing_image,
            "missing_label": missing_label,
            "duplicate_code": duplicate_code
        }
    }
    
    # ì¶”ê°€ ì •ë³´ ë³‘í•©
    summary.update(kwargs)
    
    return summary


def validate_preprocessed_data(df: pd.DataFrame) -> Dict[str, Any]:
    """
    ì „ì²˜ë¦¬ëœ ë°ì´í„° ê²€ì¦
    
    Args:
        df: ì „ì²˜ë¦¬ëœ DataFrame
        
    Returns:
        Dict[str, Any]: ê²€ì¦ ê²°ê³¼
    """
    validation_results = {
        "valid": True,
        "issues": []
    }
    
    # 1. í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
    required_columns = ["image_path", "label_path", "code", "is_pair"]
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        validation_results["valid"] = False
        validation_results["issues"].append(f"Missing columns: {missing_columns}")
        return validation_results  # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ë” ì´ìƒ ê²€ì¦ ë¶ˆê°€
    
    # 2. íŒŒì¼ ê²½ë¡œ ìœ íš¨ì„± í™•ì¸
    if len(df) > 0:
        # Noneì´ ì•„ë‹Œ ê²½ë¡œë“¤ì´ ì‹¤ì œ íŒŒì¼ì„ ê°€ë¦¬í‚¤ëŠ”ì§€ í™•ì¸
        invalid_images = 0
        invalid_labels = 0
        
        for idx, row in df.iterrows():
            if row["image_path"] is not None and not Path(row["image_path"]).exists():
                invalid_images += 1
            if row["label_path"] is not None and not Path(row["label_path"]).exists():
                invalid_labels += 1
        
        if invalid_images > 0:
            validation_results["valid"] = False
            validation_results["issues"].append(f"Invalid image paths: {invalid_images}")
        
        if invalid_labels > 0:
            validation_results["valid"] = False
            validation_results["issues"].append(f"Invalid label paths: {invalid_labels}")
        
        # 3. ì½”ë“œ ì¤‘ë³µ í™•ì¸
        duplicate_codes = df["code"].duplicated().sum()
        if duplicate_codes > 0:
            validation_results["valid"] = False
            validation_results["issues"].append(f"Duplicate codes found: {duplicate_codes}")
    
    return validation_results