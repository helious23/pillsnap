"""
ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ

ëª©ì : scan ê²°ê³¼ë¥¼ ì •ê·œí™”í•˜ì—¬ CSV ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë¥¼ ìƒì„±
ì…ë ¥:
    - df: ìŠ¤ìº” ê²°ê³¼ DataFrame (scan.pyì˜ ì¶œë ¥)
    - cfg: ì„¤ì • ê°ì²´ (manifest_filename, quarantine_dirname ì†ì„± í¬í•¨)
    - artifacts_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
ì¶œë ¥:
    - ì •ê·œí™”ëœ DataFrame
    - artifacts/{manifest_filename} CSV íŒŒì¼ ìƒì„±
ê²€ì¦ í¬ì¸íŠ¸:
    - ê¸°ë³¸ ì»¬ëŸ¼ ì¡´ì¬ ë³´ì¥ ë° TypeError ì²˜ë¦¬
    - ê²½ë¡œ ì •ê·œí™” (paths.norm() ì‚¬ìš©í•˜ì—¬ ì ˆëŒ€ê²½ë¡œ ë³€í™˜)
    - íŒŒì¼ ì¡´ì¬ì„± ì¬ê²€ì¦ ë° ëˆ„ë½ íŒŒì¼ ì œê±°
    - code ì¤‘ë³µ ì œê±° (ë§ˆì§€ë§‰ í•­ëª© ìœ ì§€)
    - CSV ì €ì¥ê³¼ DataFrame í–‰ ìˆ˜ ì¼ì¹˜ í™•ì¸
"""

import json
import pandas as pd
from pathlib import Path
from typing import Union, Dict, Any, Optional
import logging

import paths

logger = logging.getLogger(__name__)


def preprocess(
    df: pd.DataFrame, cfg: Any, *, artifacts_dir: Union[str, Path] = "artifacts"
) -> pd.DataFrame:
    """
    ìŠ¤ìº” ê²°ê³¼ DataFrameì„ ì •ê·œí™”í•˜ì—¬ CSV ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±

    Args:
        df: ìŠ¤ìº” ê²°ê³¼ DataFrame
        cfg: ì„¤ì • ê°ì²´ (manifest_filename, quarantine_dirname ì†ì„± í•„ìš”)
        artifacts_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ

    Returns:
        pd.DataFrame: ì •ê·œí™”ëœ DataFrame

    Raises:
        ValueError: í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ëœ ê²½ìš°
    """
    print(f"ğŸ”„ Starting preprocessing with {len(df)} rows...")

    # 1) ê¸°ë³¸ ì»¬ëŸ¼ ì¡´ì¬ ë³´ì¥
    required_columns = ["image_path", "label_path", "code", "is_pair"]
    missing_columns = [col for col in required_columns if col not in df.columns]

    if missing_columns:
        raise ValueError(f"Missing required columns: {missing_columns}")

    print(f"âœ… Required columns verified: {required_columns}")

    # ì‘ì—…ìš© DataFrame ë³µì‚¬
    working_df = df.copy()
    summary = {}

    # 2) ê²½ë¡œ ì •ê·œí™”
    print("ğŸ“ Normalizing file paths...")

    def normalize_path(path_str):
        """ê²½ë¡œ ì •ê·œí™” í—¬í¼ í•¨ìˆ˜"""
        if path_str is None or pd.isna(path_str):
            return None
        try:
            return str(paths.norm(path_str))
        except Exception as e:
            logger.warning(f"Failed to normalize path '{path_str}': {e}")
            return None

    working_df["image_path"] = working_df["image_path"].apply(normalize_path)
    working_df["label_path"] = working_df["label_path"].apply(normalize_path)

    print("ğŸ“ Path normalization completed")

    # 3) íŒŒì¼ ì¡´ì¬ ì¬ê²€ì¦
    print("ğŸ” Re-validating file existence...")

    initial_count = len(working_df)

    # ì´ë¯¸ì§€ íŒŒì¼ ì¡´ì¬ í™•ì¸
    def file_exists(path_str):
        """íŒŒì¼ ì¡´ì¬ í™•ì¸ í—¬í¼ í•¨ìˆ˜"""
        if path_str is None or pd.isna(path_str):
            return False
        return Path(path_str).exists()

    # ì´ë¯¸ì§€ íŒŒì¼ ëˆ„ë½ ì œê±° (Noneì´ê±°ë‚˜ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°)
    def should_keep_image_row(row):
        if row["image_path"] is None or pd.isna(row["image_path"]):
            return False
        return Path(row["image_path"]).exists()

    def should_keep_label_row(row):
        if row["label_path"] is None or pd.isna(row["label_path"]):
            return False
        return Path(row["label_path"]).exists()

    # ì´ë¯¸ì§€ íŒŒì¼ í•„í„°ë§
    before_image_filter = len(working_df)
    working_df = working_df[working_df.apply(should_keep_image_row, axis=1)]
    missing_image_count = before_image_filter - len(working_df)

    # ë¼ë²¨ íŒŒì¼ í•„í„°ë§
    before_label_filter = len(working_df)
    working_df = working_df[working_df.apply(should_keep_label_row, axis=1)]
    missing_label_count = before_label_filter - len(working_df)

    summary["missing_image"] = missing_image_count
    summary["missing_label"] = missing_label_count

    print(f"ğŸ—‘ï¸  Removed {missing_image_count} rows with missing images")
    print(f"ğŸ—‘ï¸  Removed {missing_label_count} rows with missing labels")

    # 4) code ì¤‘ë³µ ì œê±°
    print("ğŸ”„ Removing duplicate codes...")

    before_dedup_count = len(working_df)

    # ë§ˆì§€ë§‰ í•­ëª© ìœ ì§€ (keep='last')
    working_df = working_df.drop_duplicates(subset=["code"], keep="last")

    after_dedup_count = len(working_df)
    duplicate_count = before_dedup_count - after_dedup_count

    summary["duplicate_code"] = duplicate_count

    print(f"ğŸ—‘ï¸  Removed {duplicate_count} duplicate codes")

    # artifacts_dir ê²½ë¡œ ì¤€ë¹„ (JSON íŒŒì‹± ì „ì— í•„ìš”)
    artifacts_path = Path(artifacts_dir)
    artifacts_path.mkdir(parents=True, exist_ok=True)

    # 5) JSON íŒŒì‹± ë° EDI ì¶”ì¶œ (í’ë¶€í™” ë‹¨ê³„)
    print("ğŸ” Parsing JSON labels for EDI extraction...")

    # ë¹ˆ DataFrameì¸ ê²½ìš° JSON íŒŒì‹± ìŠ¤í‚µ
    if len(working_df) == 0:
        # ë¹ˆ DataFrameì— ì»¬ëŸ¼ë§Œ ì¶”ê°€
        for col in [
            "mapping_code",
            "edi_code",
            "json_ok",
            "drug_N",
            "dl_name",
            "drug_shape",
            "print_front",
            "print_back",
        ]:
            working_df[col] = pd.Series(dtype="object" if col != "json_ok" else "bool")
        print("âœ… JSON parse summary: ok=0, fail=0, unique_edi=0 (empty DataFrame)")
    else:

        def parse_label_json(label_path: Optional[str]) -> Dict[str, Any]:
            """ë¼ë²¨ JSON íŒŒì‹±í•˜ì—¬ EDI ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
            result = {
                "mapping_code": None,
                "edi_code": None,
                "json_ok": False,
                "drug_N": None,
                "dl_name": None,
                "drug_shape": None,
                "print_front": None,
                "print_back": None,
            }

            if label_path is None or pd.isna(label_path):
                return result

            try:
                label_file = Path(label_path)
                if not label_file.exists():
                    return result

                with open(label_file, "r", encoding="utf-8") as f:
                    data = json.load(f)

                # images[0] ì—ì„œ ë°ì´í„° ì¶”ì¶œ
                if "images" in data and len(data["images"]) > 0:
                    img_data = data["images"][0]
                    result["mapping_code"] = img_data.get("dl_mapping_code")
                    result["edi_code"] = img_data.get("di_edi_code")
                    result["drug_N"] = img_data.get("drug_N")
                    result["dl_name"] = img_data.get("dl_name")
                    result["drug_shape"] = img_data.get("drug_shape")
                    result["print_front"] = img_data.get("print_front")
                    result["print_back"] = img_data.get("print_back")
                    result["json_ok"] = True

            except (json.JSONDecodeError, KeyError, IndexError) as e:
                logger.debug(f"JSON parsing failed for {label_path}: {e}")
            except Exception as e:
                logger.warning(f"Unexpected error parsing {label_path}: {e}")

            return result

        # JSON íŒŒì‹± ì ìš©
        json_results = working_df["label_path"].apply(parse_label_json)
        json_df = pd.DataFrame(json_results.tolist())

        # ìƒˆ ì»¬ëŸ¼ë“¤ ì¶”ê°€
        for col in json_df.columns:
            working_df[col] = json_df[col]

        # JSON íŒŒì‹± í†µê³„
        json_ok_count = working_df["json_ok"].sum()
        json_fail_count = len(working_df) - json_ok_count
        unique_edi_count = working_df["edi_code"].dropna().nunique()

        print(
            f"âœ… JSON parse summary: ok={json_ok_count}, fail={json_fail_count}, unique_edi={unique_edi_count}"
        )

        # mapping_codeì™€ code(basename) ì¼ì¹˜ì„± ì²´í¬ (ì„ íƒ)
        if json_ok_count > 0:

            def check_code_consistency(row):
                if row["mapping_code"] and row["code"]:
                    # basenameì´ mapping_codeë¡œ ì‹œì‘í•˜ëŠ”ì§€ ì²´í¬
                    return row["code"].startswith(row["mapping_code"])
                return None

            working_df["code_consistent"] = working_df.apply(
                check_code_consistency, axis=1
            )
            inconsistent_count = (~working_df["code_consistent"]).sum()
            if inconsistent_count > 0:
                logger.warning(
                    f"Found {inconsistent_count} rows where code doesn't match mapping_code prefix"
                )

        # EDI ëˆ„ë½ ê²½ê³ 
        edi_missing_count = working_df["edi_code"].isna().sum()
        if edi_missing_count > 0:
            missing_rate = (
                edi_missing_count / len(working_df) * 100 if len(working_df) > 0 else 0
            )
            if missing_rate > 5:
                print(
                    f"âš ï¸  Warning: High EDI missing rate: {missing_rate:.1f}% ({edi_missing_count}/{len(working_df)})"
                )
                # ëˆ„ë½ ìƒ˜í”Œ ì €ì¥
                missing_edi_df = working_df[working_df["edi_code"].isna()][
                    ["image_path", "label_path", "code", "mapping_code"]
                ]
                if len(missing_edi_df) > 0:
                    missing_edi_path = artifacts_path / "missing_edi_step11.csv"
                    missing_edi_df.head(100).to_csv(missing_edi_path, index=False)
                    print(f"ğŸ“ Saved missing EDI samples to: {missing_edi_path}")

    # ìŠ¤í‚¤ë§ˆ ë³´ì¡´ - ë¹ˆ ê²°ê³¼ë¼ë„ í•„ìˆ˜ ì»¬ëŸ¼ êµ¬ì¡° ìœ ì§€
    required_cols = [
        "image_path",
        "label_path",
        "code",
        "is_pair",
        "mapping_code",
        "edi_code",
        "json_ok",
        "drug_N",
        "dl_name",
        "drug_shape",
        "print_front",
        "print_back",
    ]

    # í•„ìˆ˜ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ì¶”ê°€
    for col in required_cols:
        if col not in working_df.columns:
            working_df[col] = pd.Series(dtype="object" if col != "is_pair" else "bool")

    # ì»¬ëŸ¼ ìˆœì„œ ê³ ì •
    working_df = working_df[required_cols]

    # ë¹ˆ DataFrameì´ë©´ ìŠ¤í‚¤ë§ˆë§Œ ìœ ì§€ (0,4)
    if len(working_df) == 0:
        working_df = pd.DataFrame(columns=required_cols)
        print("ğŸ“‹ Empty result - preserving schema (0,4)")
    else:
        # 5) ì¬í˜„ì„±ì„ ìœ„í•œ ì •ë ¬ (CSV ì €ì¥ ì „ í•„ìˆ˜)
        print("ğŸ”„ Sorting DataFrame for reproducibility...")
        working_df = working_df.sort_values(
            ["code", "image_path", "label_path"]
        ).reset_index(drop=True)
        print("âœ… Sorted by code, image_path, label_path")

    # artifacts_pathëŠ” ì´ë¯¸ ìœ„ì—ì„œ ìƒì„±ë¨
    print(f"ğŸ“ Artifacts directory ensured: {artifacts_path}")

    # 7) CSV ì €ì¥ (index=False ê³ ì •)
    manifest_path = artifacts_path / cfg.manifest_filename
    working_df.to_csv(manifest_path, index=False)

    print(f"ğŸ’¾ Manifest saved: {manifest_path}")
    print(f"ğŸ“Š Final DataFrame: {len(working_df)} rows")

    # 8) ìš”ì•½ ì¶œë ¥
    total_removed = initial_count - len(working_df)
    print("ğŸ“ˆ Preprocessing summary:")
    print(f"   Initial rows: {initial_count}")
    print(f"   Final rows: {len(working_df)}")
    print(f"   Total removed: {total_removed}")
    print(f"   Removal breakdown: {summary}")

    # ì €ì¥ëœ CSVì™€ DataFrame í–‰ ìˆ˜ ì¼ì¹˜ í™•ì¸
    try:
        saved_df = pd.read_csv(manifest_path)
        if len(saved_df) != len(working_df):
            logger.warning(
                f"Row count mismatch: DataFrame={len(working_df)}, CSV={len(saved_df)}"
            )
        else:
            print(f"âœ… CSV verification passed: {len(saved_df)} rows")
    except pd.errors.EmptyDataError:
        # ë¹ˆ DataFrameì¸ ê²½ìš° CSVì— í—¤ë”ë§Œ ìˆì„ ìˆ˜ ìˆìŒ
        if len(working_df) == 0:
            print("âœ… CSV verification passed: empty DataFrame saved correctly")
        else:
            logger.warning(f"CSV is empty but DataFrame has {len(working_df)} rows")

    return working_df


def build_summary_dict(
    initial_count: int,
    final_count: int,
    missing_image: int = 0,
    missing_label: int = 0,
    duplicate_code: int = 0,
    **kwargs,
) -> Dict[str, Any]:
    """
    ì „ì²˜ë¦¬ ìš”ì•½ ì •ë³´ ìƒì„±

    Args:
        initial_count: ì´ˆê¸° í–‰ ìˆ˜
        final_count: ìµœì¢… í–‰ ìˆ˜
        missing_image: ëˆ„ë½ëœ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜
        missing_label: ëˆ„ë½ëœ ë¼ë²¨ íŒŒì¼ ìˆ˜
        duplicate_code: ì¤‘ë³µ ì½”ë“œ ìˆ˜
        **kwargs: ì¶”ê°€ í†µê³„ ì •ë³´

    Returns:
        Dict[str, Any]: ìš”ì•½ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    total_removed = initial_count - final_count
    removal_rate = (total_removed / initial_count * 100) if initial_count > 0 else 0

    summary = {
        "initial_count": initial_count,
        "final_count": final_count,
        "total_removed": total_removed,
        "removal_rate_percent": round(removal_rate, 2),
        "breakdown": {
            "missing_image": missing_image,
            "missing_label": missing_label,
            "duplicate_code": duplicate_code,
        },
    }

    # ì¶”ê°€ ì •ë³´ ë³‘í•©
    summary.update(kwargs)

    return summary


def validate_preprocessed_data(df: pd.DataFrame) -> Dict[str, Any]:
    """
    ì „ì²˜ë¦¬ëœ ë°ì´í„° ê²€ì¦

    Args:
        df: ì „ì²˜ë¦¬ëœ DataFrame

    Returns:
        Dict[str, Any]: ê²€ì¦ ê²°ê³¼
    """
    validation_results = {"valid": True, "issues": []}

    # 1. í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
    required_columns = ["image_path", "label_path", "code", "is_pair"]
    missing_columns = [col for col in required_columns if col not in df.columns]

    if missing_columns:
        validation_results["valid"] = False
        validation_results["issues"].append(f"Missing columns: {missing_columns}")
        return validation_results  # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ë” ì´ìƒ ê²€ì¦ ë¶ˆê°€

    # 2. íŒŒì¼ ê²½ë¡œ ìœ íš¨ì„± í™•ì¸
    if len(df) > 0:
        # Noneì´ ì•„ë‹Œ ê²½ë¡œë“¤ì´ ì‹¤ì œ íŒŒì¼ì„ ê°€ë¦¬í‚¤ëŠ”ì§€ í™•ì¸
        invalid_images = 0
        invalid_labels = 0

        for idx, row in df.iterrows():
            if row["image_path"] is not None and not Path(row["image_path"]).exists():
                invalid_images += 1
            if row["label_path"] is not None and not Path(row["label_path"]).exists():
                invalid_labels += 1

        if invalid_images > 0:
            validation_results["valid"] = False
            validation_results["issues"].append(
                f"Invalid image paths: {invalid_images}"
            )

        if invalid_labels > 0:
            validation_results["valid"] = False
            validation_results["issues"].append(
                f"Invalid label paths: {invalid_labels}"
            )

        # 3. ì½”ë“œ ì¤‘ë³µ í™•ì¸
        duplicate_codes = df["code"].duplicated().sum()
        if duplicate_codes > 0:
            validation_results["valid"] = False
            validation_results["issues"].append(
                f"Duplicate codes found: {duplicate_codes}"
            )

    return validation_results
