#!/usr/bin/env python3
"""
OptimizationAdvisor ì‹¤ì œ ë™ì‘ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import time
import json
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.utils.optimization_advisor import (
    create_rtx5080_advisor,
    OptimizationLevel,
    TrainingMetrics,
    quick_performance_check
)


def test_optimization_advisor():
    """OptimizationAdvisor ì‹¤ì œ ë™ì‘ í…ŒìŠ¤íŠ¸"""
    print("=" * 70)
    print("RTX 5080 OptimizationAdvisor ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # RTX 5080 ìµœì í™” Advisor ìƒì„±
    advisor = create_rtx5080_advisor(OptimizationLevel.BALANCED)
    
    # í•˜ë“œì›¨ì–´ í”„ë¡œíŒŒì¼ ì¶œë ¥
    hw = advisor.hardware_profile
    print(f"ğŸ–¥ï¸  í•˜ë“œì›¨ì–´ í”„ë¡œíŒŒì¼:")
    print(f"   GPU: {hw.gpu_name} ({hw.gpu_memory_gb:.1f}GB)")
    print(f"   CPU: {hw.cpu_cores} ì½”ì–´")
    print(f"   ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {hw.system_memory_gb:.1f}GB")
    print(f"   ìŠ¤í† ë¦¬ì§€: {hw.storage_type.upper()}")
    print(f"   CUDA: {hw.cuda_version}, PyTorch: {hw.pytorch_version}")
    print()
    
    # Stageë³„ ê¶Œê³ ì‚¬í•­ í™•ì¸
    print("ğŸ“‹ Stageë³„ ê¸°ë³¸ ê¶Œê³ ì‚¬í•­:")
    for stage in ["stage_1", "stage_2", "stage_3", "stage_4"]:
        recommendations = advisor.get_stage_recommendations(stage)
        print(f"   {stage.upper()}:")
        print(f"     ë°°ì¹˜ í¬ê¸°: {recommendations['max_batch_size']}")
        print(f"     í•™ìŠµë¥ : {recommendations['base_learning_rate']}")
        print(f"     ë©”ëª¨ë¦¬ ëª©í‘œ: {recommendations['memory_target']*100:.0f}%")
        print(f"     ì›Œì»¤ ìˆ˜: {recommendations['dataloader_workers']}")
    print()
    
    # ì‹œë®¬ë ˆì´ì…˜ëœ í›ˆë ¨ ì‹œë‚˜ë¦¬ì˜¤ë“¤
    scenarios = [
        {
            "name": "ğŸš¨ ë©”ëª¨ë¦¬ ë¶€ì¡± ìƒí™©",
            "metrics": TrainingMetrics(
                batch_size=64,
                learning_rate=1e-3,
                epoch_time_seconds=600,
                samples_per_second=25,
                gpu_utilization=95.0,
                gpu_memory_usage_gb=15.8,  # ê±°ì˜ í•œê³„
                cpu_utilization=80.0,
                io_wait_percent=15.0,
                validation_accuracy=0.72,
                training_loss=0.35
            )
        },
        {
            "name": "ğŸŒ I/O ë³‘ëª© ìƒí™©",
            "metrics": TrainingMetrics(
                batch_size=32,
                learning_rate=5e-4,
                epoch_time_seconds=450,
                samples_per_second=35,
                gpu_utilization=65.0,
                gpu_memory_usage_gb=10.0,
                cpu_utilization=40.0,
                io_wait_percent=30.0,  # I/O ëŒ€ê¸° ë†’ìŒ
                validation_accuracy=0.78,
                training_loss=0.28
            )
        },
        {
            "name": "âš¡ GPU ì—°ì‚° ë³‘ëª© ìƒí™©",
            "metrics": TrainingMetrics(
                batch_size=16,
                learning_rate=3e-4,
                epoch_time_seconds=350,
                samples_per_second=55,
                gpu_utilization=98.0,  # GPU í¬í™”
                gpu_memory_usage_gb=12.0,
                cpu_utilization=45.0,
                io_wait_percent=5.0,
                validation_accuracy=0.85,
                training_loss=0.22
            )
        },
        {
            "name": "âœ… ìµœì í™”ëœ ìƒí™©",
            "metrics": TrainingMetrics(
                batch_size=24,
                learning_rate=2e-4,
                epoch_time_seconds=280,
                samples_per_second=85,
                gpu_utilization=88.0,
                gpu_memory_usage_gb=13.6,  # 85% ì‚¬ìš©
                cpu_utilization=50.0,
                io_wait_percent=5.0,
                validation_accuracy=0.92,
                training_loss=0.15
            )
        }
    ]
    
    print("ğŸ” ë‹¤ì–‘í•œ í›ˆë ¨ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„:")
    print()
    
    for i, scenario in enumerate(scenarios, 1):
        print(f"{scenario['name']} (ì‹œë‚˜ë¦¬ì˜¤ {i})")
        print("-" * 50)
        
        # ì„±ëŠ¥ ë¶„ì„
        report = advisor.analyze_current_performance(scenario['metrics'])
        
        # í˜„ì¬ ìƒíƒœ ì¶œë ¥
        metrics = report.current_metrics
        print(f"í˜„ì¬ ì„±ëŠ¥:")
        print(f"  ë°°ì¹˜ í¬ê¸°: {metrics.batch_size}")
        print(f"  ì²˜ë¦¬ëŸ‰: {metrics.samples_per_second:.1f} samples/sec")
        print(f"  GPU ì‚¬ìš©ë¥ : {metrics.gpu_utilization:.1f}%")
        print(f"  GPU ë©”ëª¨ë¦¬: {metrics.gpu_memory_usage_gb:.1f}GB / {hw.gpu_memory_gb:.1f}GB")
        print(f"  ì •í™•ë„: {metrics.validation_accuracy:.2%}")
        print()
        
        # ë¶„ì„ ê²°ê³¼ ì¶œë ¥
        print(f"ë¶„ì„ ê²°ê³¼:")
        print(f"  ìµœì í™” ì ìˆ˜: {report.overall_score:.2f}/1.0")
        print(f"  ë³‘ëª© ì§€ì : {report.bottleneck_type.value}")
        print(f"  ë³‘ëª© ì‹¬ê°ë„: {report.bottleneck_severity:.2f}")
        print(f"  ì˜ˆìƒ ì†ë„ í–¥ìƒ: {report.estimated_speedup:.1f}x")
        print()
        
        # ì£¼ìš” ê¶Œê³ ì‚¬í•­ ì¶œë ¥
        if report.recommendations:
            print(f"ğŸ¯ ì£¼ìš” ê¶Œê³ ì‚¬í•­ ({len(report.recommendations)}ê°œ):")
            for rec in report.recommendations[:3]:  # ìƒìœ„ 3ê°œë§Œ
                print(f"  {rec.implementation_priority}. {rec.category}")
                print(f"     í˜„ì¬: {rec.current_value} â†’ ê¶Œê³ : {rec.recommended_value}")
                print(f"     ì´ìœ : {rec.reasoning}")
                print(f"     ì˜ˆìƒ ê°œì„ : {rec.expected_improvement:.1f}% (ì‹ ë¢°ë„: {rec.confidence:.0%})")
                print()
        else:
            print("âœ… ì¶”ê°€ ìµœì í™” í•„ìš” ì—†ìŒ")
            print()
        
        print("=" * 70)
        print()
    
    # ì „ì²´ ìš”ì•½
    print("ğŸ“Š ìµœì¢… ìµœì í™” ìš”ì•½:")
    summary = advisor.get_optimization_summary()
    
    if summary.get("status") != "no_data":
        print(f"  í˜„ì¬ ì ìˆ˜: {summary['current_score']:.2f}/1.0")
        print(f"  ì£¼ìš” ë³‘ëª©: {summary['bottleneck']}")
        print(f"  ì´ ê¶Œê³ ì‚¬í•­: {summary['total_recommendations']}ê°œ")
        print(f"  ìš°ì„ ìˆœìœ„ ë†’ì€ ê¶Œê³ : {summary['high_priority_recommendations']}ê°œ")
        print(f"  ì˜ˆìƒ ì†ë„ í–¥ìƒ: {summary['estimated_speedup']:.1f}x")
    print()
    
    # ë¹ ë¥¸ ì„±ëŠ¥ ì²´í¬ ë°ëª¨
    print("âš¡ ë¹ ë¥¸ ì„±ëŠ¥ ì²´í¬ ë°ëª¨:")
    quick_report = quick_performance_check(
        batch_size=32,
        learning_rate=1e-3,
        gpu_memory_gb=14.0,
        samples_per_second=60.0
    )
    
    print(f"  ìµœì í™” ì ìˆ˜: {quick_report.overall_score:.2f}")
    print(f"  ë³‘ëª©: {quick_report.bottleneck_type.value}")
    print(f"  ê¶Œê³ ì‚¬í•­: {len(quick_report.recommendations)}ê°œ")
    print()
    
    # ë³´ê³ ì„œ ì €ì¥ ë°ëª¨
    report_path = "/tmp/optimization_report.json"
    advisor.export_report(quick_report, report_path)
    print(f"ğŸ“„ ë³´ê³ ì„œ ì €ì¥ë¨: {report_path}")
    
    # ì €ì¥ëœ ë³´ê³ ì„œ í¬ê¸° í™•ì¸
    file_size = Path(report_path).stat().st_size / 1024
    print(f"   íŒŒì¼ í¬ê¸°: {file_size:.1f} KB")
    print()
    
    print("âœ… OptimizationAdvisor í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("   ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì •í™•í•œ ë¶„ì„ê³¼ ê¶Œê³ ì‚¬í•­ì„ ì œê³µí•©ë‹ˆë‹¤.")
    print("   RTX 5080 í™˜ê²½ì— ìµœì í™”ëœ ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    test_optimization_advisor()