#!/usr/bin/env python3
"""
í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ìœ í‹¸ë¦¬í‹°
- í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„ ë° ê°€ì¤‘ì¹˜ ê³„ì‚°
- Inverse frequency, Effective number ë“± ë‹¤ì–‘í•œ ë°©ë²• ì§€ì›
"""

import argparse
import json
from typing import Dict, List

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

from base import DataQualityBase


class ClassWeightCalculator(DataQualityBase):
    """í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
    
    def __init__(self, args):
        super().__init__(args)
        self.method = args.method
        self.beta = args.beta
        self.normalize = args.normalize
        self.clip_max = args.clip_max
        
    def analyze_class_distribution(self, df: pd.DataFrame) -> Dict:
        """í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„"""
        class_counts = df['mapping_code'].value_counts()
        
        # ê¸°ë³¸ í†µê³„
        stats_dict = {
            "num_classes": len(class_counts),
            "total_samples": len(df),
            "mean": class_counts.mean(),
            "std": class_counts.std(),
            "median": class_counts.median(),
            "min": class_counts.min(),
            "max": class_counts.max(),
            "q1": class_counts.quantile(0.25),
            "q3": class_counts.quantile(0.75)
        }
        
        # ë¶ˆê· í˜• ì§€í‘œ
        # Gini ê³„ìˆ˜ ê³„ì‚°
        sorted_counts = sorted(class_counts.values)
        n = len(sorted_counts)
        index = np.arange(1, n + 1)
        gini = (2 * np.sum(index * sorted_counts)) / (n * np.sum(sorted_counts)) - (n + 1) / n
        
        # Imbalance ratio
        imbalance_ratio = class_counts.max() / class_counts.min()
        
        stats_dict.update({
            "gini_coefficient": gini,
            "imbalance_ratio": imbalance_ratio,
            "cv": class_counts.std() / class_counts.mean()  # Coefficient of variation
        })
        
        return stats_dict, class_counts
    
    def calculate_inverse_frequency_weights(self, class_counts: pd.Series) -> Dict[str, float]:
        """Inverse frequency ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        total_samples = class_counts.sum()
        num_classes = len(class_counts)
        
        weights = {}
        for class_code, count in class_counts.items():
            # ê¸°ë³¸ inverse frequency
            weight = total_samples / (num_classes * count)
            weights[class_code] = weight
            
        return weights
    
    def calculate_effective_number_weights(self, class_counts: pd.Series) -> Dict[str, float]:
        """Effective number ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        weights = {}
        
        for class_code, count in class_counts.items():
            # Effective number: (1 - Î²^n) / (1 - Î²)
            if self.beta == 1:
                effective_num = count
            else:
                effective_num = (1 - self.beta ** count) / (1 - self.beta)
            
            # WeightëŠ” effective numberì˜ ì—­ìˆ˜ì— ë¹„ë¡€
            weight = 1.0 / effective_num
            weights[class_code] = weight
            
        return weights
    
    def calculate_balanced_weights(self, class_counts: pd.Series) -> Dict[str, float]:
        """Scikit-learn style balanced weights"""
        total_samples = class_counts.sum()
        num_classes = len(class_counts)
        
        weights = {}
        for class_code, count in class_counts.items():
            weight = total_samples / (num_classes * count)
            weights[class_code] = weight
            
        return weights
    
    def calculate_sqrt_weights(self, class_counts: pd.Series) -> Dict[str, float]:
        """Square root ê¸°ë°˜ ê°€ì¤‘ì¹˜ (moderate balancing)"""
        weights = {}
        max_count = class_counts.max()
        
        for class_code, count in class_counts.items():
            # sqrt ì—­ìˆ˜ë¡œ moderateí•˜ê²Œ ì¡°ì •
            weight = np.sqrt(max_count / count)
            weights[class_code] = weight
            
        return weights
    
    def normalize_weights(self, weights: Dict[str, float]) -> Dict[str, float]:
        """ê°€ì¤‘ì¹˜ ì •ê·œí™”"""
        if not self.normalize:
            return weights
            
        values = list(weights.values())
        
        if self.normalize == "mean":
            # í‰ê· ì´ 1ì´ ë˜ë„ë¡
            mean_weight = np.mean(values)
            return {k: v / mean_weight for k, v in weights.items()}
            
        elif self.normalize == "max":
            # ìµœëŒ€ê°’ì´ 1ì´ ë˜ë„ë¡
            max_weight = max(values)
            return {k: v / max_weight for k, v in weights.items()}
            
        elif self.normalize == "sum":
            # í•©ì´ í´ë˜ìŠ¤ ìˆ˜ê°€ ë˜ë„ë¡
            sum_weight = sum(values)
            num_classes = len(weights)
            return {k: v * num_classes / sum_weight for k, v in weights.items()}
            
        return weights
    
    def clip_weights(self, weights: Dict[str, float]) -> Dict[str, float]:
        """ê·¹ë‹¨ê°’ í´ë¦¬í•‘"""
        if not self.clip_max:
            return weights
            
        return {k: min(v, self.clip_max) for k, v in weights.items()}
    
    def plot_weight_distribution(self, class_counts: pd.Series, weights: Dict[str, float], save_path: str = None):
        """ê°€ì¤‘ì¹˜ ë¶„í¬ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. í´ë˜ìŠ¤ ë¶„í¬
        ax = axes[0, 0]
        counts_sorted = class_counts.sort_values(ascending=False)
        ax.bar(range(len(counts_sorted)), counts_sorted.values)
        ax.set_xlabel('Class Rank')
        ax.set_ylabel('Sample Count')
        ax.set_title('Class Distribution (sorted)')
        ax.set_yscale('log')
        
        # 2. ê°€ì¤‘ì¹˜ ë¶„í¬
        ax = axes[0, 1]
        weight_values = [weights[c] for c in counts_sorted.index]
        ax.bar(range(len(weight_values)), weight_values)
        ax.set_xlabel('Class Rank (by count)')
        ax.set_ylabel('Weight')
        ax.set_title('Weight Distribution')
        
        # 3. Count vs Weight scatter
        ax = axes[1, 0]
        counts_list = []
        weights_list = []
        for class_code in class_counts.index:
            counts_list.append(class_counts[class_code])
            weights_list.append(weights[class_code])
        ax.scatter(counts_list, weights_list, alpha=0.5)
        ax.set_xlabel('Sample Count')
        ax.set_ylabel('Weight')
        ax.set_title('Count vs Weight Relationship')
        ax.set_xscale('log')
        ax.set_yscale('log')
        
        # 4. Weight histogram
        ax = axes[1, 1]
        ax.hist(list(weights.values()), bins=50, edgecolor='black')
        ax.set_xlabel('Weight Value')
        ax.set_ylabel('Number of Classes')
        ax.set_title('Weight Value Distribution')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=100, bbox_inches='tight')
            self.logger.info(f"Saved weight distribution plot: {save_path}")
        
        plt.close()
    
    def validate_weights(self, weights: Dict[str, float], class_counts: pd.Series) -> Dict:
        """ê°€ì¤‘ì¹˜ ê²€ì¦"""
        weight_values = list(weights.values())
        
        validation = {
            "num_weights": len(weights),
            "min_weight": min(weight_values),
            "max_weight": max(weight_values),
            "mean_weight": np.mean(weight_values),
            "std_weight": np.std(weight_values),
            "weight_range": max(weight_values) / min(weight_values)
        }
        
        # Effective sample size ê³„ì‚°
        effective_samples = {}
        for class_code, count in class_counts.items():
            if class_code in weights:
                effective_samples[class_code] = count * weights[class_code]
        
        eff_values = list(effective_samples.values())
        validation.update({
            "effective_min": min(eff_values),
            "effective_max": max(eff_values),
            "effective_std": np.std(eff_values),
            "effective_cv": np.std(eff_values) / np.mean(eff_values)
        })
        
        return validation
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰"""
        self.console.print("[bold cyan]âš–ï¸ Class Weight Calculator[/bold cyan]")
        self.console.print(f"Method: {self.method}")
        self.console.print(f"Beta: {self.beta}")
        self.console.print(f"Normalization: {self.normalize}")
        
        # Manifest ë¡œë“œ
        train_df, val_df = self.load_manifests()
        
        # í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„
        train_stats, train_counts = self.analyze_class_distribution(train_df)
        val_stats, val_counts = self.analyze_class_distribution(val_df)
        
        # ë¶„í¬ í†µê³„ í‘œì‹œ
        self.print_summary_table("Train Class Distribution", {
            f"Classes": train_stats['num_classes'],
            f"Samples": train_stats['total_samples'],
            f"MeanÂ±Std": f"{train_stats['mean']:.1f}Â±{train_stats['std']:.1f}",
            f"Min/Max": f"{train_stats['min']}/{train_stats['max']}",
            f"Gini": f"{train_stats['gini_coefficient']:.3f}",
            f"Imbalance": f"{train_stats['imbalance_ratio']:.1f}x"
        })
        
        # ìƒìœ„/í•˜ìœ„ í´ë˜ìŠ¤ í‘œì‹œ
        self.console.print("\n[yellow]Top 5 Most Frequent Classes:[/yellow]")
        for class_code, count in train_counts.head(5).items():
            self.console.print(f"  {class_code}: {count:,} samples")
            
        self.console.print("\n[yellow]Bottom 5 Least Frequent Classes:[/yellow]")
        for class_code, count in train_counts.tail(5).items():
            self.console.print(f"  {class_code}: {count:,} samples")
        
        # ê°€ì¤‘ì¹˜ ê³„ì‚°
        if self.method == "inverse":
            weights = self.calculate_inverse_frequency_weights(train_counts)
        elif self.method == "effective":
            weights = self.calculate_effective_number_weights(train_counts)
        elif self.method == "balanced":
            weights = self.calculate_balanced_weights(train_counts)
        elif self.method == "sqrt":
            weights = self.calculate_sqrt_weights(train_counts)
        else:
            raise ValueError(f"Unknown method: {self.method}")
        
        # ì •ê·œí™” ë° í´ë¦¬í•‘
        weights = self.normalize_weights(weights)
        weights = self.clip_weights(weights)
        
        # ê°€ì¤‘ì¹˜ ê²€ì¦
        validation = self.validate_weights(weights, train_counts)
        
        self.print_summary_table("Weight Statistics", {
            "Min weight": f"{validation['min_weight']:.4f}",
            "Max weight": f"{validation['max_weight']:.4f}",
            "Mean weight": f"{validation['mean_weight']:.4f}",
            "Weight range": f"{validation['weight_range']:.1f}x",
            "Effective CV": f"{validation['effective_cv']:.3f}"
        })
        
        # ê°€ì¤‘ì¹˜ íŒŒì¼ ì €ì¥
        weight_file = self.report_dir / f"class_weights_{self.method}_{self.timestamp}.json"
        
        if not self.args.dry_run:
            # JSONìœ¼ë¡œ ì €ì¥
            with open(weight_file, 'w') as f:
                json.dump(weights, f, indent=2)
            self.logger.info(f"Saved weights to: {weight_file}")
            
            # NumPy ë°°ì—´ë¡œë„ ì €ì¥ (í•™ìŠµ ì½”ë“œìš©)
            weight_array_file = weight_file.with_suffix('.npy')
            # í´ë˜ìŠ¤ ìˆœì„œëŒ€ë¡œ ì •ë ¬
            sorted_classes = sorted(weights.keys())
            weight_array = np.array([weights[c] for c in sorted_classes])
            np.save(weight_array_file, weight_array)
            
            # í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ë§¤í•‘ ì €ì¥
            class_to_idx = {c: i for i, c in enumerate(sorted_classes)}
            mapping_file = weight_file.with_suffix('.mapping.json')
            with open(mapping_file, 'w') as f:
                json.dump(class_to_idx, f, indent=2)
        
        # ì‹œê°í™”
        plot_path = self.report_dir / f"weight_distribution_{self.timestamp}.png"
        self.plot_weight_distribution(train_counts, weights, plot_path if not self.args.dry_run else None)
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_data = {
            "method": self.method,
            "parameters": {
                "beta": self.beta,
                "normalize": self.normalize,
                "clip_max": self.clip_max
            },
            "distribution_stats": {
                "train": train_stats,
                "val": val_stats
            },
            "weight_stats": validation,
            "weight_file": str(weight_file),
            "top_weights": {k: weights[k] for k in list(train_counts.tail(10).index)},  # í¬ê·€ í´ë˜ìŠ¤ë“¤
            "bottom_weights": {k: weights[k] for k in list(train_counts.head(10).index)}  # ë¹ˆë²ˆí•œ í´ë˜ìŠ¤ë“¤
        }
        self.save_report(report_data, "class_weight_calculation")
        
        # ì‚¬ìš© ê°€ì´ë“œ ì¶œë ¥
        self.console.print("\n[green]ğŸ“ Usage Guide:[/green]")
        self.console.print("```python")
        self.console.print("# In your training script:")
        self.console.print(f"weights = json.load(open('{weight_file}'))")
        self.console.print("# Or for array:")
        self.console.print(f"weight_array = np.load('{weight_file.with_suffix('.npy')}')")
        self.console.print("criterion = nn.CrossEntropyLoss(weight=torch.tensor(weight_array))")
        self.console.print("```")
        
        # ìµœì¢… ê²°ë¡ 
        if validation['effective_cv'] < 0.5:
            self.print_conclusion("PASS", 
                f"âœ… Weights successfully calculated!\n"
                f"Effective CV reduced to {validation['effective_cv']:.3f}")
        else:
            self.print_conclusion("WARNING",
                f"âš ï¸ Weights calculated but imbalance still high\n"
                f"Effective CV: {validation['effective_cv']:.3f}")
        
        return 0


def main():
    parser = argparse.ArgumentParser(
        description="Calculate class weights for imbalanced dataset",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    DataQualityBase.add_common_arguments(parser)
    
    parser.add_argument(
        '--method',
        type=str,
        choices=['inverse', 'effective', 'balanced', 'sqrt'],
        default='balanced',
        help='Weight calculation method (default: balanced)'
    )
    
    parser.add_argument(
        '--beta',
        type=float,
        default=0.999,
        help='Beta parameter for effective number method (default: 0.999)'
    )
    
    parser.add_argument(
        '--normalize',
        type=str,
        choices=['none', 'mean', 'max', 'sum'],
        default='mean',
        help='Weight normalization method (default: mean)'
    )
    
    parser.add_argument(
        '--clip-max',
        type=float,
        default=10.0,
        help='Maximum weight value for clipping (default: 10.0)'
    )
    
    # ì‚¬ìš© ì˜ˆì‹œ
    parser.epilog = """
Examples:
  # Calculate balanced weights (default)
  python calculate_class_weights.py
  
  # Use effective number method
  python calculate_class_weights.py --method effective --beta 0.999
  
  # Inverse frequency with max normalization
  python calculate_class_weights.py --method inverse --normalize max
  
  # Moderate balancing with sqrt
  python calculate_class_weights.py --method sqrt --clip-max 5.0
"""
    
    args = parser.parse_args()
    calculator = ClassWeightCalculator(args)
    return calculator.run()


if __name__ == "__main__":
    exit(main())