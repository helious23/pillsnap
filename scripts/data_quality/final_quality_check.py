#!/usr/bin/env python3
"""
ìµœì¢… í’ˆì§ˆ ê²€ì¦ ìœ í‹¸ë¦¬í‹°
- ëª¨ë“  ë°ì´í„° í’ˆì§ˆ ì§€í‘œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê²€ì¦
- PASS/FAIL íŒì • ë° ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
"""

import argparse
import json
from pathlib import Path
from typing import Dict, List, Tuple

import pandas as pd
import numpy as np

from base import DataQualityBase


class FinalQualityChecker(DataQualityBase):
    """ìµœì¢… í’ˆì§ˆ ê²€ì¦"""
    
    def __init__(self, args):
        super().__init__(args)
        self.checks_passed = []
        self.checks_failed = []
        self.warnings = []
        
    def check_class_consistency(self, train_df: pd.DataFrame, val_df: pd.DataFrame) -> Tuple[bool, Dict]:
        """Train/Val í´ë˜ìŠ¤ ì¼ì¹˜ì„± ê²€ì‚¬"""
        train_classes = set(train_df['mapping_code'].unique())
        val_classes = set(val_df['mapping_code'].unique())
        
        val_only = val_classes - train_classes
        train_only = train_classes - val_classes
        
        result = {
            "train_classes": len(train_classes),
            "val_classes": len(val_classes),
            "common_classes": len(train_classes & val_classes),
            "val_only_classes": list(val_only),
            "train_only_classes": list(train_only)[:10]  # ì²˜ìŒ 10ê°œë§Œ
        }
        
        passed = len(val_only) == 0
        
        if passed:
            self.checks_passed.append("Class Consistency")
        else:
            self.checks_failed.append(f"Class Consistency ({len(val_only)} val-only classes)")
            
        return passed, result
    
    def check_combination_ratio(self, train_df: pd.DataFrame, val_df: pd.DataFrame, 
                               target_min: float = 0.15, target_max: float = 0.35) -> Tuple[bool, Dict]:
        """Combination ë¹„ìœ¨ ê²€ì‚¬"""
        train_combo_ratio = (train_df['image_type'] == 'combination').mean()
        val_combo_ratio = (val_df['image_type'] == 'combination').mean()
        
        result = {
            "train_combo_ratio": train_combo_ratio,
            "val_combo_ratio": val_combo_ratio,
            "target_range": f"{target_min:.0%}-{target_max:.0%}"
        }
        
        train_in_range = target_min <= train_combo_ratio <= target_max
        val_in_range = target_min <= val_combo_ratio <= target_max
        
        passed = train_in_range and val_in_range
        
        if passed:
            self.checks_passed.append("Combination Ratio")
        elif train_combo_ratio < 0.10 or val_combo_ratio < 0.10:
            self.checks_failed.append(f"Combination Ratio (Train: {train_combo_ratio:.1%}, Val: {val_combo_ratio:.1%})")
        else:
            self.warnings.append(f"Combination Ratio suboptimal (Train: {train_combo_ratio:.1%}, Val: {val_combo_ratio:.1%})")
            
        return passed, result
    
    def check_corrupted_files(self, train_df: pd.DataFrame, val_df: pd.DataFrame) -> Tuple[bool, Dict]:
        """ì†ìƒ íŒŒì¼ ê²€ì‚¬"""
        blacklist_path = self.report_dir / "blacklist.txt"
        
        if blacklist_path.exists():
            with open(blacklist_path, 'r') as f:
                blacklist = set(line.strip() for line in f if line.strip())
        else:
            blacklist = set()
        
        # K-001900 ê´€ë ¨ íŒŒì¼ ì²´í¬ (ì•Œë ¤ì§„ ë¬¸ì œ)
        k001900_count = sum(1 for path in blacklist if "K-001900" in path)
        
        result = {
            "blacklisted_files": len(blacklist),
            "k001900_affected": k001900_count,
            "blacklist_exists": blacklist_path.exists()
        }
        
        # í˜„ì¬ manifestì— ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        all_paths = set(train_df['image_path'].tolist() + val_df['image_path'].tolist())
        remaining_corrupted = len(all_paths & blacklist)
        
        result['remaining_in_manifest'] = remaining_corrupted
        
        passed = remaining_corrupted == 0
        
        if passed:
            self.checks_passed.append("No Corrupted Files")
        else:
            self.checks_failed.append(f"Corrupted Files ({remaining_corrupted} still in manifest)")
            
        return passed, result
    
    def check_class_balance(self, train_df: pd.DataFrame) -> Tuple[bool, Dict]:
        """í´ë˜ìŠ¤ ê· í˜• ê²€ì‚¬"""
        class_counts = train_df['mapping_code'].value_counts()
        
        # Gini ê³„ìˆ˜ ê³„ì‚°
        sorted_counts = sorted(class_counts.values)
        n = len(sorted_counts)
        index = np.arange(1, n + 1)
        gini = (2 * np.sum(index * sorted_counts)) / (n * np.sum(sorted_counts)) - (n + 1) / n
        
        # ë¶ˆê· í˜• ë¹„ìœ¨
        imbalance_ratio = class_counts.max() / class_counts.min()
        
        result = {
            "num_classes": len(class_counts),
            "min_samples": class_counts.min(),
            "max_samples": class_counts.max(),
            "mean_samples": class_counts.mean(),
            "gini_coefficient": gini,
            "imbalance_ratio": imbalance_ratio
        }
        
        # ì„ê³„ê°’ ê¸°ì¤€
        if gini < 0.5:
            status = "good"
            self.checks_passed.append("Class Balance")
            passed = True
        elif gini < 0.7:
            status = "moderate"
            self.warnings.append(f"Class imbalance moderate (Gini: {gini:.3f})")
            passed = True
        else:
            status = "severe"
            self.checks_failed.append(f"Severe class imbalance (Gini: {gini:.3f})")
            passed = False
            
        result['status'] = status
        
        return passed, result
    
    def check_yolo_consistency(self) -> Tuple[bool, Dict]:
        """YOLO ë°ì´í„°ì…‹ ì¼ê´€ì„± ê²€ì‚¬"""
        yolo_path = self.data_root / "yolo_configs" / "yolo_dataset"
        
        if not yolo_path.exists():
            return True, {"yolo_dataset_exists": False}
        
        images_path = yolo_path / "images"
        labels_path = yolo_path / "labels"
        
        # ì´ë¯¸ì§€ì™€ ë¼ë²¨ íŒŒì¼ ê°œìˆ˜ í™•ì¸
        image_files = set(f.stem for f in images_path.glob("*.png")) if images_path.exists() else set()
        label_files = set(f.stem for f in labels_path.glob("*.txt")) if labels_path.exists() else set()
        
        # ë¶ˆì¼ì¹˜ ì°¾ê¸°
        images_only = image_files - label_files
        labels_only = label_files - image_files
        
        result = {
            "yolo_dataset_exists": True,
            "num_images": len(image_files),
            "num_labels": len(label_files),
            "images_without_labels": len(images_only),
            "labels_without_images": len(labels_only)
        }
        
        passed = len(images_only) == 0 and len(labels_only) == 0
        
        if passed:
            self.checks_passed.append("YOLO Consistency")
        else:
            self.warnings.append(f"YOLO dataset mismatch (images: {len(image_files)}, labels: {len(label_files)})")
            
        return passed, result
    
    def check_sample_counts(self, train_df: pd.DataFrame, val_df: pd.DataFrame) -> Tuple[bool, Dict]:
        """ìƒ˜í”Œ ìˆ˜ ì ì •ì„± ê²€ì‚¬"""
        train_count = len(train_df)
        val_count = len(val_df)
        total_count = train_count + val_count
        
        val_ratio = val_count / total_count if total_count > 0 else 0
        
        result = {
            "train_samples": train_count,
            "val_samples": val_count,
            "total_samples": total_count,
            "val_ratio": val_ratio
        }
        
        # Stageë³„ ì˜ˆìƒ ìƒ˜í”Œ ìˆ˜ì™€ ë¹„êµ
        expected_totals = {
            "stage1": 5000,
            "stage2": 25000,
            "stage3": 100000,
            "stage4": 500000
        }
        
        # í˜„ì¬ stage ì¶”ì •
        for stage, expected in expected_totals.items():
            if total_count <= expected * 1.1:  # 10% í—ˆìš© ì˜¤ì°¨
                result['estimated_stage'] = stage
                break
        
        # Val ë¹„ìœ¨ ì²´í¬ (15-25% ê¶Œì¥)
        if 0.15 <= val_ratio <= 0.25:
            self.checks_passed.append("Sample Split Ratio")
            passed = True
        else:
            self.warnings.append(f"Val ratio {val_ratio:.1%} (recommended: 15-25%)")
            passed = True  # Warning only
            
        return passed, result
    
    def generate_summary_report(self, all_results: Dict) -> str:
        """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        lines = []
        lines.append("=" * 60)
        lines.append("FINAL QUALITY CHECK SUMMARY")
        lines.append("=" * 60)
        
        # ì „ì²´ ìƒíƒœ
        total_checks = len(self.checks_passed) + len(self.checks_failed)
        lines.append(f"\nâœ… Passed: {len(self.checks_passed)}/{total_checks}")
        lines.append(f"âŒ Failed: {len(self.checks_failed)}/{total_checks}")
        lines.append(f"âš ï¸  Warnings: {len(self.warnings)}")
        
        # ì„¸ë¶€ ê²°ê³¼
        lines.append("\n" + "-" * 40)
        lines.append("DETAILED RESULTS:")
        lines.append("-" * 40)
        
        for check_name, result in all_results.items():
            if check_name == "class_consistency":
                lines.append(f"\nğŸ“‹ Class Consistency:")
                lines.append(f"   Train: {result['train_classes']} classes")
                lines.append(f"   Val: {result['val_classes']} classes")
                if result['val_only_classes']:
                    lines.append(f"   âŒ Val-only: {result['val_only_classes']}")
                    
            elif check_name == "combination_ratio":
                lines.append(f"\nğŸ”„ Combination Ratio:")
                lines.append(f"   Train: {result['train_combo_ratio']:.1%}")
                lines.append(f"   Val: {result['val_combo_ratio']:.1%}")
                lines.append(f"   Target: {result['target_range']}")
                
            elif check_name == "corrupted_files":
                lines.append(f"\nğŸ—‘ï¸ Corrupted Files:")
                lines.append(f"   Blacklisted: {result['blacklisted_files']}")
                lines.append(f"   Remaining: {result['remaining_in_manifest']}")
                
            elif check_name == "class_balance":
                lines.append(f"\nâš–ï¸ Class Balance:")
                lines.append(f"   Gini: {result['gini_coefficient']:.3f}")
                lines.append(f"   Imbalance: {result['imbalance_ratio']:.1f}x")
                lines.append(f"   Status: {result['status']}")
                
        # ê¶Œì¥ì‚¬í•­
        lines.append("\n" + "=" * 60)
        lines.append("RECOMMENDATIONS:")
        lines.append("=" * 60)
        
        if self.checks_failed:
            lines.append("\nğŸš¨ Critical Issues to Fix:")
            for issue in self.checks_failed:
                lines.append(f"   - {issue}")
                
        if self.warnings:
            lines.append("\nâš ï¸ Warnings to Consider:")
            for warning in self.warnings:
                lines.append(f"   - {warning}")
                
        if not self.checks_failed and not self.warnings:
            lines.append("\nâœ… All checks passed! Dataset is ready for training.")
            
        return "\n".join(lines)
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰"""
        self.console.print("[bold cyan]ğŸ” Final Quality Check[/bold cyan]")
        
        # Manifest ë¡œë“œ
        train_df, val_df = self.load_manifests()
        
        # ëª¨ë“  ê²€ì‚¬ ìˆ˜í–‰
        all_results = {}
        
        # 1. í´ë˜ìŠ¤ ì¼ì¹˜ì„±
        passed, result = self.check_class_consistency(train_df, val_df)
        all_results['class_consistency'] = result
        
        # 2. Combination ë¹„ìœ¨
        passed, result = self.check_combination_ratio(train_df, val_df)
        all_results['combination_ratio'] = result
        
        # 3. ì†ìƒ íŒŒì¼
        passed, result = self.check_corrupted_files(train_df, val_df)
        all_results['corrupted_files'] = result
        
        # 4. í´ë˜ìŠ¤ ê· í˜•
        passed, result = self.check_class_balance(train_df)
        all_results['class_balance'] = result
        
        # 5. YOLO ì¼ê´€ì„±
        passed, result = self.check_yolo_consistency()
        all_results['yolo_consistency'] = result
        
        # 6. ìƒ˜í”Œ ìˆ˜
        passed, result = self.check_sample_counts(train_df, val_df)
        all_results['sample_counts'] = result
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        summary = self.generate_summary_report(all_results)
        self.console.print(summary)
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_data = {
            "summary": {
                "passed_checks": self.checks_passed,
                "failed_checks": self.checks_failed,
                "warnings": self.warnings,
                "overall_status": "PASS" if not self.checks_failed else "FAIL"
            },
            "detailed_results": all_results,
            "manifest_paths": {
                "train": str(self.train_manifest_path),
                "val": str(self.val_manifest_path)
            }
        }
        
        report_path = self.save_report(report_data, "final_quality_check")
        
        # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ë„ ì €ì¥
        summary_path = self.report_dir / f"quality_summary_{self.timestamp}.txt"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(summary)
        self.logger.info(f"Saved summary: {summary_path}")
        
        # ìµœì¢… ê²°ë¡ 
        if not self.checks_failed:
            self.print_conclusion("PASS", 
                f"âœ… All critical checks passed!\n"
                f"{len(self.warnings)} warnings to consider")
        else:
            self.print_conclusion("FAIL",
                f"âŒ {len(self.checks_failed)} critical issues found\n"
                f"Please run the appropriate fix utilities")
        
        return 0 if not self.checks_failed else 1


def main():
    parser = argparse.ArgumentParser(
        description="Final quality check for training data",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    DataQualityBase.add_common_arguments(parser)
    
    # ì‚¬ìš© ì˜ˆì‹œ
    parser.epilog = """
Examples:
  # Run final quality check
  python final_quality_check.py
  
  # Check cleaned manifests
  python final_quality_check.py --train-manifest artifacts/stage3/manifest_train.cleaned.csv \\
                                --val-manifest artifacts/stage3/manifest_val.cleaned.csv
  
  # With custom report path
  python final_quality_check.py --report-path /path/to/report.json
"""
    
    args = parser.parse_args()
    checker = FinalQualityChecker(args)
    return checker.run()


if __name__ == "__main__":
    exit(main())