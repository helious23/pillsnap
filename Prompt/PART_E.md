# Part E â€” Export(ONNX) Â· ë²„ì „ íƒœê¹… Â· Torchâ†”ONNX ë™ë“±ì„± ê²€ì¦ Â· .env ì—°ë™ Â· í…ŒìŠ¤íŠ¸

# ëª©ì :

# 1) Two-Stage í•™ìŠµ ì‚°ì¶œë¬¼(detection.pt + classification.pt)ì„ ê°ê° ONNXë¡œ ë‚´ë³´ë‚¸ë‹¤.

# 2) í†µí•© ì¶”ë¡  íŒŒì´í”„ë¼ì¸(detection â†’ crop â†’ classification â†’ edi_code)ì„ êµ¬ì„±í•œë‹¤.

# 3) Torch vs ONNX ë™ë“±ì„±ì„ ì •ëŸ‰ ê²€ì¦(detection: mAP ì°¨ì´â‰¤0.01, classification: MSEâ‰¤1e-6)í•œë‹¤.

# 4) ë²„ì „ íƒœê¹…(UTC+git SHA)ê³¼ export_report.jsonì„ ë‚¨ê¸´ë‹¤.

# 5) .envì˜ MODEL_PATHë¥¼ í†µí•´ API/ëŸ°íƒ€ì„ ëª¨ë¸ ê³ ì •ì„ ì§€ì›í•œë‹¤.

[ì „ì œ/ê²½ë¡œ/ê·œì¹™]

- ì½”ë“œ ë£¨íŠ¸: /home/max16/pillsnap
- exp_dir: **/home/max16/ssd_pillsnap/exp/exp01** (SSD ì´ì „ ì™„ë£¼)
- ëª¨ë“  ë°ì´í„° ê²½ë¡œëŠ” **SSD ê¸°ë°˜** (/home/max16/ssd_pillsnap/)ë§Œ ì‚¬ìš©. HDD ê²½ë¡œ(/mnt/data/) ë°±ì—…ìš©.
- ê¸°ë³¸ ì‘ì—…ì€ ê²€ì¶œ+ë¶„ë¥˜(Detection+Classification, Two-Stage). ì•½í’ˆ ê²€ì¶œ í›„ ë¶„ë¥˜í•˜ì—¬ edi_code ë°˜í™˜.

[í•„ìˆ˜ ì˜ì¡´ì„±(WSL)]

- Python íŒ¨í‚¤ì§€: torch, timm, ultralytics, onnx, onnxruntime **(GPUê°€ ìˆìœ¼ë©´ onnxruntime-gpu)**, numpy
- CLI: yq (mikefarah ë²„ì „; YAML íŒŒì„œ)
  - ì ê²€: `yq --version` / ë¯¸ì„¤ì¹˜ ì‹œ ì„¤ì¹˜ ì•ˆë‚´ ì¶œë ¥ í›„ ì¢…ë£Œ
- ê¶Œì¥: pip install tensorboard (E-10ì˜ TBWriter ëŒ€ë¹„)

[ì´ íŒŒíŠ¸ì—ì„œ êµ¬í˜„/ìˆ˜ì •í•  íŒŒì¼]

1. scripts/export_onnx.sh # Two-Stage ì›í´ë¦­ export + ë¹„êµ ë¦¬í¬íŠ¸
2. src/models/detector_yolo11m.py + src/models/classifier_efficientnetv2_s.py # export_detection_onnx(), export_classification_onnx(), build_detection_model(), build_classification_model(), count_params(), prepare_version_tag()
3. src/infer.py # í†µí•© ì¶”ë¡  íŒŒì´í”„ë¼ì¸; detection â†’ crop â†’ classification â†’ edi_code ë§¤í•‘; Torch/ONNX ëŸ°ì²˜(select_onnx_providers í¬í•¨)
4. src/utils.py # utc_timestamp(), get_git_sha(), save_json(), ensure_dir(), list_checkpoints(), find_best_checkpoint()
5. tests/test_export_compare.py # ë™ë“±ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸(detection: mAP, classification: MSE/Top-1)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
E-1) Two-Stage Export ì‚¬ì–‘(ê°•ì œ)

- ì…ë ¥ ckpt: 
  - Detection: {exp_dir}/checkpoints/detection_best.pt 
  - Classification: {exp_dir}/checkpoints/classification_best.pt
- ì¶œë ¥ onnx:
  - Detection: {exp_dir}/export/detection-<UTC>-<gitsha|nogit>.onnx
  - Classification: {exp_dir}/export/classification-<UTC>-<gitsha|nogit>.onnx
  - UTC fmt: %Y%m%d-%H%M%S (ì˜ˆ: 20250810-143012)
- opset: 17 | dynamic_axes: True (í˜¸í™˜ì„± ê²€ì¦ë¨)
  - Detection input: [N,3,640,640] â†’ N:batch, H:height, W:width (ë™ì )
  - Detection output: [N,boxes,6] (x1,y1,x2,y2,conf,class_id) (ë™ì )
  - Classification input: [N,3,384,384] â†’ N:batch (ë™ì )
  - Classification output: [N,num_classes] (ë™ì  N)
  
# ONNX í˜¸í™˜ì„± ê²€ì¦ (opset 17 ì§€ì› í™•ì¸)
onnx_compatibility:
  yolov11m:
    source: "Ultralytics ê³µì‹ ë¬¸ì„œ"
    opset_support: 17
    verification: "ê³µì‹ ì´ìŠˆ ë¡œê·¸ì—ì„œ opset 17 ë‚´ë³´ë‚´ê¸° ì„±ê³µ ì‚¬ë¡€ í™•ì¸"
    fallback: "opset 18 ì‹œë„ í›„ ORT ë²„ì „ ì—…ê·¸ë ˆì´ë“œ ê²€í† "
  efficientnetv2_s:
    source: "timm ë¼ì´ë¸ŒëŸ¬ë¦¬ + PyTorch ê³µì‹ Exporter"
    opset_support: 17
    verification: "timm ì¸¡ ONNX Export ì§€ì›, PyTorch ê³µì‹ ë¬¸ì„œ ê¸°ì¤€ opset 17 ì§€ì›"
    error_handling: "íŠ¹ì • ì—°ì‚° ë¯¸ì§€ì› ì‹œ PyTorchê°€ ëª…ì‹œì  ì˜¤ë¥˜ ë©”ì‹œì§€ ì œê³µ"
  onnx_runtime:
    opset_support: "17 í¬í•¨ í­ë„“ì€ ë²„ì „ ì§€ì›"
    providers: ["CUDAExecutionProvider", "CPUExecutionProvider"]
    loading_safety: "ê²€ì¦ëœ í˜¸í™˜ì„±ìœ¼ë¡œ ë¡œë”© ì¸¡ë©´ì—ì„œ ì•ˆì „"

- Detection Export:
  - YOLOv11 ëª¨ë¸ â†’ ultralytics ë‚´ì¥ export ê¸°ëŠ¥ ì‚¬ìš©
  - yolo_model.export(format="onnx", opset=17, dynamic=True)
  - ê°€ëŠ¥í•˜ë©´ NMS ì´ì „ì˜ raw boxes/scores/class ì¶œë ¥ì„ ì‚¬ìš©í•˜ê³ , í›„ì²˜ë¦¬ëŠ” ê³µí†µ Python NMSë¡œ ìˆ˜í–‰

- Classification Export:
  - timm.create_model(backbone, pretrained=False, num_classes)
  - torch.onnx.export(
    model, dummy, onnx_path,
    input_names=["input"], output_names=["logits"],
    opset_version=17, do_constant_folding=True,
    dynamic_axes={"input": {0:"batch"}, "logits": {0:"batch"}}
    )

- ì˜ˆì™¸/ëŒ€ì‘:
  - Unsupported op â†’ ì¹œì ˆ ë¡œê·¸ + "opset 18 ì‹œë„" ì•ˆë‚´(ê¸°ë³¸ì€ 17 ê³ ì •)
  - half/bfloat export ì´ìŠˆ ì‹œ: export ì „ model.float()ë¡œ ë³€í™˜(ëŸ°íƒ€ì„ì€ ORTê°€ ìµœì í™”)
  
- ë¶€ê°€ ì‚°ì¶œë¬¼:
  - {exp_dir}/export/export_report.json (ì•„ë˜ E-3 ìŠ¤í‚¤ë§ˆ)
  - latest_detection.onnx, latest_classification.onnx ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
  - í†µí•© íŒŒì´í”„ë¼ì¸ ìŠ¤í¬ë¦½íŠ¸: pillsnap_pipeline.py (detection â†’ classification ì—°ê²°)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
E-2) Torchâ†”ONNX ë™ë“±ì„± ê²€ì¦(ê°•ì œ)

- ìƒ˜í”Œ ì¶”ì¶œ(í¸í–¥ ìµœì†Œí™”): config.export.compareì— ë”°ë¼ **ê³„ì¸µì (stratified) ìƒ˜í”Œë§** ìˆ˜í–‰
  - mode: "smoke" | "coverage" | "full"
    - smoke: ë¹ ë¥¸ ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸. sample_count(ê¸°ë³¸ 32)ê°œ ë¬´ì‘ìœ„ ì¶”ì¶œ
    - coverage: **í´ë˜ìŠ¤ ê· í˜• ìƒ˜í”Œë§**. per_class_k(ê¸°ë³¸ 1)ì”© ìˆ˜ì§‘í•˜ë˜,
      min_classes(ê¸°ë³¸ 1000) ì´ìƒì„ ì¶©ì¡±í•˜ê³  max_total(ê¸°ë³¸ 5000) ë‚´ì—ì„œ ì œí•œ
    - full: val ì „ì²´ ì‚¬ìš©(ìŠ¤íŠ¸ë¦¬ë° ë°°ì¹˜ ì¶”ë¡ , ë©”ëª¨ë¦¬ ì œì•½ ì—†ìŒ)
  - stratify_by: ["class"] (í•„ìˆ˜)
  - (ì„ íƒ) hardness_bins: [0.0,0.3,0.6,0.85,1.0]ì—ì„œ **ê²°ì • ê²½ê³„ ê·¼ì²˜ ìƒ˜í”Œ**ì„ binë³„ ì¶”ê°€(hard_per_bin)
- ì „ì²˜ë¦¬: src/data.pyì˜ **ê²€ì¦ íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼**(Resize/CenterCrop/Normalize)
- ë‘ ì—”ì§„ ì‹¤í–‰:
  - Torch: model(x).softmax(dim=1) â†’ prob_t
  - ONNX: onnxruntime.InferenceSession â†’ prob_o
- ì‹¤ìš©ì  í†µê³„ ê¸°ì¤€(coverage/full):
  - classification: MSE(mean) â‰¤ 1e-4, MSE p99 â‰¤ 5e-4, Top-1 mismatch_rate â‰¤ 1% (0.01)
  - fp16 í™˜ê²½ ì™„í™”: MSE(mean) â‰¤ 5e-4, Top-1 mismatch_rate â‰¤ 2% (0.02)
  - detection: mAP Î”â‰¤0.01(0.5 & 0.5:0.95) + p95 IoU Î”â‰¤0.01
  - ë³´ê³ : per-class mismatch Top-10, ê²½ê³„ ì‚¬ë¡€ ë¡œê·¸
- ì‹¤íŒ¨ ì‹œ:
  - í—ˆìš© ì˜¤ì°¨ ìƒí–¥ ê°€ì´ë“œ(ì˜ˆì‹œ: 1e-5)ì™€ ì›ì¸ í›„ë³´(op, ì •ê·œí™” ë¶ˆì¼ì¹˜, ë³€í™˜ ì°¨ì´)ë¥¼ ë¡œê·¸
  - export_report.jsonì— ì‹¤íŒ¨ ì´ìœ  ê¸°ë¡ í›„ **ë¹„ì •ìƒ ì¢…ë£Œ ì½”ë“œ(1)** ë°˜í™˜
  - ê²€ì¶œ ë¹„êµëŠ” Torch/ONNX ëª¨ë‘ ê³µí†µ NMS ì ìš© ê²°ê³¼ë¥¼ ê¸°ì¤€ìœ¼ë¡œ mAP Î”â‰¤0.01ì„ ê²€ì¦

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
E-3) export_report.json ìŠ¤í‚¤ë§ˆ(ê°•ì œ)
{
"backbone": "efficientnet_v2_s",
"num_classes": 5000,
"ckpt_path": "/mnt/data/exp/exp01/checkpoints/best.pt",
"onnx_path": "/mnt/data/exp/exp01/export/model-20250810-143012-ab12cd3.onnx",
"exported_at_utc": "2025-08-10T14:30:12Z",
"git_sha": "ab12cd3" | "nogit",
"input_shape": [1,3,224,224],
"opset": 17,
"dynamic_axes": true,
"params_million": 21.32,
"providers": ["CUDAExecutionProvider","CPUExecutionProvider"],
"postprocess": {
  "nms": {
    "conf_threshold": 0.3,
    "iou_threshold": 0.5,
    "max_detections": 100,
    "class_agnostic": false
  },
  "applied_in": "python_common"
},
"compare": {
  "mode": "coverage",
  "sample_policy": "stratified",
  "sample_count": 1200,
  "per_class_k": 1,
  "min_classes": 1000,
  "max_total": 5000,
  "sampled_classes": 1187,
  "sampled_images": 1200,
  "mse_mean": 7.4e-7,
  "mse_p99": 3.2e-6,
  "top1_match_rate": 1.0,
  "top1_mismatch_count": 0,
  "failed": false,
  "failure_reason": null,
  "deltas": {"map_0_5": 0.0, "map_0_5_0_95": 0.0},
  "notes": "stratified by class; hardness bins enabled"
}
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
E-4) onnxruntime í”„ë¡œë°”ì´ë” ì„ íƒ ì •ì±…(select_onnx_providers)

- ê¸°ë³¸ ìš°ì„ ìˆœìœ„: CUDAExecutionProvider â†’ CPUExecutionProvider
- CUDA EP ì˜µì…˜ ì˜ˆ:
  provider_options=[{"cudnn_conv_use_max_workspace": 1}]
- ì„¸ì…˜ ì˜µì…˜:
  sess_options.graph_optimization_level = ORT_ENABLE_ALL
  - CPU ê²½ë¡œ: sess_options.intra_op_num_threads=0, inter_op_num_threads=0 (ìë™)
  - ë©”ëª¨ë¦¬/ìŠ¤ë ˆë“œ ì œì–´ëŠ” ORT ë²„ì „ì— ë”°ë¼ ì¼ë¶€ ì˜µì…˜ëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¡œê·¸ë¡œ ì‹¤ì œ ì ìš©ê°’ì„ ë‚¨ê¸´ë‹¤.
- ì˜ˆì™¸:
  - GPUê°€ ì—†ì–´ CUDA EP ìƒì„± ì‹¤íŒ¨ ì‹œ ìë™ í´ë°±(CPU EP), ë¡œê·¸ì— ëª…ì‹œ
  - í…ì„œRT EPëŠ” ë³¸ í”„ë¡œì íŠ¸ ê¸°ë³¸ ë¹„í™œì„±(í•„ìš” ì‹œ ë³„ë„ ì•ˆë‚´)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
E-5) .env ì—°ë™ (MODEL_PATH)

- .env(.env.example ê¸°ë°˜)ì— MODEL_PATH í‚¤ë¥¼ ë‘ì–´ **API/ëŸ°íƒ€ì„ì—ì„œ ìš°ì„  ì‚¬ìš©**:
  MODEL_PATH=/mnt/data/exp/exp01/export/model-20250810-143012-ab12cd3.onnx
- src/api/service.pyì˜ ModelManagerëŠ” ë¡œë”© ìš°ì„ ìˆœìœ„ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°€ì ¸ê°„ë‹¤:
  1. ENV: MODEL_PATH (ì¡´ì¬/ê°€ë…ì„± ê²€ì‚¬)
  2. config.export.out_dir ë‚´ ìµœì‹  onnx (latest.onnx ë˜ëŠ” ê°€ì¥ ìµœê·¼ íŒŒì¼)
  3. ì—†ìœ¼ë©´ Torch ckpt(best.pt) ê²½ë¡œ ì‚¬ìš©(ì—”ì§„ Torchë¡œ í´ë°±)
- /reload ì—”ë“œí¬ì¸íŠ¸ëŠ” JSONì˜ "model_path"ë¡œ ê°•ì œ ì „í™˜í•˜ê³  ë²„ì „ ì—…ë°ì´íŠ¸

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
E-6) scripts/export_onnx.sh (ì™„ì „ ë¡œì§ ì§€ì‹œ)

- set -euo pipefail
- ì‚¬ì „ ì ê²€: `command -v yq >/dev/null || { echo "[E-6] yq ë¯¸ì„¤ì¹˜: https://github.com/mikefarah/yq ì°¸ê³ "; exit 1; }`
- (ì˜µì…˜) OPSET í™˜ê²½ë³€ìˆ˜ ì§€ì›: `OPSET=${OPSET:-17}` # ê¸°ë³¸ 17, ì‹¤íŒ¨ ì‹œ 18 ì‹œë„ë¡œ ì•ˆë‚´ ë¡œê·¸
- VENV="$HOME/pillsnap/.venv"; ROOT="/home/max16/pillsnap"
- source "$VENV/bin/activate" && cd "$ROOT"
- CFG="config.yaml"
- EXP_DIR=$(yq '.paths.exp_dir' "$CFG")
- CKPT="$EXP_DIR/checkpoints/best.pt"
- OUT_DIR=$(yq '.export.out_dir' "$CFG")
- ìƒ˜í”Œ ìˆ˜: N=$(yq '.export.compare.sample_count' "$CFG")
- ì‚¬ì „ ì²´í¬: CKPT/OUT_DIR ì¡´ì¬ í™•ì¸, OUT_DIR ì—†ìœ¼ë©´ ìƒì„±
- python - <<'PY'

# í™˜ê²½ë³€ìˆ˜ OPSETì„ ì½ì–´ ê¸°ë³¸ opset_versionì„ ì„¤ì •í•˜ê³ , ì‹¤íŒ¨ ì‹œ opset 18 ì¬ì‹œë„ ì•ˆë‚´ë§Œ ë¡œê·¸ë¡œ ë‚¨ê¸¸ ê²ƒ.

# 1) cfg ë¡œë“œ, 2) ëª¨ë¸ ë¹Œë“œ+ckpt ë¡œë“œ, 3) onnx export, 4) Torch/ONNX ë¹„êµ, 5) export_report.json ì €ì¥

# - ì‹¤íŒ¨ ì‹œ sys.exit(1)

# - ì„±ê³µ ì‹œ onnx ê²½ë¡œ echo

PY

- ì¢…ë£Œ ì½”ë“œ ë¶„ê¸°: 0ì´ë©´ SUCCESS(onnx ê²½ë¡œ ì¶œë ¥), ì•„ë‹ˆë©´ FAIL(ë¡œê·¸ ê²½ë¡œ ì•ˆë‚´)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
E-7) src/model.pyì— ì¶”ê°€í•  í•¨ìˆ˜(êµ¬í˜„ ì§€ì‹œ)

- build_model(backbone, num_classes, channels_last):
  m = timm.create_model(backbone, pretrained=False, num_classes=num_classes)
  if channels_last: m = m.to(memory_format=torch.channels_last)
  return m
- prepare_version_tag():
  sha = get_git_sha() # ì—†ìœ¼ë©´ "nogit"
  ts = utc_timestamp() # UTC ë¬¸ìì—´
  tag = f"{ts}-{sha[:7] if sha!='nogit' else 'nogit'}"
  return tag
- count_params(model) -> float(M): sum(p.numel() for p in model.parameters())/1e6
- export_onnx(cfg, ckpt_path, out_dir):
  1. build_model() â†’ load_state_dict(ì—„ê²© ê²€ì‚¬/ë¶ˆì¼ì¹˜ ì‹œ ì¹œì ˆ ë¡œê·¸)
  2. model.eval().float(); device = cuda or cpu
  3. dummy = torch.randn(*cfg.export.input_shape, device=device)
  4. torch.onnx.export(..., opset=17, dynamic_axes=..., do_constant_folding=True)
  5. íŒŒì¼ëª… = f"model-{prepare_version_tag()}.onnx"
  6. ë°˜í™˜: onnx_path, meta(dict: params_million, opset, input_shape)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
E-8) TensorRT ìµœì í™” Export (RTX 5080 íŠ¹í™”)

**TensorRT ìµœì  í”„ë¡œë°”ì´ë” ì„¤ì •**:
```python
def select_onnx_providers_tensorrt_first():
    """TensorRT â†’ CUDA â†’ CPU ìš°ì„ ìˆœìœ„ë¡œ í”„ë¡œë°”ì´ë” ì„ íƒ"""
    available_providers = ort.get_available_providers()
    
    # TensorRT ìš°ì„  ì‹œë„
    if "TensorrtExecutionProvider" in available_providers:
        try:
            # RTX 5080 ìµœì í™” ì„¤ì •
            tensorrt_options = {
                'device_id': 0,
                'trt_max_workspace_size': 8 * 1024 * 1024 * 1024,  # 8GB
                'trt_max_partition_iterations': 1000,
                'trt_min_subgraph_size': 1,
                'trt_fp16_enable': True,  # RTX 5080 Tensor Cores í™œìš©
                'trt_int8_enable': False,  # í•„ìš”ì‹œ í™œì„±í™”
                'trt_engine_cache_enable': True,
                'trt_engine_cache_path': "/mnt/data/exp/exp01/tensorrt_cache",
            }
            providers = [
                ('TensorrtExecutionProvider', tensorrt_options),
                'CUDAExecutionProvider',
                'CPUExecutionProvider'
            ]
            logger.info("TensorRT EP enabled with RTX 5080 optimizations")
            return providers
        except Exception as e:
            logger.warning(f"TensorRT EP failed: {e}, falling back to CUDA")
    
    # CUDA í´ë°±
    if "CUDAExecutionProvider" in available_providers:
        return ['CUDAExecutionProvider', 'CPUExecutionProvider']
    
    # CPU ìµœì¢… í´ë°±
    return ['CPUExecutionProvider']
```

**INT8 ì–‘ìí™” ìº˜ë¦¬ë¸Œë ˆì´ì…˜**:
```python
def create_calibration_dataset(data_path: str, num_samples: int = 100):
    """INT8 ì–‘ìí™”ë¥¼ ìœ„í•œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„°ì…‹ ìƒì„±"""
    import glob
    from PIL import Image
    
    image_paths = glob.glob(f"{data_path}/**/*.jpg", recursive=True)[:num_samples]
    calibration_data = []
    
    for path in image_paths:
        img = Image.open(path).convert('RGB')
        # preprocessì™€ ë™ì¼í•œ ì „ì²˜ë¦¬ ì ìš©
        img_tensor = preprocess([path], img_size=384)[0:1]  # ë°°ì¹˜=1
        calibration_data.append(img_tensor.numpy())
    
    return calibration_data

def quantize_model_int8(onnx_model_path: str, calibration_data_path: str):
    """ONNX ëª¨ë¸ INT8 ì–‘ìí™”"""
    from onnxruntime.quantization import quantize_dynamic, QuantType
    
    input_model = onnx_model_path
    output_model = onnx_model_path.replace('.onnx', '_int8.onnx')
    
    calibration_data = create_calibration_dataset(calibration_data_path, 100)
    
    quantize_dynamic(
        input_model,
        output_model,
        weight_type=QuantType.QInt8,
        nodes_to_exclude=['Sigmoid', 'Tanh']  # ì •í™•ë„ ë¯¼ê° ë…¸ë“œ ì œì™¸
    )
    
    logger.info(f"INT8 quantized model saved: {output_model}")
    return output_model
```

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
E-9) src/infer.py (ê³µí†µ ì „ì²˜ë¦¬Â·Torch/ONNX/TensorRT ëŸ°ì²˜)

- **ì „ì²˜ë¦¬ ë‹¨ì¼í™” ê°•ì œ**: preprocess(paths: list[str], img_size: int) -> Tensor[N,3,H,W]
  - src/data.pyì˜ val ì „ì²˜ë¦¬ì™€ **ì™„ì „ ë™ì¼** (Resize/CenterCrop/Normalize)
  - **Torch/ONNXê°€ ëª¨ë‘ ì´ í•¨ìˆ˜ë¥¼ ê³µìœ ** (ì¤‘ë³µ êµ¬í˜„ ê¸ˆì§€)
  - ì—”ì§„ë³„ ì •ê·œí™”/ë¦¬ì‚¬ì´ì¦ˆ/ë ˆì´ì•„ì›ƒ ì°¨ì´ë¡œ ì¸í•œ ë™ë“±ì„± ë¬¸ì œ ë°©ì§€
- infer_torch(model, batch_tensor, device) -> logits/prob
- infer_onnx(onnx_path, batch_tensor) -> logits/prob
  - select_onnx_providers_tensorrt_first()ë¡œ TensorRT ìš°ì„  ì„¸ì…˜ êµ¬ì„±
- infer_tensorrt(onnx_path, batch_tensor) -> logits/prob
  - TensorRT ì „ìš© ì¶”ë¡  (RTX 5080 ìµœì í™”)

```
cli: python -m src.infer --engine torch|onnx|tensorrt \
       --model <path> \
       --inputs "/mnt/data/AIHub_576/val/**/*.{jpg,jpeg,png}" \
       --batch 16 \
       --quantize int8  # ì„ íƒì  ì–‘ìí™”
```

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
E-9) tests/test_export_compare.py (ë™ë“±ì„± í…ŒìŠ¤íŠ¸ ì§€ì‹œ)

- ì¤€ë¹„: exp_dir/checkpoints/best.pt ê°€ì •(ì—†ìœ¼ë©´ í…ŒìŠ¤íŠ¸ skip or xfail)
- ì‹œë‚˜ë¦¬ì˜¤:
  1. export ìŠ¤í¬ë¦½íŠ¸ í˜¸ì¶œ(or í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ)ë¡œ onnx ìƒì„±
  2. valì—ì„œ ìµœëŒ€ Nê°œ ìƒ˜í”Œ ìˆ˜ì§‘ â†’ preprocess ë™ì¼ ì ìš©
  3. Torch vs ONNX ì‹¤í–‰ â†’ MSE/Top-1 ë¹„êµ
  4. export_report.json ë‚´ìš©/ìŠ¤í‚¤ë§ˆ ê²€ì¦(opset/dynamic_axes/providers/failed=false)
- ê²½ê³„:
  - GPU ë¶€ì¬ ì‹œ CPU ê²½ë¡œë¡œ ìë™ í´ë°±í•˜ëŠ”ì§€ í™•ì¸
  - config.export.input_shape ë³€ê²½ ì‹œ ì •ìƒ ë°˜ì˜ë˜ëŠ”ì§€ í™•ì¸
  - onnxruntime ë¯¸ì„¤ì¹˜ í™˜ê²½ì—ì„œëŠ” í…ŒìŠ¤íŠ¸ë¥¼ ìë™ skip(xfail) ì²˜ë¦¬.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
E-10) ì‹¤í–‰ ì˜ˆì‹œ
$ bash scripts/export_onnx.sh

# ì„±ê³µ ì‹œ:

# [OK] Exported: /mnt/data/exp/exp01/export/model-20250810-143012-ab12cd3.onnx

# Report: /mnt/data/exp/exp01/export/export_report.json

# ìˆ˜ë™ ë¹„êµ(ë””ë²„ê·¸)

$ python -m src.infer --engine torch --model /mnt/data/exp/exp01/checkpoints/best.pt --inputs "/mnt/data/AIHub_576/val/**/\*.jpg" --batch 16
$ python -m src.infer --engine onnx --model /mnt/data/exp/exp01/export/model-20250810-143012-ab12cd3.onnx --inputs "/mnt/data/AIHub_576/val/**/\*.jpg" --batch 16

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
E-11) íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

- Unsupported operator:
  â†’ opset 17 ìœ ì§€ ê¶Œì¥. ì•ˆë˜ë©´ ì„ì‹œë¡œ 18 ì‹œë„ í›„ ORT ë²„ì „ ì—…ê·¸ë ˆì´ë“œ ê²€í† .
- ê°’ ë¶ˆì¼ì¹˜(MSEâ†‘/Top-1 mismatch):
  â†’ fp16 í™˜ê²½ì—ì„œëŠ” ì™„í™”ëœ ê¸°ì¤€ ì ìš©(MSE â‰¤ 5e-4, Top-1 mismatch â‰¤ 2%)
  â†’ ì „ì²˜ë¦¬ ë¶ˆì¼ì¹˜(ì •ê·œí™”/í¬ë¡­) í™•ì¸, ëª¨ë¸ eval/float ì¬í™•ì¸, dynamic_axes ëˆ„ë½ ì ê²€
- CUDA EP ì‹¤íŒ¨:
  â†’ nvidia-smi/ë“œë¼ì´ë²„ í™•ì¸, onnxruntime-gpu ì„¤ì¹˜ ê³ ë ¤(ê¸°ë³¸ì€ onnxruntime). ìë™ CPU í´ë°± ë™ì‘ í™•ì¸.
- ë©”ëª¨ë¦¬ ë¶€ì¡±:
  â†’ ë°°ì¹˜ ì¶•ì†Œ, 224â†’192/160 ì„ì‹œ ì¶•ì†Œ, do_constant_folding=True ìœ ì§€.

## ğŸ¯ **PART_E í•µì‹¬ ì—…ë°ì´íŠ¸ ì™„ë£Œ**

### âœ… **ì‚¬ìš©ì ì œì–´ Two-Stage ONNX Export**
- **ë‹¨ì¼ ì•½í’ˆ**: EfficientNetV2-S â†’ ONNX (384px ì…ë ¥)
- **ì¡°í•© ì•½í’ˆ**: YOLOv11m â†’ ONNX (640px ì…ë ¥)  
- **Dynamic Axes**: ë°°ì¹˜ í¬ê¸° ê°€ë³€ ì§€ì›

### âœ… **RTX 5080 ìµœì í™” Export**
- **ONNX Runtime**: GPU EP ìë™ ì„ íƒ, CUDA/CPU í´ë°±
- **ëª¨ë¸ ìµœì í™”**: graph optimization, memory pattern ìµœì í™”
- **ì–‘ìí™”**: INT8 ë™ì  ì–‘ìí™” ì˜µì…˜ (ì •í™•ë„ í—ˆìš© ë²”ìœ„ ë‚´)

### âœ… **ì‹¤ìš©ì  ê²€ì¦ & í’ˆì§ˆ ë³´ì¥**
- **PyTorch vs ONNX**: MSE â‰¤ 1e-4, Top-1 mismatch â‰¤ 0.1%
- **ì„±ëŠ¥ ë¹„êµ**: ì¶”ë¡  ì†ë„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
- **fp16 ì™„í™”**: fp16 ë‚´ë³´ë‚´ê¸° ì‹œ ì™„í™”ëœ ê¸°ì¤€ ì ìš©

**âœ… PART_E ì™„ë£ˆ: ì‚¬ìš©ì ì œì–´ Two-Stage ONNX Export ë° ì‹¤ìš©ì  ê²€ì¦**
