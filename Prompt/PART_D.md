# Part D â€” ì¡°ê±´ë¶€ Two-Stage í•™ìŠµ + RTX 5080 16GB ìµœì í™”

**ëª©ì **: RTX 5080 16GB + 128GB RAM ìµœì í™” í•™ìŠµ, ì¡°ê±´ë¶€ Two-Stage (ë‹¨ì¼â†’ì§ì ‘ë¶„ë¥˜, ì¡°í•©â†’ê²€ì¶œí›„ë¶„ë¥˜), ì•ˆì •ì„±(OOM í´ë°±), ê°€ì‹œì„±(TensorBoard), ì¬í˜„ì„±(ì²´í¬í¬ì¸íŠ¸)

[ì „ì œ/ê³ ì •]

- **ê²½ë¡œ**: /home/max16/pillsnap (ì½”ë“œ), **/home/max16/pillsnap_data** (Native Linux SSD, í”„ë¡œì íŠ¸ ë¶„ë¦¬), **/home/max16/pillsnap_data/exp/exp01** (ì‹¤í—˜, Native Linux SSD)
- **í•˜ë“œì›¨ì–´**: AMD Ryzen 7800X3D + 128GB RAM + RTX 5080 16GB
- **RAM í™œìš©**: ë¼ë²¨ ìºì‹œ, ë°°ì¹˜ í”„ë¦¬í˜ì¹˜, ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ìºì‹œ, ì›Œì»¤ ë©”ëª¨ë¦¬ ê³µìœ 
- **í•µì‹¬**: ì‚¬ìš©ì ì œì–´ Two-Stage (pipeline_strategy: "user_controlled")
- **ëª¨ë¸**: YOLOv11m (ê²€ì¶œ, 640px) + EfficientNetV2-S (ë¶„ë¥˜, 384px, 5000í´ë˜ìŠ¤)
- **ëª¨ë¸ íŒŒì¼**: `detector_yolo11m.py`, `classifier_efficientnetv2_s.py`
- **ìµœì í™”**: AMP auto, TF32, channels_last, torch.compile, ë°°ì¹˜í¬ê¸° ìë™ì¡°ì •

[ì´ íŒŒíŠ¸ì—ì„œ êµ¬í˜„/ìˆ˜ì •í•  íŒŒì¼] (í˜„ì¬ êµ¬ì¡° ë°˜ì˜)

1. src/utils/core.py  # âœ… êµ¬í˜„ ì™„ë£Œ (ConfigLoader, PillSnapLogger)

   - load_config(cfg_path: str) -> DictConfig|dict
   - set_seed(seed: int, deterministic: bool) -> None
   - build_logger(exp_dir: str, name="pillsnap") -> logging.Logger # ì½˜ì†”+íŒŒì¼ í•¸ë“¤ëŸ¬
   - StepTimer # data/compute íƒ€ì´ë° í—¬í¼
   - TBWriter(exp_dir: str) # tensorboard ì„¸ì…˜(ì—†ìœ¼ë©´ ì•ˆì „ í´ë°±)
   - save_json(path: str, obj: dict) / read_json(path: str) -> dict
   - ensure_dir(path: str) -> None
   - get_git_sha() -> str|"nogit"
   - select_amp_dtype(policy: "auto"|"fp16"|"bf16") -> torch.dtype
   - enable_tf32(flag: bool) -> None
   - try_torch_compile(model, mode: str) -> nn.Module
   - cuda_mem_mb() -> float
   - auto_find_batch_size(fn_train_step, init_bs: int, min_bs=1, max_bs=None, amp_force_fp16=False) -> int # Annex1
   - autotune_num_workers(build_loader_fn, candidates=[4,8,12,16], warmup=30, steps=100) -> int # Annex2
   - save_ckpt(path, obj) / load_ckpt(path) -> dict # Annex8

2. src/train.py

   - main(args|cfg): Two-Stage í•™ìŠµ ì—”íŠ¸ë¦¬ (classification ìš°ì„ , detectionì€ í•„ìš”ì‹œ) + Stage 4 ì™„ë£Œì‹œ final_test_evaluation í˜¸ì¶œ
   - train_classification_stage(cfg, logger) -> classification_model_path  # ë‹¨ì¼ ì•½í’ˆìš© (ì£¼ë ¥)
   - train_detection_stage(cfg, logger) -> detection_model_path  # ì¡°í•© ì•½í’ˆìš© (ì„ íƒì )
   - build_classification_model(backbone: str, num_classes: int, channels_last: bool) -> nn.Module
   - build_yolo_model(model_size: str, num_classes: int, lazy_load: bool=True) -> YOLO model | None
   - build_criterion_detection() -> YOLOv11 ë‚´ì¥ loss
   - build_criterion_classification(cfg, class_weights: Tensor|None) -> nn.Module
   - build_optimizer(cfg, params) -> torch.optim.Optimizer
   - build_scheduler(cfg, optimizer, steps_per_epoch: int) -> torch.optim.lr_scheduler._LRScheduler
   - train_one_epoch_classification(model, loader, optimizer, scaler, criterion, scheduler, cfg, logger, tb, epoch_idx) -> dict
   - train_one_epoch_detection(yolo_model, train_loader, cfg, logger, tb, epoch_idx) -> dict  # ì¡°í•©ìš©
   - validate_classification(model, loader, criterion, cfg, logger, tb, epoch_idx) -> dict
   - validate_detection(yolo_model, val_loader, cfg, logger, tb, epoch_idx) -> dict # mAP ê³„ì‚°
   - evaluate_classification_metrics(y_true, y_pred, y_prob, topk=[1,5]) -> dict # accuracy, macro_f1, top5, confidence
   - final_test_evaluation(cfg, test_loaders, model_paths, logger, exp_dir) -> dict  # Stage 4 ì™„ë£Œ í›„ test ìµœì¢… í‰ê°€
   - evaluate_detection_metrics(predictions, targets) -> dict # mAP@0.5, mAP@0.5:0.95
   - predict_pipeline(image, mode="single", conf_threshold=0.3) -> dict  # í†µí•© ì¶”ë¡  íŒŒì´í”„ë¼ì¸
   - early_stopping(state, metric_name, mode="max", patience=7, min_delta=0.0) -> (stop: bool, best:bool)
   - oom_recovery_state_machine(...) -> Updated cfg knobs / signals # Annex6
   - (ì˜µì…˜) EMA: if cfg.train.ema.enabled: update/validate with EMA weights # Annex5

3. src/evaluate.py

   - plot_confusion_matrix(cm, class_names, out_path)
   - plot_detection_results(images, predictions, out_dir) # bbox ì‹œê°í™”
   - plot_roc_pr_curves(y_true_onehot, y_prob, out_dir) # ë¶„ë¥˜ìš© ì €ì¥
   - dump_metrics_json(metrics: dict, out_path) # {detection: {mAP@0.5, mAP@0.5:0.95}, classification: {acc,macro_f1,top5,loss}}

4. scripts/train.sh (Part B ê³¨ê²©ì„ ì‹¤ì œ ë¡œì§ìœ¼ë¡œ ì±„ì›€)
   - venv í™œì„±í™” â†’ config ê²½ë¡œ í™•ì¸ â†’ exp_dir/{logs,tb,reports,checkpoints,export} ë³´ì¥
   - í•µì‹¬ êµ¬ì„± echo í›„ python -m src.train --cfg config.yaml ì‹¤í–‰
   - stdout/stderrë¥¼ exp_dir/logs/train.out|errë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸í•˜ê³  ì¢…ë£Œì½”ë“œë³„ ë©”ì‹œì§€

[Interleaved Two-Stage í•™ìŠµ íŒŒì´í”„ë¼ì¸ ìƒì„¸ ê·œì¹™]

D-1) Interleaved Learning Strategy

- ì „ëµ: "interleaved" - ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ë¡œ ê²€ì¶œ/ë¶„ë¥˜ êµì°¨ í•™ìŠµ
- ë¹„ìœ¨: [1, 1] - det 1ìŠ¤í… â†’ cls 1ìŠ¤í… êµëŒ€ ë°˜ë³µ
- êµ¬í˜„: 
  ```python
  d, c = cfg.train.interleave_ratio  # [1, 1]
  for step in range(total_steps):
      if step % (d + c) < d:
          train_detection_step()
      else:
          train_classification_step()
  ```
- íš¨ê³¼: GPU ìœ íœ´ì‹œê°„ 50% â†’ 10% ê°ì†Œ, ì „ì²´ í•™ìŠµì‹œê°„ 30% ë‹¨ì¶•

D-2) ë””ë°”ì´ìŠ¤/ì •ë°€ë„/í¬ë§· (Both Stages)

- device = "cuda" if torch.cuda.is_available() else "cpu"
- TF32: cfg.train.detection/classification.tf32 â†’ enable_tf32(True) â‡’
  torch.backends.cuda.matmul.allow_tf32=True, torch.backends.cudnn.allow_tf32=True,
  torch.set_float32_matmul_precision("high")
- AMP:
  - Detection: YOLOv11 ìì²´ AMP ì‚¬ìš©
  - Classification: dtype = select_amp_dtype(cfg.train.classification.amp_dtype) # "auto"ë©´ bf16 ì§€ì› ì‹œ bfloat16, ì•„ë‹ˆë©´ fp16
    autocast(enabled=cfg.train.classification.amp, dtype=dtype) + GradScaler(enabled=(cfg.train.classification.amp and dtype==torch.float16))
  - ì£¼ì˜: dtype==bfloat16 ì¸ ê²½ìš° GradScalerëŠ” ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ë¹„í™œì„±.
- channels_last: ë¶„ë¥˜ ëª¨ë¸ì—ë§Œ ì ìš©
  if cfg.train.classification.channels_last=True: model = model.to(memory_format=torch.channels_last); ì…ë ¥ í…ì„œë„ to(memory_format=channels_last)ë¡œ ì „ì†¡
- torch.compile:
  - MVP: "reduce-overhead" (ì•ˆì •ì„± ìš°ì„ )
  - Advanced+: "max-autotune" (RTX 5080 ê³µê²©ì  ìµœì í™”)
  - ì›Œë°ì—…: 100 ìŠ¤í… ì»´íŒŒì¼ ì•ˆì •í™”
  - ì‹¤íŒ¨ ì‹œ í´ë°± ëª¨ë“œë¡œ ìë™ ì „í™˜ í›„ ë¡œê·¸

D-3) ëª¨ë“œë³„ ë°ì´í„°ë¡œë” êµ¬ì„±(Part C ì—°ë™)

- from src.data import build_dataloaders_twostage
- ë°˜í™˜: cls_train_loader, cls_val_loader, cls_test_loader|None, det_train_loader(optional), det_val_loader(optional), det_test_loader(optional), meta
- Classification (ì£¼ë ¥): ë‹¨ì¼ ì•½í’ˆ ì§ì ‘ ë¶„ë¥˜ìš© ë°ì´í„°ë¡œë”
- Detection (ì¦‰ì‹œ ë¡œë“œ): ì¡°í•© ì•½í’ˆìš© YOLO í¬ë§· ë°ì´í„°ë¡œë” (128GB RAMìœ¼ë¡œ ì¦‰ì‹œ ë¡œë“œ)
- ì¤‘ìš”: test_loaderë“¤ì€ Stage 4 ì™„ë£Œ ì „ì—ëŠ” None ë°˜í™˜, Stage 4 ì™„ë£Œ í›„ì—ë§Œ ìƒì„±í•˜ì—¬ ìµœì¢… í‰ê°€ì‹œ ì‚¬ìš©
- cfg.dataloader.autotune_workers=Trueë©´ autotune_num_workers()ë¡œ í›„ë³´ [4,8,12,16] ì¤‘ ì„ íƒ â†’ ìµœì¢… ë¡œë” ì¬ìƒì„±
- class_weights: cfg.loss.use_class_weights=Trueë©´ ë¶„ë¥˜ ë‹¨ê³„ì—ì„œë§Œ compute_class_weights(...) ì ìš©
- ë¡œë” ê³µí†µ: pin_memory, persistent_workers, prefetch_factor, drop_last, safe_collate
  - PyTorchâ‰¥2.0ì´ë©´ pin_memory_device="cuda" ì„¤ì •ì„ ê³ ë ¤(ì…ì¶œë ¥ ê³ ì • ì‹œ íš¨ìœ¨ â†‘).
- ë¡œê¹… ê°•í™”(ê²Œì´íŒ…/í˜¼ë™ ì¼€ì´ìŠ¤): step/epoch ë¡œê·¸ì™€ TBì— ë‹¤ìŒì„ ì¶”ê°€ ê¸°ë¡í•œë‹¤.
  - gating_score, mode_used, single_confidence, fallback_reason("low_confidence"|"multi_object_hint"|"manual_combo")
  - hard_cases ì €ì¥ ì¡°ê±´(ì˜ˆ): (mode="single" & single_confidence<0.30) or (mode="auto" & mode_used="combo") or misroute ì¶”ì •

D-2.1) 128GB RAM ìµœì í™” ì „ëµ

- **ë©”ëª¨ë¦¬ í• ë‹¹ ê³„íš (í˜„ì‹¤ì  ê¸°ë³¸ì„ )**:
  - OS + ê¸°ë³¸: ~8GB
  - í•«ì…‹ ìºì‹œ: ~25GB (6ë§Œì¥ Ã— 384px)
  - ë°°ì¹˜ í”„ë¦¬í˜ì¹˜: ~8GB (prefetch_factor=4 Ã— 2GB/ë°°ì¹˜)
  - ë¼ë²¨ ìºì‹œ: ~2GB (ë©”íƒ€ë°ì´í„°)
  - ì›Œì»¤ ë²„í¼: ~8GB (8 ì›Œì»¤ Ã— 1GB)
  - ì—¬ìœ  ë©”ëª¨ë¦¬: ~77GB (í™•ì¥ ì—¬ìœ ë¶„)

- **êµ¬ì²´ì  êµ¬í˜„ (ê¸°ë³¸ì„ )**:
  - `cache_policy="hotset"`: í•«ì…‹ 6ë§Œì¥ë§Œ ìºì‹œ (Phase 1 ê¸°ë³¸)
  - `use_lmdb=false`: ê¸°ë³¸ ë¹„í™œì„±, data_time ë³‘ëª© ì‹œì—ë§Œ í™œì„±í™”
  - `preload_samples=0`: ê¸°ë³¸ ë¯¸ì‚¬ìš©, í•„ìš”ì‹œ ë‹¨ê³„ì  ì¦ê°€
  - `prefetch_factor=4`: í˜„ì‹¤ì  ìˆ˜ì¤€ (8â†’4ë¡œ ê°ì†Œ)
  - `num_workers=8`: í˜„ì‹¤ì  ì›Œì»¤ ìˆ˜ (16â†’8ë¡œ ê°ì†Œ)

D-3) ëª¨ë¸/ì†ì‹¤/ì˜µí‹°ë§ˆ/ìŠ¤ì¼€ì¤„ëŸ¬

- ë¶„ë¥˜ ëª¨ë¸ (ì£¼ë ¥): timm.create_model(cfg.model.backbone, pretrained=True, num_classes=cfg.model.num_classes)
  - channels_lastë©´ ëª¨ë¸ ë©”ëª¨ë¦¬ í¬ë§· ë³€í™˜
- ê²€ì¶œ ëª¨ë¸ (ì„ íƒì ): YOLO ì§€ì—° ë¡œë”© - cfg.inference.lazy_load_detector=Trueë©´ í•„ìš”ì‹œë§Œ ë¡œë“œ
- ì†ì‹¤: CrossEntropy(label_smoothing=cfg.loss.label_smoothing, weight=class_weights|None)
- ì˜µí‹°ë§ˆ: AdamW(lr=cfg.train.lr, weight_decay=cfg.train.weight_decay, fused=True) # torchâ‰¥2.0 & CUDAì¼ ë•Œë§Œ; ë¯¸ì§€ì› ì‹œ ìë™ False
- ìŠ¤ì¼€ì¤„ëŸ¬: cfg.train.scheduler == "cosine" â†’ CosineAnnealing + warmup_steps
  (onecycle ì˜µì…˜ì€ í›„ì† í™•ì¥ ê°€ëŠ¥)
- zero_grad(set_to_none=cfg.train.zero_grad_set_to_none)

D-4) ë™ì  ë°°ì¹˜ íŠœë‹ (Auto Batch, Annex1)

- cfg.train.detection.auto_batch_tune=True/classification.auto_batch_tune=Trueì´ë©´
  1. max_batch = cfg.train.detection.auto_batch_max (16) / classification.auto_batch_max (64)
  2. vram_headroom = 0.88 (12% ì—¬ìœ ë¡œ torch.compile ì˜¤ë²„í—¤ë“œ ëŒ€ì‘)
  3. fn_train_stepëŠ” ëª‡ stepë§Œ ì‹¤ì œ forward/backwardê¹Œì§€ ìˆ˜í–‰
  4. OOM ë°œìƒ ì‹œ: torch.cuda.empty_cache() â†’ grad_accum ì¦ê°€ â†’ batch ë°˜ê° ìˆœ
  5. í™•ì •ëœ batch_sizeë¡œ train_loader ì¬ìƒì„±, ë¡œê·¸/TB ê¸°ë¡

D-5) í•™ìŠµ ë£¨í”„(1 epoch + Interleaved Strategy)

**Interleaved Two-Stage í•™ìŠµ (PART_B ì„¤ì • ë°˜ì˜)**:
```python
# cfg.train.strategy == "interleaved"ì¸ ê²½ìš°
d, c = cfg.train.interleave_ratio  # [1, 1]
det_loader_iter = iter(det_loader)
cls_loader_iter = iter(cls_loader)

for global_step in range(total_steps):
    if global_step % (d + c) < d:
        # Detection í•™ìŠµ ìŠ¤í…
        batch = next(det_loader_iter)
        loss = train_detection_step(det_model, batch, ...)
    else:
        # Classification í•™ìŠµ ìŠ¤í…
        batch = next(cls_loader_iter)
        loss = train_classification_step(cls_model, batch, ...)
```

**ê¸°ë³¸ í•™ìŠµ ë£¨í”„ (ê° ëª¨ë¸ë³„)**:
- Timerë¡œ stepë³„ data_time/compute_time ì¸¡ì •
- for step, (x,y,meta) in enumerate(train_loader):
  - x,yë¥¼ deviceë¡œ ì „ì†¡(to(..., non_blocking=True)); channels_lastë©´ x = x.to(memory_format=torch.channels_last)
  - with autocast(...):
    logits = model(x)
    loss = criterion(logits, y) / cfg.train.grad_accum_steps
  - scaler.scale(loss).backward()
  ```python
  if (step + 1) % grad_accum_steps == 0:
    if cfg.train.grad_clip:
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.train.grad_clip)
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad(set_to_none=True)
    scheduler.step()
  ```
  - compute throughput = (`effective_batch = batch_size * grad_accum_steps`) / (data_time + compute_time)
  - step_log_intervalë§ˆë‹¤:
    logger.info(f"[{epoch}/{step}] loss=... lr=... data=... compute=... thr=... cuda={cuda_mem_mb():.1f}")
    TB: train/loss, train/lr, perf/data_time, perf/compute_time, perf/throughput, perf/cuda_mem
- (ì˜µì…˜) EMA: stepë§ˆë‹¤ ema.update(model)

D-6) ê²€ì¦/ì§€í‘œ/ì–¼ë¦¬ìŠ¤í†±

- validate():
  - Stageë³„ í•™ìŠµ ì¤‘ì—ëŠ” val_loaderë§Œ ì‚¬ìš© (test_loader ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€)
  - no_grad + autocast(enabled=False) # ì•ˆì •ì„± ìœ„í•´ ê²€ì¦ì€ fp32
  - ì „ì²´ ë¡œë” ìˆœíšŒí•˜ì—¬ loss/ì •í™•ë„/topk ìˆ˜ì§‘; macro_f1 ê³„ì‚°(sklearn)
  - y_prob = softmax(logits); y_pred = argmax
- í‰ê°€ ì§€í‘œ(dict):
  { "val_loss":..., "acc":..., "macro_f1":..., "top5":..., "n":... }
  â†’ TBì— val/accuracy, val/macro_f1, val/loss ë¡œ ê¸°ë¡
- ê²€ì¶œ mAP ê³„ì‚°ì€ Torch/ONNX ê³µí†µ NMSë¥¼ ì ìš©í•œ ê²°ê³¼ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜í–‰í•œë‹¤.
- early_stopping():
  - monitor=cfg.train.early_stopping.monitor (ë¶„ë¥˜: macro_f1, ê²€ì¶œ: mAP@0.5), mode="max"
  - patience, min_delta ì ìš©
  - best ê°±ì‹  ì‹œ best.pt ì €ì¥(Annex8 ìŠ¤í‚¤ë§ˆ)
  - stop=Trueë©´ í•™ìŠµ ì¢…ë£Œ

# ìµœì¢… í‰ê°€ (Stage 4 ì™„ë£Œ í›„)
- final_test_evaluation(cfg, test_loaders, model_paths, logger, exp_dir):
  â€¢ ì „ì œì¡°ê±´: Stage 4 (500K, 5000í´ë˜ìŠ¤) ì™„ë£Œ ë° best.pt ì¡´ì¬ í™•ì¸
  â€¢ ëª¨ë¸ ë¡œë”©: classification_best.pt, detection_best.pt (ìˆëŠ” ê²½ìš°)
  â€¢ Test ë°ì´í„° í‰ê°€: test_loaders (ë‹¨ì¼+ì¡°í•©) 1íšŒ ì‹¤í–‰, í•™ìŠµì¤‘ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€ í™•ì¸
  â€¢ ì„±ëŠ¥ ë¹„êµ: val_metrics vs test_metrics ì°¨ì´ ê³„ì‚° (overfitting ì§„ë‹¨)
  â€¢ ëª©í‘œ ë‹¬ì„± í™•ì¸: ë‹¨ì¼ 92% (macro_f1), ì¡°í•© mAP@0.5=0.85
  â€¢ ê²°ê³¼ ì €ì¥: exp_dir/reports/final_test_results.json
  â€¢ ìƒìš©í™” ê¶Œê³ : ëª©í‘œ ë‹¬ì„±ì‹œ "READY_FOR_PRODUCTION", ë¯¸ë‹¬ì‹œ ê°œì„ ì‚¬í•­ ì œì‹œ
  â€¢ ë°˜í™˜ êµ¬ì¡°: {"test_results": {"classification": {"accuracy": 0.921, "macro_f1": 0.889}, "detection": {"mAP_0.5": 0.847}}, "business_goals": {"overall_target_met": false}, "recommendation": "NEEDS_IMPROVEMENT", "ready_for_production": false, "improvements": [...]}  

D-7) ì²´í¬í¬ì¸íŠ¸/ì¬ê°œ(Annex8)

- last.pt: ë§¤ epoch ì €ì¥(ëª¨ë¸/opt/sched/scaler/epoch/best_metric/config í¬í•¨)
- best.pt: monitor ê¸°ì¤€ ìµœê³ ì¼ ë•Œë§Œ ê°±ì‹ 
- resume:
  - cfg.train.resume == "last"ë©´ ìë™ íƒìƒ‰ ë¡œë“œ(ì—†ìœ¼ë©´ ê²½ê³ )
  - ì‹¤ìˆ˜ ë°©ì§€: ëª¨ë¸ ë°±ë³¸/num_classes ë¶ˆì¼ì¹˜ ì‹œ ì¹œì ˆí•œ ì—ëŸ¬ë¡œ ì¤‘ë‹¨

D-8) OOM í´ë°± ìƒíƒœ ë¨¸ì‹  + ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§(Annex6)

- ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§(ì˜ˆë°©):
  - ë§¤ epoch ì‹œì‘ ì‹œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì²´í¬
  - 80% ì´ˆê³¼ ì‹œ ê²½ê³  ë¡œê·¸(oom/memory_warning)
  - ì‚¬ìš©ë¥ ê³¼ ë°°ì¹˜ í¬ê¸° ìƒê´€ê´€ê³„ TB ê¸°ë¡
- CUDA OOM RuntimeError ê°ì§€ ì‹œ ê°€ë“œë ˆì¼ ì ìš©:
  ```python
  def oom_recovery_state_machine(retries, current_state, cfg):
      \"\"\"OOM ë³µêµ¬ë¥¼ ìœ„í•œ í•™ìŠµ ì¼ê´€ì„± ë³´ì¥ ìƒíƒœ ë¨¸ì‹ \"\"\"
      retries += 1
      if retries > cfg.train.oom.max_retries:  # ê¸°ë³¸ 4íšŒ
          return {"action": "emergency_exit", "save_checkpoint": True}
      
      time.sleep(cfg.train.oom.cooldown_sec)  # ê¸°ë³¸ 2ì´ˆ
      
      # í˜„ì¬ ê¸€ë¡œë²Œ ë°°ì¹˜ í¬ê¸°
      G_old = current_state["batch_size"] * current_state["grad_accum"]
      
      # S1: AMP fp16 ê°•ì œ (1íšŒë§Œ, ëª¨ë¸/ìˆ˜í•™ ê·¸ëŒ€ë¡œ)
      if cfg.train.oom.escalate_to_fp16 and not current_state.get("did_amp_escalate"):
          logger.warning(f"OOM Recovery S1: Escalating to fp16 (preserving schedules)")
          return {
              "action": "amp_fp16",
              "preserve_schedules": True,  # ìŠ¤ì¼€ì¤„ë§ ë³€ê²½ ì—†ìŒ
              "next_state": {**current_state, "did_amp_escalate": True}
          }
      
      # S2: ë§ˆì´í¬ë¡œë°°ì¹­ (ê¸€ë¡œë²Œ ë°°ì¹˜ ìœ ì§€ ìš°ì„ )
      if current_state["grad_accum"] < cfg.train.oom.max_grad_accum:
          bs_new = max(current_state["batch_size"] // 2, cfg.train.oom.min_batch)
          accum_new = min(math.ceil(G_old / bs_new), cfg.train.oom.max_grad_accum)
          G_new = bs_new * accum_new
          
          if G_new == G_old:  # ê¸€ë¡œë²Œ ë°°ì¹˜ ìœ ì§€ë¨
              logger.warning(f"OOM Recovery S2: Microbatching bs={bs_new}, accum={accum_new} (global batch preserved: {G_old})")
              return {
                  "action": "microbatching",
                  "batch_size": bs_new,
                  "grad_accum": accum_new,
                  "preserve_schedules": True,  # LR/WD/EMA ë³€ê²½ ì—†ìŒ
                  "replay_batch": True
              }
      
      # S3: ê¸€ë¡œë²Œ ë°°ì¹˜ ë³€ê²½ (ë¶ˆê°€í”¼í•  ë•Œë§Œ)
      bs_new = max(current_state["batch_size"] // 2, cfg.train.oom.min_batch)
      accum_new = min(cfg.train.oom.max_grad_accum, math.ceil(G_old / bs_new))
      G_new = bs_new * accum_new
      
      # í•™ìŠµ ì¼ê´€ì„± ë³´ì¥: ìƒ˜í”Œ ê¸°ì¤€ ìŠ¤ì¼€ì¤„ë§
      lr_scale = G_new / G_old  # Linear Scaling Rule
      steps_per_epoch_ratio = G_old / G_new
      
      logger.warning(f"OOM Recovery S3: Global batch change G_old={G_old}â†’G_new={G_new}, lr_scale={lr_scale:.4f}, by_samples scheduling")
      return {
          "action": "global_batch_change",
          "batch_size": bs_new,
          "grad_accum": accum_new,
          "lr_scale": lr_scale,
          "wd_scale": steps_per_epoch_ratio,  # ì—í­ë‹¹ WD ì´ëŸ‰ ìœ ì§€
          "scheduler_mode": "by_samples",     # ìƒ˜í”Œ ê¸°ì¤€ ì „í™˜
          "replay_batch": True,               # ë™ì¼ ì‹œë“œë¡œ ì¬ì‹¤í–‰
          "audit_log": {"G_old": G_old, "G_new": G_new, "lr_scale": lr_scale}
      }
      
      # S3: grad_accum ì¦ê°€ (ìƒí•œ ì²´í¬)
      if grad_accum < cfg.train.oom.max_grad_accum:  # ê¸°ë³¸ 8
          grad_accum = min(grad_accum * 2, cfg.train.oom.max_grad_accum)
          return retries, did_empty_cache, did_amp_escalate, grad_accum, batch_size
      
      # S4: batch_size ê°ì†Œ (í•˜í•œ ì²´í¬)
      if batch_size > cfg.train.oom.min_batch:  # ê¸°ë³¸ 1
          batch_size = max(batch_size // 2, cfg.train.oom.min_batch)
          rebuild_dataloader(batch_size)
          return retries, did_empty_cache, did_amp_escalate, grad_accum, batch_size
      
      fail("min_batch & max_grad_accum reached")
  ```
- **OOM í´ë°± í•µì‹¬ ì›ì¹™**:
  - **ìƒí•œ ê³ ì •**: max_retries=4, max_grad_accum=8, min_batch=1
  - **ìš°ì„ ìˆœìœ„**: ê¸€ë¡œë²Œ ë°°ì¹˜ ë³´ì¡´ â†’ ì„ í˜• ìŠ¤ì¼€ì¼ë§ â†’ ìµœí›„ ë°°ì¹˜ ì¶•ì†Œ
  - **AMP ì •ì±…**: auto(bf16>fp16) ê¸°ë³¸, OOM ì‹œ 1íšŒ fp16 ê°•ì œë§Œ
  - **ë¡œê¹… ê°•í™”**: ê° ë‹¨ê³„ ì§„ì…/ì„±ê³µ/ì‹¤íŒ¨ë¥¼ ìƒì„¸ ë¡œê·¸+TB(oom/retries)

- **config.yaml í•„ìˆ˜ OOM ê°€ë“œë ˆì¼ í‚¤** (ì—†ìœ¼ë©´ ì‹¤íŒ¨):
  ```yaml
  train:
    oom:
      max_retries: 4              # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
      max_grad_accum: 8           # ìµœëŒ€ gradient accumulation
      min_batch: 1                # ìµœì†Œ ë°°ì¹˜ í¬ê¸°
      cooldown_sec: 2             # ì¬ì‹œë„ ê°„ ëŒ€ê¸° ì‹œê°„
      escalate_to_fp16: true      # AMP fp16 ê°•ì œ ì „í™˜ í—ˆìš©
  ```

D-9) ë¦¬í¬íŠ¸/ì‹œê°í™”

- src/evaluate.py:
  - í˜¼ë™í–‰ë ¬(cm) ì €ì¥, ROC/PR ì»¤ë¸Œ(ë‹¤ì¤‘ í´ë˜ìŠ¤ë©´ macro/one-vs-rest) ì €ì¥
  - reports/metrics.jsonì— epoch-level ëˆ„ì /ìµœì‹  ì €ì¥(save_json)
- scripts/train.sh:
  - ì¢…ë£Œ í›„ "SUCCESS"ë©´ {exp_dir}/reports/metrics.jsonì˜ ìµœì‹  ì§€í‘œ íŒŒì¼ ê²½ë¡œë¥¼ echo
  - hard_cases/*.jsonl ë° ì…ë ¥ ì¸ë„¤ì¼/í¬ë¡­(ì˜µì…˜)ì„ ë³´ì¡´í•˜ì—¬ ì£¼ê°„ ë¶„ì„ì— í™œìš©

D-10) ë¡œê¹…/í”„ë¡œíŒŒì¼

- ë¡œê·¸ ë¼ì¸ ê³µí†µ í¬ë§·:
  [train] e={epoch}/{max} s={step}/{len} loss={:.4f} lr={:.2e} data={:.3f}s comp={:.3f}s thr={:.1f}/s cuda={:.0f}MB
- profile_interval(ê¸°ë³¸ 50 step)ë§ˆë‹¤ ìƒì„¸ ë¼ì¸ ë¤í”„
- tensorboard ë””ë ‰í„°ë¦¬: {exp_dir}/tb (TBWriterê°€ ìë™ ìƒì„±)
  - (ì„¤ì¹˜ í•„ìš”) pip install tensorboard

D-11) ì¬í˜„ì„±/ì„±ëŠ¥ ëª¨ë“œ

- set_seed(cfg.train.seed, cfg.train.deterministic)
- torch.use_deterministic_algorithms(cfg.train.deterministic, warn_only=True)
- deterministic=False(ì„±ëŠ¥ ëª¨ë“œ)ì¼ ë•Œ cudnn.benchmark=True
- Trueì¼ ë•Œ cudnn.deterministic=True & benchmark=False

D-12) ë‹¨ìˆœ Go/No-Go í‰ê°€ ì‹œìŠ¤í…œ

```python
STAGE_TARGETS = {
  1: {"min_accuracy": 0.78, "max_latency_ms": 220, "min_coverage": 0.20,
      "min_eval_samples": 2000, "max_acc_drop": 0.02, "epsilon_acc": 0.002},
  2: {"min_accuracy": 0.82, "max_latency_ms": 220, "min_coverage": 0.40,
      "min_eval_samples": 5000, "max_acc_drop": 0.02, "epsilon_acc": 0.002},
  3: {"min_accuracy": 0.85, "max_latency_ms": 200, "min_coverage": 0.70,
      "min_eval_samples": 10000, "max_acc_drop": 0.01, "epsilon_acc": 0.002},
  4: {"min_accuracy": 0.85, "max_latency_ms": 200, "min_coverage": 0.70,
      "min_eval_samples": 10000, "max_acc_drop": 0.01, "epsilon_acc": 0.002}
}

def simple_stage_evaluation(stage: int, results: dict, baseline: dict = None) -> tuple[bool, list[str]]:
    """Go/No-Go í‰ê°€ with 5ê°œ í™•ì¥ í¬ì¸íŠ¸"""
    t = STAGE_TARGETS[stage]
    reasons = []
    
    # 1. ì§€ì—°ì‹œê°„ ê¸°ì¤€ (p95)
    p95 = results.get("latency_p95_ms", float("inf"))
    ok_lat = p95 <= t.get("max_latency_ms", float("inf"))
    if not ok_lat: reasons.append("LATENCY_P95_TOO_HIGH")
    
    # 2. í‘œë³¸/ì»¤ë²„ë¦¬ì§€
    n = int(results.get("n_eval_samples", 0))
    ok_n = n >= t.get("min_eval_samples", 0)
    if not ok_n: reasons.append("NOT_ENOUGH_SAMPLES")
    
    cov = results.get("class_coverage", 0.0)
    ok_cov = cov >= t.get("min_coverage", 0.0)
    if not ok_cov: reasons.append("COVERAGE_TOO_LOW")
    
    # 3. ë¦¬ê·¸ë ˆì…˜ ê°€ë“œ
    acc = results.get("accuracy", 0.0)
    ok_reg = True
    if baseline and "accuracy" in baseline:
        max_drop = t.get("max_acc_drop", 0.0)
        if acc < baseline["accuracy"] - max_drop:
            ok_reg = False
            reasons.append("REGRESSION_TOO_LARGE")
    
    # 4. íˆìŠ¤í…Œë¦¬ì‹œìŠ¤
    epsilon = t.get("epsilon_acc", 0.0)
    ok_acc = acc >= (t["min_accuracy"] - epsilon)
    if not ok_acc: reasons.append("ACC_BELOW_TARGET")
    
    # 5. ì‚¬ìœ  ë°˜í™˜
    ok_crash = bool(results.get("no_crashes", False))
    if not ok_crash: reasons.append("CRASH_DETECTED")
    
    ok_mem = bool(results.get("memory_stable", False))
    if not ok_mem: reasons.append("MEMORY_UNSTABLE")
    
    go = all([ok_lat, ok_n, ok_cov, ok_reg, ok_acc, ok_crash, ok_mem])
    return go, reasons
```

D-13) Stageë³„ í•˜ë“œì›¨ì–´ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ

**Stageë³„ í•˜ë“œì›¨ì–´ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ ë¡œì§**:
```python
# src/utils.py - apply_stage_overrides í•¨ìˆ˜
def apply_stage_overrides(cfg, current_stage: int) -> dict:
    """Stageë³„ í•˜ë“œì›¨ì–´ ì„¤ì •ì„ ë™ì ìœ¼ë¡œ ì ìš©"""
    stage_key = f"stage_{current_stage}"
    stage_overrides = cfg.train.get("stage_overrides", {}).get(stage_key, {})
    
    if not stage_overrides:
        logger.info(f"Stage {current_stage}: No overrides configured")
        return cfg
    
    original_cfg = deepcopy(cfg)
    applied_changes = []
    
    # ë°°ì¹˜ í¬ê¸° ì˜¤ë²„ë¼ì´ë“œ
    if "batch_size_override" in stage_overrides:
        override_bs = stage_overrides["batch_size_override"]
        cfg.train.batch_size = override_bs
        applied_changes.append(f"batch_size: {override_bs}")
    
    # ì»´íŒŒì¼ ëª¨ë“œ ì˜¤ë²„ë¼ì´ë“œ
    if "compile_mode" in stage_overrides:
        override_mode = stage_overrides["compile_mode"]
        cfg.optimization.torch_compile = override_mode
        applied_changes.append(f"torch_compile: {override_mode}")
    
    # ë©”ëª¨ë¦¬ í¬ë§· ì˜¤ë²„ë¼ì´ë“œ
    if "memory_format" in stage_overrides:
        override_format = stage_overrides["memory_format"]
        cfg.optimization.channels_last = (override_format == "channels_last")
        applied_changes.append(f"channels_last: {override_format == 'channels_last'}")
    
    # ì›Œì»¤ ìˆ˜ ì˜¤ë²„ë¼ì´ë“œ
    if "num_workers_override" in stage_overrides:
        override_workers = stage_overrides["num_workers_override"]
        cfg.dataloader.num_workers = override_workers
        applied_changes.append(f"num_workers: {override_workers}")
    
    logger.info(f"Stage {current_stage} overrides applied: {', '.join(applied_changes)}")
    return cfg
```

**Stageë³„ ìµœì í™” ì „ëµ**:
```python
# src/train.py - main í•¨ìˆ˜ ë‚´ Stage ì˜¤ë²„ë¼ì´ë“œ ì ìš©
def main(cfg):
    current_stage = cfg.data.progressive_validation.current_stage
    
    # Stageë³„ í•˜ë“œì›¨ì–´ ì„¤ì • ì ìš©
    cfg = apply_stage_overrides(cfg, current_stage)
    
    # Stageë³„ íŠ¹í™” ìµœì í™”
    if current_stage <= 2:  # ì‘ì€ ë°ì´í„°ì…‹
        # ì‘ì€ ë°°ì¹˜, ë¹ ë¥¸ ì»´íŒŒì¼, ì ì€ ì›Œì»¤
        enable_small_dataset_optimizations(cfg)
    elif current_stage >= 3:  # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹
        # í° ë°°ì¹˜, ìµœëŒ€ ìµœì í™”, ë§ì€ ì›Œì»¤
        enable_large_dataset_optimizations(cfg)
    
    logger.info(f"Stage {current_stage} hardware optimization enabled")

def enable_small_dataset_optimizations(cfg):
    """Stage 1-2: ì‘ì€ ë°ì´í„°ì…‹ ìµœì í™”"""
    # ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ì„ ìœ„í•œ ì„¤ì •
    if cfg.train.detection.auto_batch_max > 64:
        cfg.train.detection.auto_batch_max = 64  # ì‘ì€ ë°°ì¹˜
    if cfg.train.classification.auto_batch_max > 96:
        cfg.train.classification.auto_batch_max = 96
    
    # ì»´íŒŒì¼ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
    if cfg.optimization.torch_compile == "max-autotune":
        cfg.optimization.torch_compile = "reduce-overhead"
    
    logger.info("Small dataset optimizations applied")

def enable_large_dataset_optimizations(cfg):
    """Stage 3-4: ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ìµœì í™”"""
    # ìµœëŒ€ ì„±ëŠ¥ì„ ìœ„í•œ ì„¤ì •
    cfg.optimization.torch_compile = "max-autotune"
    cfg.optimization.channels_last = True
    
    # VRAM ìµœëŒ€ í™œìš©
    if cfg.train.auto_batch_vram_headroom < 0.90:
        cfg.train.auto_batch_vram_headroom = 0.92
    
    logger.info("Large dataset optimizations applied")
```

D-14) Stageë³„ ì°¨ë³„í™”ëœ í‰ê°€ ê¸°ì¤€ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)

[Stage 1: íŒŒì´í”„ë¼ì¸ ê²€ì¦ (5,000ì¥, 50í´ë˜ìŠ¤)]
```yaml
mandatory_checks:
  - pipeline_complete: true           # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì—ëŸ¬ ì—†ì´ ì™„ë£Œ
  - detection_model_loading: true     # YOLOv11x ëª¨ë¸ ì •ìƒ ë¡œë”©
  - classification_model_loading: true # EfficientNetV2-L ëª¨ë¸ ì •ìƒ ë¡œë”©
  - onnx_conversion_success: true     # ë‘ ëª¨ë¸ ëª¨ë‘ ONNX ë³€í™˜ ì„±ê³µ

performance_targets:
  detection:
    mAP_0.5: â‰¥0.30                  # ê¸°ë³¸ ê²€ì¶œ ê°€ëŠ¥ì„± í™•ì¸
    inference_time_ms: â‰¤50          # RTX 5080ì—ì„œ 640px ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥ì„±
  classification:
    accuracy: â‰¥0.40                 # 50í´ë˜ìŠ¤ ê¸°ì¤€ (ë¬´ì‘ìœ„ 2% Ã— 20ë°°)
    inference_time_ms: â‰¤10          # 384px ë¶„ë¥˜ ì‹¤ì‹œê°„ ê°€ëŠ¥
  system:
    gpu_memory_gb: â‰¤14              # VRAM ì•ˆì •ì„± í™•ì¸
    data_loading_s: â‰¤2              # 128GB RAM í™œìš©ë„ ê²€ì¦

recommendation_thresholds:
  RECOMMEND_PROCEED: all_mandatory + all_performance_targets
  SUGGEST_OPTIMIZE: performance >= 70% of targets
  WARN_STOP: performance < 70% of targets
```

[Stage 2: ê¸°ë³¸ ì„±ëŠ¥ (25,000ì¥, 250í´ë˜ìŠ¤)]
```yaml
mandatory_checks:
  - auto_batch_tuning_success: true   # Auto Batch íŠœë‹ ì„±ê³µ
  - tensorboard_logging: true         # TensorBoard ë¡œê¹… ì •ìƒ
  - class_balance_verification: true  # í´ë˜ìŠ¤ ê· í˜• ê²€ì¦ í†µê³¼
  - memory_optimization_working: true # 128GB RAM ìµœì í™” ë™ì‘

performance_targets:
  detection:
    mAP_0.5: â‰¥0.50                  # ê¸°ë³¸ ê²€ì¶œ ì„±ëŠ¥
    mAP_0.5_0.95: â‰¥0.35             # COCO í‘œì¤€ ì§€í‘œ
    precision: â‰¥0.45                # ê²€ì¶œ ì •ë°€ë„
    recall: â‰¥0.40                   # ê²€ì¶œ ì¬í˜„ìœ¨
  classification:
    accuracy: â‰¥0.60                 # 250í´ë˜ìŠ¤ ëŒ€ìƒ í–¥ìƒëœ ì„±ëŠ¥
    macro_f1: â‰¥0.55                 # í´ë˜ìŠ¤ ë¶ˆê· í˜• í™˜ê²½ì—ì„œì˜ ì‹¤ì œ ì„±ëŠ¥
    top5_accuracy: â‰¥0.80            # ìƒìœ„ 5ê°œ í›„ë³´ ì •í™•ë„
  optimization:
    batch_size_achieved: â‰¥8         # RTX 5080 ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
    throughput_img_s: â‰¥100          # ì²˜ë¦¬ëŸ‰ ê¸°ì¤€

advanced_metrics:
  - class_imbalance_ratio: â‰¤10:1    # í´ë˜ìŠ¤ ë¶ˆê· í˜• ê´€ë¦¬
  - convergence_stability: loss_variance < 0.1
  - memory_efficiency: peak_usage/avg_usage < 1.5
```

[Stage 3: í™•ì¥ì„± í…ŒìŠ¤íŠ¸ (100,000ì¥, 1,000í´ë˜ìŠ¤)]
```yaml
mandatory_checks:
  - scalability_test_pass: true      # í™•ì¥ì„± í…ŒìŠ¤íŠ¸ í†µê³¼
  - multi_batch_processing: true     # ë‹¤ì¤‘ ë°°ì¹˜ ì²˜ë¦¬ ê°€ëŠ¥
  - memory_leak_test: true           # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ìŒ
  - stability_test_pass: true        # ì¥ì‹œê°„ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸

performance_targets:
  detection:
    mAP_0.5: â‰¥0.70                  # ë†’ì€ ê²€ì¶œ ì„±ëŠ¥
    mAP_0.5_0.95: â‰¥0.50             # ê³ ê¸‰ ê²€ì¶œ ì„±ëŠ¥
    recall_0.5: â‰¥0.75               # ê²€ì¶œìœ¨
    precision_0.5: â‰¥0.70            # ê²€ì¶œ ì •ë°€ë„
  classification:
    accuracy: â‰¥0.75                 # 1,000í´ë˜ìŠ¤ ëŒ€ìƒ ë†’ì€ ì„±ëŠ¥
    macro_f1: â‰¥0.70                 # ë¶ˆê· í˜• í™˜ê²½ì—ì„œ ë†’ì€ ì„±ëŠ¥
    top5_accuracy: â‰¥0.90            # ë†’ì€ í›„ë³´ ì •í™•ë„
    per_class_min_f1: â‰¥0.30         # ìµœì†Œ í´ë˜ìŠ¤ ì„±ëŠ¥ ë³´ì¥
  scalability:
    multi_batch_size: â‰¥32           # ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨ì„±
    memory_stable_hours: â‰¥12        # ì¥ì‹œê°„ ì•ˆì •ì„±
    api_response_time: â‰¤100ms       # API ì‘ë‹µ ì‹œê°„
```

[Stage 4: ìµœì¢… í”„ë¡œë•ì…˜ (500,000ì¥, 5,000í´ë˜ìŠ¤)]
```yaml
mandatory_checks:
  - full_dataset_processing: true    # ì „ì²´ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì™„ë£Œ
  - production_api_deployment: true  # í”„ë¡œë•ì…˜ API ë°°í¬ ì„±ê³µ
  - cloudflare_tunnel_setup: true    # Cloudflare Tunnel ì„¤ì • ì™„ë£Œ
  - automated_monitoring: true       # ìë™ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ë™ì‘

performance_targets:
  detection:
    mAP_0.5: â‰¥0.85                  # PART_A ìµœì¢… ëª©í‘œ
    mAP_0.5_0.95: â‰¥0.65             # ìµœê³  ìˆ˜ì¤€ ê²€ì¶œ ì„±ëŠ¥
    recall_0.5: â‰¥0.90               # ìµœê³  ìˆ˜ì¤€ ê²€ì¶œìœ¨
    precision_0.5: â‰¥0.85            # ìµœê³  ìˆ˜ì¤€ ì •ë°€ë„
  classification:
    accuracy: â‰¥0.92                 # PART_A ìµœì¢… ëª©í‘œ
    macro_f1: â‰¥0.88                 # ì¡°ê¸°ì¢…ë£Œ ì§€í‘œ í†µì¼
    top5_accuracy: â‰¥0.98            # ê±°ì˜ ì™„ë²½í•œ top-5 ì„±ëŠ¥
    top1_per_class: â‰¥0.85           # ëª¨ë“  í´ë˜ìŠ¤ì—ì„œ ë†’ì€ ì„±ëŠ¥
  production:
    inference_latency_ms: â‰¤30       # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤ì‹œê°„ ì„±ëŠ¥
    throughput_img_s: â‰¥200          # ë†’ì€ ì²˜ë¦¬ëŸ‰
    api_uptime: â‰¥99.9%              # ë†’ì€ ê°€ìš©ì„±
```

D-14) OptimizationAdvisor ê¶Œì¥ ì‹œìŠ¤í…œ (êµ¬ì²´ì  êµ¬í˜„)

```python
class OptimizationAdvisor:
    """í•™ìŠµ ì‹¤íŒ¨ ì‹œ êµ¬ì²´ì  ì¡°ì • ì œì•ˆ ì‹œìŠ¤í…œ"""
    
    def evaluate_and_recommend(self, stage: int, results: dict) -> dict:
        """ì„±ëŠ¥ í‰ê°€ í›„ ê¶Œì¥ì‚¬í•­ ìƒì„± â†’ ì‚¬ìš©ì ê²°ì •"""
        
        # ê³µí†µ ì¤‘ë‹¨ ê¸°ì¤€ (ëª¨ë“  Stage)
        if (results.get("repeated_oom", 0) > 5 or
            math.isnan(results.get("loss", 0)) or
            results.get("data_corruption_rate", 0) > 0.01):
            return self._critical_failure_recommendation()
        
        # Stageë³„ ì°¨ë³„í™”ëœ í‰ê°€
        stage_configs = get_stage_config(stage)
        
        # í•„ìˆ˜ ì²´í¬ í™•ì¸
        mandatory_pass = all(
            results["mandatory_checks"].get(check, False) 
            for check in stage_configs["mandatory_checks"]
        )
        
        if not mandatory_pass:
            return self._mandatory_failure_recommendation(stage, results, stage_configs)
        
        # ì„±ëŠ¥ ê¸°ì¤€ í™•ì¸ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±
        performance_score = calculate_performance_score(results, stage_configs)
        suggestions = self.diagnose_and_suggest(results, stage)
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„± (ìë™ ê²°ì • X, ì‚¬ìš©ì ì„ íƒê¶Œ ì œê³µ)
        if performance_score >= 1.0:  # ëª¨ë“  ëª©í‘œ ë‹¬ì„±
            recommendation = {
                "recommendation": "RECOMMEND_PROCEED",
                "color": "ğŸŸ¢", 
                "reason": "all_targets_met",
                "message": f"Stage {stage} ëª¨ë“  ëª©í‘œ ë‹¬ì„±!",
                "performance_score": performance_score,
                "user_options": [
                    "[1] Stage {stage+1}ë¡œ ì§„í–‰",
                    "[2] í˜„ì¬ Stageì—ì„œ ì¶”ê°€ ìµœì í™”", 
                    "[3] ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"
                ]
            }
        elif performance_score >= 0.7:  # 70% ì´ìƒ ë‹¬ì„±
            recommendation = {
                "recommendation": "RECOMMEND_PROCEED",
                "color": "ğŸŸ¢",
                "reason": "sufficient_performance", 
                "message": f"Stage {stage} ì¶©ë¶„í•œ ì„±ëŠ¥ ë‹¬ì„±",
                "performance_score": performance_score,
                "user_options": [
                    "[1] í˜„ì¬ ì„±ëŠ¥ìœ¼ë¡œ ë‹¤ìŒ Stage ì§„í–‰",
                    "[2] ê¶Œì¥ ìµœì í™” ì ìš© í›„ ì¬ì‹œë„",
                    "[3] ìˆ˜ë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •"
                ]
            }
        elif performance_score >= 0.5:  # 50% ì´ìƒ ë‹¬ì„±
            recommendation = {
                "recommendation": "SUGGEST_OPTIMIZE",
                "color": "ğŸŸ¡",
                "reason": "performance_below_target",
                "message": f"Stage {stage} ì„±ëŠ¥ ë¯¸ë‹¬. ìµœì í™” ê¶Œì¥",
                "suggestions": suggestions,
                "user_options": [
                    "[1] ê¶Œì¥ì‚¬í•­ ì ìš© í›„ ì¬ì‹œë„",
                    "[2] í˜„ì¬ ì„±ëŠ¥ìœ¼ë¡œ ì§„í–‰ (ìœ„í—˜)",
                    "[3] ìƒì„¸ ë””ë²„ê¹… ëª¨ë“œ"
                ]
            }
        else:  # 50% ë¯¸ë§Œ
            recommendation = {
                "recommendation": "WARN_STOP",
                "color": "ğŸ”´", 
                "reason": "performance_too_low",
                "message": f"Stage {stage} ì„±ëŠ¥ì´ ë§¤ìš° ë‚®ìŒ",
                "performance_score": performance_score,
                "suggestions": suggestions,
                "user_options": [
                    "[1] ê°•ë ¥í•œ ìµœì í™” ì ìš© í›„ ì¬ì‹œë„",
                    "[2] ì•„í‚¤í…ì²˜ ì¬ê²€í† ",
                    "[3] ë°ì´í„° í’ˆì§ˆ ì ê²€"
                ]
            }
        
        # ì‚¬ìš©ìì—ê²Œ ì„ íƒê¶Œ ì œê³µ
        return self._present_recommendation_to_user(recommendation)
    
    def _present_recommendation_to_user(self, recommendation: dict) -> dict:
        """í„°ë¯¸ë„ì— ê¶Œì¥ì‚¬í•­ ì¶œë ¥ ë° ì‚¬ìš©ì ì„ íƒ ëŒ€ê¸°"""
        
        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ¯ Stage í‰ê°€ ì™„ë£Œ                             â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ {recommendation['color']} {recommendation['message']}
â•‘ 
â•‘ ğŸ“Š ì„±ëŠ¥ ì ìˆ˜: {recommendation.get('performance_score', 'N/A'):.3f}
â•‘ 
â•‘ ğŸ’¡ ê¶Œì¥ì‚¬í•­:
""")
        
        if 'suggestions' in recommendation:
            for i, suggestion in enumerate(recommendation['suggestions'][:3], 1):
                print(f"â•‘   {i}. {suggestion}")
        
        print("â•‘")
        print("â•‘ ğŸ­ ì„ íƒ ì˜µì…˜:")
        for option in recommendation['user_options']:
            print(f"â•‘   {option}")
        
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        user_choice = input("\nì„ íƒí•˜ì„¸ìš” [1-3]: ")
        
        return {
            "recommendation": recommendation["recommendation"],
            "user_choice": user_choice,
            "suggestions": recommendation.get("suggestions", []),
            "performance_score": recommendation.get("performance_score", 0)
        }

def generate_optimization_suggestions(results: dict, stage: int) -> list:
    """ì„±ëŠ¥ ê¸°ë°˜ ìµœì í™” ì œì•ˆ ìƒì„±"""
    suggestions = []
    
    # ë©”ëª¨ë¦¬ ê´€ë ¨
    if results.get("gpu_memory_gb", 0) > 14:
        suggestions.append("ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ (í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ)")
    
    # ê²€ì¶œ ì„±ëŠ¥ ê´€ë ¨
    detection_map = results.get("detection_mAP_0.5", 0)
    if detection_map < get_stage_target(stage, "detection.mAP_0.5") * 0.8:
        suggestions.append("ê²€ì¶œ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (í•™ìŠµë¥ , ì•µì»¤ í¬ê¸°)")
        suggestions.append("ë°ì´í„° ì¦ê°• ê°•í™” ê³ ë ¤")
    
    # ë¶„ë¥˜ ì„±ëŠ¥ ê´€ë ¨  
    classification_acc = results.get("classification_accuracy", 0)
    if classification_acc < get_stage_target(stage, "classification.accuracy") * 0.8:
        suggestions.append("ë¶„ë¥˜ ëª¨ë¸ ì •ê·œí™” ê°•í™” (ë“œë¡­ì•„ì›ƒ, ë ˆì´ë¸” ìŠ¤ë¬´ë”©)")
        suggestions.append("í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘ ê°•í™” (ê°€ì¤‘ ìƒ˜í”Œë§)")
    
    # ì‹œìŠ¤í…œ ì„±ëŠ¥ ê´€ë ¨
    if results.get("data_loading_time_s", 0) > 3:
        suggestions.append("ë°ì´í„° ë¡œë” ì›Œì»¤ ìˆ˜ ì¡°ì •")
        suggestions.append("LMDB ìºì‹œ í™œìš© í™•ëŒ€")
    
    return suggestions
```

D-12) CLI/ì˜¤ë²„ë¼ì´ë“œ

- python -m src.train --cfg config.yaml train.batch_size=128 dataloader.num_workers=12 train.resume=last
- ê·œì¹™: CLI > config.yaml > .env(.envëŠ” API/ì„œë¹™ìš©)

[Annex 1 â€” Auto Batch Tuner (í•„ìˆ˜)]

- ì…ë ¥: init_bs, min=1, max=init_bs*4 (ë©”ëª¨ë¦¬ ì—¬ìœ  ì‹œ ê°€ë³€)
- ì•Œê³ ë¦¬ì¦˜: ì›Œë°ì—…(ëª‡ step) â†’ ì‹œë„ â†’ OOM ì‹œ empty_cacheâ†’fp16 ê°•ì œâ†’accumâ†‘â†’batchâ†“ â†’ ì„±ê³µ ì‹œ ìƒí–¥ íƒìƒ‰
- ì¶œë ¥: í™•ì • batch_size; TB(perf/auto_batch_found), ë¡œê·¸ì— í›„ë³´/ì‹œë„/ê²°ì • ì‚¬ìœ 

[Annex 2 â€” Worker Autotune (í•„ìˆ˜)]

- í›„ë³´: [4,8,12,16] (configì—ì„œ ê°€ì ¸ì™€ë„ ë¨)
- ê° í›„ë³´ë¡œ 50~100 step data_time í‰ê·  ì¸¡ì • â†’ ìµœì†Ÿê°’ ì„ íƒ
- TB(perf/num_workers), ë¡œê·¸ì— í›„ë³´ë³„ ê²°ê³¼ì™€ ì„ íƒê°’

[Annex 3 â€” AMP/TF32/compile/channels_last ì •ì±… (ê°•ì œ)]

- AMP: autocast+Scaler, "auto"ë©´ bf16 ìš°ì„  â†’ ë¯¸ì§€ì› ì‹œ fp16
- TF32: allow_tf32=True + matmul_precision("high")
- channels_last: ëª¨ë¸Â·ì…ë ¥ ëª¨ë‘ ë©”ëª¨ë¦¬ í¬ë§· ì ìš©(ê°€ëŠ¥í•  ë•Œë§Œ)
- torch.compile: 3ê°€ì§€ ëª¨ë“œë§Œ ì§€ì› ("none", "reduce-overhead", "max-autotune"), ì‹¤íŒ¨ ì‹œ ê²½ê³  í›„ ì¼ë°˜ ëª¨ë¸ ì‚¬ìš©

[Annex 5 â€” EMA (ì˜µì…˜)]

- enabled=Trueë©´ timm.utils.ModelEmaV2 ë˜ëŠ” ë™ë“± ë˜í¼ ì‚¬ìš©(decayâ‰ˆ0.9998)
- í‰ê°€/ì €ì¥ì€ EMA ê°€ì¤‘ì¹˜ ê¸°ì¤€(ì˜µì…˜: raw/ema ë‘˜ ë‹¤ ê¸°ë¡)

[Annex 6 â€” OOM í´ë°± ìƒíƒœ ë¨¸ì‹  (ê°•ì œ)]

- ìˆœì„œ/ë¡œê·¸/TB íƒœê·¸ë¥¼ ìœ„ â€œD-8â€ ê·œì¹™ëŒ€ë¡œ êµ¬í˜„

[Annex 7 â€” ì„±ëŠ¥ ë¡œê¹… ìŠ¤í‚¤ë§ˆ (ê°•ì œ)]

- ìŠ¤í…ë³„: step, epoch, loss, lr, data_time, compute_time, throughput(img/s), cuda_mem(MB)
- TB íƒœê·¸: train/loss, train/lr, perf/data_time, perf/compute_time, perf/throughput, perf/cuda_mem
- ê²€ì¦: val/loss, val/accuracy, val/macro_f1, (ì˜µì…˜) val/top5

[Annex 8 â€” ì²´í¬í¬ì¸íŠ¸ ìŠ¤í‚¤ë§ˆ (ê°•ì œ)]

- best.pt / last.pt
- í¬í•¨: model_state, optimizer_state, scheduler_state, scaler_state, epoch, best_metric, config_snapshot
- resume ë¡œë”© ì‹œ key ë¶ˆì¼ì¹˜/ë°±ë³¸ ì°¨ì´ ê²€ì¦ ë° ì¹œì ˆí•œ ì—ëŸ¬

[ì‹¤í–‰ ëª…ë ¹ ì˜ˆì‹œ]
$ bash scripts/core/setup_venv.sh
$ bash scripts/train.sh

# ì¬ê°œ

$ python -m src.train --cfg config.yaml train.resume=last

# í•˜ì´í¼ ì˜¤ë²„ë¼ì´ë“œ

$ python -m src.train --cfg config.yaml train.batch_size=128 dataloader.num_workers=12

# ë¹ ë¥¸ ìŠ¤ëª¨í¬(1~2 epoch)ë¡œ íŒŒì´í”„ë¼ì¸ ì ê²€ ê¶Œì¥

[ì„±ê³µ íŒì •(ì´ íŒŒíŠ¸ ì™„ë£Œ ê¸°ì¤€)]

- exp_dir/logs/train.out|err ìƒì„± ë° í•™ìŠµ ë¡œê·¸ê°€ ìœ„ í¬ë§·ìœ¼ë¡œ ì¶œë ¥ë¨
- exp_dir/checkpoints/{last.pt, best.pt} ìƒì„±
- exp_dir/reports/metrics.json ê°±ì‹ (ìµœì†Œ acc/macro_f1/val_loss í¬í•¨)
- exp_dir/tb/ í•˜ìœ„ì— TensorBoard ì´ë²¤íŠ¸ íŒŒì¼ ìƒì„±
- Stage 4 ì™„ë£Œì‹œ exp_dir/reports/final_test_results.json ìƒì„±
- OOMì´ë‚˜ ë°°ì¹˜/ì›Œì»¤ ì¡°ì • ë¡œê·¸ê°€ ë‚¨ê³ , ì‹¤íŒ¨í•´ë„ ì¹œì ˆí•œ ë©”ì‹œì§€ë¡œ ì¢…ë£Œ

### Stage 1 ì‹œê°„ ìº¡ ì„±ê³µ íŒì •(ëª…ì‹œ)

- config.yamlì˜ `data.progressive_validation.stage_1`ì— ë‹¤ìŒì´ ëª¨ë‘ ì¶©ì¡±ë˜ë©´ â€œì„±ê³µ(íŒŒì´í”„ë¼ì¸ ê²€ì¦ í†µê³¼)â€ë¡œ ê°„ì£¼í•œë‹¤:
  - `allow_success_on_time_cap: true`
  - ì‹¤ì œ ì‹¤í–‰ ì‹œê°„ì´ `time_limit_hours`ì— ë„ë‹¬í•˜ì—¬ ì¡°ê¸° ì¢…ë£Œ
  - ì²˜ë¦¬ ìƒ˜í”Œ ìˆ˜ê°€ `min_samples_required` ì´ìƒ
  - ì»¤ë²„ëœ í´ë˜ìŠ¤ ìˆ˜ê°€ `min_class_coverage` ì´ìƒ
  - í•„ìˆ˜ ì²´í¬(mandatory_checks): pipeline_complete, classification_model_loading(OK)
  - ì§€í‘œ ì¸¡ì •ì´ ê°€ëŠ¥í•˜ë©´ ê¸°ë¡í•˜ë˜, ë¯¸ë‹¬ì´ì–´ë„ ì‹œê°„ ìº¡ ì„±ê³µì€ ìœ ì§€

## ğŸ¯ **PART_D í•µì‹¬ ì—…ë°ì´íŠ¸ ì™„ë£Œ**

### âœ… **ì¡°ê±´ë¶€ Two-Stage í•™ìŠµ íŒŒì´í”„ë¼ì¸**
- **ë‹¨ì¼ ì•½í’ˆ**: EfficientNetV2-S ì§ì ‘ ë¶„ë¥˜ (384px, 5000í´ë˜ìŠ¤)
- **ì¡°í•© ì•½í’ˆ**: YOLOv11m ê²€ì¶œ â†’ í¬ë¡­ â†’ EfficientNetV2-S ë¶„ë¥˜  
- **ì¡°ê±´ë¶€ ì „í™˜**: ì‚¬ìš©ì ì„ íƒ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ë¶„ê¸°
- **Commercial ì•„í‚¤í…ì²˜**: í•¨ìˆ˜ ê¸°ë°˜ ìƒˆ ì»´í¬ë„ŒíŠ¸ ì¶”ê°€ ì™„ë£Œ

### âœ… **RTX 5080 16GB ìµœì í™”**
- **Auto Batch**: OOM ê°ì§€ â†’ ìë™ ë°°ì¹˜ í¬ê¸° ì¡°ì • (ê²€ì¶œ 16, ë¶„ë¥˜ 64)
- **Mixed Precision**: AMP auto (bfloat16 > fp16), TF32 í™œì„±í™”
- **Memory Format**: channels_last ìµœì í™”, torch.compile("reduce-overhead")
- **OOM í´ë°±**: empty_cache â†’ fp16 ê°•ì œ â†’ grad_accumâ†‘ â†’ batchâ†“

### âœ… **128GB RAM í™œìš©**
- **Worker Autotune**: 16 ìŠ¤ë ˆë“œ ìµœì  í™œìš©, data_time ìµœì†Œí™”
- **Prefetch**: prefetch_factor=8, persistent_workers=True
- **Memory Pin**: pin_memory_device="cuda" RTX 5080 ì§ì ‘ í•€

### âœ… **ì•ˆì •ì„± & ê°€ì‹œì„±**
- **ì²´í¬í¬ì¸íŠ¸**: best.pt/last.pt ìë™ ì €ì¥, ì¬ê°œ ì§€ì›
- **TensorBoard**: ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (mAP, accuracy, loss, throughput)
- **ë¡œê¹…**: ë‹¨ê³„ë³„ data_time, compute_time, VRAM ì‚¬ìš©ëŸ‰

**âœ… PART_D ì™„ë£Œ: RTX 5080 ìµœì í™”ëœ ì¡°ê±´ë¶€ Two-Stage í•™ìŠµ íŒŒì´í”„ë¼ì¸**
