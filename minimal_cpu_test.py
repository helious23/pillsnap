#!/usr/bin/env python3
"""
ìµœì†Œ CPU í…ŒìŠ¤íŠ¸ - ì‘ì€ ëª¨ë¸ ì‚¬ìš©
"""
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
from pathlib import Path
import time
import json

# í™˜ê²½ ì„¤ì •
torch.set_num_threads(1)
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"


class SimpleModel(nn.Module):
    """ë§¤ìš° ì‘ì€ í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸"""

    def __init__(self, num_classes=19):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
        )
        self.classifier = nn.Linear(32, num_classes)

    def forward(self, x):
        x = self.features(x)
        return self.classifier(x)


def main():
    print("ğŸš€ Minimal CPU Test")
    start_time = time.time()

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    run_tag = f"minimal_cpu_{int(time.time())}"
    output_dir = Path(f"artifacts/cpu_runs/{run_tag}")
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“ Output: {output_dir}")

    # ë°ì´í„°ì…‹ ë¡œë“œ
    from pillsnap.stage2.dataset_cls import PillsnapClsDataset

    transform = transforms.Compose(
        [
            transforms.Resize((64, 64)),  # ë§¤ìš° ì‘ì€ í¬ê¸°
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )

    dataset = PillsnapClsDataset(
        "artifacts/manifest_enriched.csv",
        "artifacts/classes_step11.json",
        transform=transform,
        require_exists=False,
    )

    print(f"ğŸ“Š Dataset: {len(dataset)} samples")

    # ìµœì†Œ ë°ì´í„°ë¡œë”
    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)

    # ì‘ì€ ëª¨ë¸ ìƒì„±
    model = SimpleModel(num_classes=19)
    print(f"ğŸ”§ Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜
    optimizer = optim.SGD(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()

    print("ğŸƒ Starting minimal training...")

    # ë§¤ìš° ì§§ì€ í•™ìŠµ
    model.train()
    losses = []

    for batch_idx, (images, targets) in enumerate(loader):
        if batch_idx >= 3:  # 3ê°œ ìƒ˜í”Œë§Œ ì²˜ë¦¬
            break

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        losses.append(loss.item())
        print(
            f"  Sample {batch_idx+1}: loss={loss.item():.4f}, target={targets.item()}"
        )

    avg_loss = sum(losses) / len(losses)
    print(f"âœ… Training completed: avg_loss={avg_loss:.4f}")

    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    checkpoint = {
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "train_loss": avg_loss,
        "num_classes": 19,
        "model_type": "SimpleModel",
    }

    torch.save(checkpoint, output_dir / "minimal_checkpoint.pt")

    # ë©”íŠ¸ë¦­ ì €ì¥
    metrics = {
        "training_completed": True,
        "device": "cpu",
        "samples_processed": len(losses),
        "train_loss": avg_loss,
        "elapsed_seconds": time.time() - start_time,
        "model_parameters": sum(p.numel() for p in model.parameters()),
        "run_tag": run_tag,
        "model_type": "SimpleModel",
    }

    with open(output_dir / "metrics.json", "w") as f:
        json.dump(metrics, f, indent=2)

    elapsed = time.time() - start_time
    print(f"ğŸ Minimal test completed in {elapsed:.1f}s")
    print(f"ğŸ“Š Results saved to: {output_dir}")

    return metrics, output_dir


if __name__ == "__main__":
    result, outdir = main()
    print(f"âœ… SUCCESS: {result['run_tag']}")
    print(f"ğŸ“ Directory: {outdir}")
