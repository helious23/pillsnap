"""
ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ ì»´í¬ë„ŒíŠ¸ í†µí•© í…ŒìŠ¤íŠ¸
ìƒì—…ìš© ìˆ˜ì¤€ì˜ ìƒˆë¡œ ì¶”ê°€ëœ ëª¨ë“  ì‹œìŠ¤í…œ ê²€ì¦

í…ŒìŠ¤íŠ¸ ëŒ€ìƒ:
- Training Stage Components
- Evaluation Components  
- Data Loading Components
- Memory Management
- State Management
"""

import pytest
import sys
import time
import tempfile
import shutil
from pathlib import Path
from typing import Dict, Any

import torch
import torch.nn as nn

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.training.train_classification_stage import ClassificationStageTrainer
from src.training.train_detection_stage import DetectionStageTrainer
from src.training.batch_size_auto_tuner import BatchSizeAutoTuner, create_sample_batch_generator
from src.training.training_state_manager import TrainingStateManager, ModelMetadata
from src.evaluation.evaluate_detection_metrics import DetectionMetricsEvaluator, DetectionMetrics
from src.evaluation.evaluate_pipeline_end_to_end import EndToEndPipelineEvaluator
from src.training.memory_monitor_gpu_usage import GPUMemoryMonitor


class TestTrainingComponents:
    """í•™ìŠµ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸"""
    
    def test_classification_stage_trainer_initialization(self):
        """ë¶„ë¥˜ Stage í•™ìŠµê¸° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        trainer = ClassificationStageTrainer(
            num_classes=10,
            target_accuracy=0.8,
            device="cpu"  # CIì—ì„œ GPU ì—†ì„ ìˆ˜ ìˆìŒ
        )
        
        assert trainer.num_classes == 10
        assert trainer.target_accuracy == 0.8
        assert trainer.device.type == "cpu"
        assert trainer.model is None  # ì•„ì§ ì„¤ì • ì•ˆë¨
    
    def test_classification_trainer_model_setup(self):
        """ë¶„ë¥˜ í•™ìŠµê¸° ëª¨ë¸ ì„¤ì • í…ŒìŠ¤íŠ¸"""
        trainer = ClassificationStageTrainer(num_classes=5, device="cpu")
        
        # ëª¨ë¸ ì„¤ì •
        trainer.setup_model_and_optimizers(
            learning_rate=1e-3,
            mixed_precision=False  # CPUì—ì„œëŠ” ë¹„í™œì„±í™”
        )
        
        assert trainer.model is not None
        assert trainer.optimizer is not None
        assert trainer.scheduler is not None
        assert len(list(trainer.model.parameters())) > 0
    
    def test_detection_stage_trainer_initialization(self):
        """ê²€ì¶œ Stage í•™ìŠµê¸° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        trainer = DetectionStageTrainer(target_map=0.5, device="cpu")
        
        assert trainer.target_map == 0.5
        assert trainer.device == "cpu"
        assert trainer.best_map == 0.0
    
    def test_detection_trainer_model_setup(self):
        """ê²€ì¶œ í•™ìŠµê¸° ëª¨ë¸ ì„¤ì • í…ŒìŠ¤íŠ¸"""
        trainer = DetectionStageTrainer(device="cpu")
        
        # YOLO ëª¨ë¸ ì„¤ì • (CPUì—ì„œëŠ” ì œí•œì )
        trainer.setup_model()
        assert trainer.model is not None


class TestBatchSizeAutoTuner:
    """ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì • í…ŒìŠ¤íŠ¸"""
    
    def test_auto_tuner_initialization(self):
        """ìë™ íŠœë„ˆ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        tuner = BatchSizeAutoTuner(
            initial_batch_size=16,
            max_batch_size=64,
            memory_threshold_gb=8.0
        )
        
        assert tuner.initial_batch_size == 16
        assert tuner.max_batch_size == 64
        assert tuner.memory_threshold_gb == 8.0
    
    def test_batch_size_optimization_cpu(self):
        """CPUì—ì„œ ë°°ì¹˜ í¬ê¸° ìµœì í™” í…ŒìŠ¤íŠ¸ (ê°•í™”ëœ ê²€ì¦)"""
        # ê°„ë‹¨í•œ ë”ë¯¸ ëª¨ë¸
        model = nn.Sequential(
            nn.Linear(100, 50),
            nn.ReLU(),
            nn.Linear(50, 10)
        )
        
        # ë°°ì¹˜ ìƒì„±ê¸°
        batch_generator = create_sample_batch_generator((100,), 10)
        
        tuner = BatchSizeAutoTuner(
            initial_batch_size=4,
            max_batch_size=16,
            memory_threshold_gb=1.0  # ë‚®ì€ ì„ê³„ê°’
        )
        
        # ìµœì í™” ì‹¤í–‰
        start_time = time.time()
        result = tuner.find_optimal_batch_size(
            model=model,
            sample_batch_generator=batch_generator,
            device=torch.device('cpu')
        )
        execution_time = time.time() - start_time
        
        # ê°•í™”ëœ ê²€ì¦
        assert result.optimal_batch_size > 0, "ìµœì  ë°°ì¹˜ í¬ê¸°ëŠ” 0ë³´ë‹¤ ì»¤ì•¼ í•¨"
        assert result.optimal_batch_size <= 16, "ìµœì  ë°°ì¹˜ í¬ê¸°ëŠ” ìµœëŒ€ê°’ì„ ì´ˆê³¼í•  ìˆ˜ ì—†ìŒ"
        assert result.optimal_batch_size >= 4, "ìµœì  ë°°ì¹˜ í¬ê¸°ëŠ” ì´ˆê¸°ê°’ë³´ë‹¤ ì‘ì„ ìˆ˜ ì—†ìŒ"
        assert result.tuning_time_seconds > 0, "íŠœë‹ ì‹œê°„ì´ ê¸°ë¡ë˜ì–´ì•¼ í•¨"
        assert result.tuning_time_seconds < 60.0, "íŠœë‹ ì‹œê°„ì´ 60ì´ˆë¥¼ ì´ˆê³¼í•˜ë©´ ì•ˆë¨"
        assert result.throughput_samples_per_sec > 0, "ì²˜ë¦¬ëŸ‰ì´ 0ë³´ë‹¤ ì»¤ì•¼ í•¨"
        assert execution_time < result.tuning_time_seconds + 5.0, "ì‹¤í–‰ ì‹œê°„ ì¼ê´€ì„± ê²€ì¦"
    
    def test_inference_batch_size_suggestion(self):
        """ì¶”ë¡ ìš© ë°°ì¹˜ í¬ê¸° ì œì•ˆ í…ŒìŠ¤íŠ¸"""
        model = nn.Linear(50, 5)
        
        def input_generator(batch_size):
            return torch.randn(batch_size, 50)
        
        tuner = BatchSizeAutoTuner()
        
        optimal_batch = tuner.suggest_batch_size_for_inference(
            model=model,
            sample_input_generator=input_generator,
            target_latency_ms=100.0
        )
        
        assert optimal_batch >= 1
        assert optimal_batch <= 64


class TestTrainingStateManager:
    """í•™ìŠµ ìƒíƒœ ê´€ë¦¬ í…ŒìŠ¤íŠ¸"""
    
    def setup_method(self):
        """í…ŒìŠ¤íŠ¸ ì¤€ë¹„"""
        self.temp_dir = tempfile.mkdtemp()
        self.experiment_name = "test_experiment"
    
    def teardown_method(self):
        """í…ŒìŠ¤íŠ¸ ì •ë¦¬"""
        if Path(self.temp_dir).exists():
            shutil.rmtree(self.temp_dir)
    
    def test_state_manager_initialization(self):
        """ìƒíƒœ ê´€ë¦¬ì ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        manager = TrainingStateManager(
            experiment_name=self.experiment_name,
            stage=1,
            checkpoint_dir=self.temp_dir
        )
        
        assert manager.experiment_name == self.experiment_name
        assert manager.stage == 1
        assert manager.experiment_dir.exists()
    
    def test_checkpoint_save_and_load(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸"""
        manager = TrainingStateManager(
            experiment_name=self.experiment_name,
            checkpoint_dir=self.temp_dir
        )
        
        # ë”ë¯¸ ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì €
        model = nn.Linear(10, 5)
        optimizer = torch.optim.Adam(model.parameters())
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = manager.save_checkpoint(
            epoch=5,
            model=model,
            optimizer=optimizer,
            metric_value=0.85,
            is_best=True
        )
        
        assert Path(checkpoint_path).exists()
        
        # ìƒˆ ëª¨ë¸ì— ë¡œë“œ
        new_model = nn.Linear(10, 5)
        new_optimizer = torch.optim.Adam(new_model.parameters())
        
        resume_info = manager.load_checkpoint(
            model=new_model,
            optimizer=new_optimizer,
            load_best=True
        )
        
        # ê°•í™”ëœ ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
        assert resume_info['epoch'] == 5, f"ì—í¬í¬ ë¶ˆì¼ì¹˜: ì˜ˆìƒ 5, ì‹¤ì œ {resume_info['epoch']}"
        assert resume_info['best_metric'] == 0.85, f"ìµœê³  ì„±ëŠ¥ ë¶ˆì¼ì¹˜: ì˜ˆìƒ 0.85, ì‹¤ì œ {resume_info['best_metric']}"
        assert resume_info['best_epoch'] == 5, f"ìµœê³  ì„±ëŠ¥ ì—í¬í¬ ë¶ˆì¼ì¹˜: ì˜ˆìƒ 5, ì‹¤ì œ {resume_info['best_epoch']}"
        assert 'training_config' in resume_info, "training_configê°€ ëˆ„ë½ë¨"
        assert 'timestamp' in resume_info, "timestampê°€ ëˆ„ë½ë¨"
        
        # ëª¨ë¸ ìƒíƒœ ê²€ì¦ - ë¡œë“œëœ ëª¨ë¸ì´ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
        test_input = torch.randn(1, 10)
        with torch.no_grad():
            output = new_model(test_input)
            assert output.shape == (1, 5), f"ëª¨ë¸ ì¶œë ¥ í˜•íƒœ ì˜¤ë¥˜: {output.shape}"
            assert torch.isfinite(output).all(), "ëª¨ë¸ ì¶œë ¥ì— ë¬´í•œê°’ì´ë‚˜ NaNì´ í¬í•¨ë¨"
    
    def test_deployment_model_saving(self):
        """ë°°í¬ìš© ëª¨ë¸ ì €ì¥ í…ŒìŠ¤íŠ¸"""
        manager = TrainingStateManager(
            experiment_name=self.experiment_name,
            checkpoint_dir=self.temp_dir
        )
        
        model = nn.Linear(20, 3)
        
        metadata = ModelMetadata(
            model_name="test_model",
            stage=1,
            num_classes=3,
            input_size=(20,),
            best_metric_value=0.92,
            best_metric_name="accuracy",
            training_samples=1000,
            validation_samples=200,
            training_time_hours=1.5,
            created_timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
        )
        
        deploy_path = manager.save_final_model_for_deployment(
            model=model,
            model_metadata=metadata,
            include_training_artifacts=True
        )
        
        assert Path(deploy_path).exists()
        assert Path(deploy_path).parent.joinpath("model_metadata.json").exists()
        assert Path(deploy_path).parent.joinpath("deployment_guide.md").exists()


class TestEvaluationComponents:
    """í‰ê°€ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸"""
    
    def test_detection_metrics_evaluator(self):
        """ê²€ì¶œ ë©”íŠ¸ë¦­ í‰ê°€ê¸° í…ŒìŠ¤íŠ¸"""
        evaluator = DetectionMetricsEvaluator(num_classes=1)
        
        # ë”ë¯¸ YOLO ê²°ê³¼
        dummy_results = {
            'metrics': {
                'mAP_0.5': 0.75,
                'mAP_0.5:0.95': 0.55,
                'precision': 0.80,
                'recall': 0.70
            },
            'num_images': 500
        }
        
        metrics = evaluator.evaluate_yolo_results(dummy_results)
        
        assert isinstance(metrics, DetectionMetrics)
        assert metrics.map_50 == 0.75
        assert metrics.precision == 0.80
        assert metrics.f1_score > 0  # ê³„ì‚°ë¨
        assert metrics.num_images == 500
    
    def test_stage_target_achievement(self):
        """Stage ëª©í‘œ ë‹¬ì„± í‰ê°€ í…ŒìŠ¤íŠ¸"""
        evaluator = DetectionMetricsEvaluator()
        
        # ì¢‹ì€ ì„±ëŠ¥ ë©”íŠ¸ë¦­
        good_metrics = DetectionMetrics(
            map_50=0.85,
            map_50_95=0.65,
            precision=0.90,
            recall=0.80,
            f1_score=0.85,
            ap_per_class=[0.85],
            precision_per_class=[0.90],
            recall_per_class=[0.80],
            num_classes=1,
            num_images=1000
        )
        
        achievement = evaluator.evaluate_stage_target_achievement(good_metrics, stage=1)
        assert achievement['stage1_detection_completed'] == True
        
        # ë‚˜ìœ ì„±ëŠ¥ ë©”íŠ¸ë¦­  
        poor_metrics = DetectionMetrics(
            map_50=0.15,
            map_50_95=0.10,
            precision=0.20,
            recall=0.15,
            f1_score=0.17,
            ap_per_class=[0.15],
            precision_per_class=[0.20],
            recall_per_class=[0.15],
            num_classes=1,
            num_images=1000
        )
        
        achievement = evaluator.evaluate_stage_target_achievement(poor_metrics, stage=1)
        assert achievement['stage1_detection_completed'] == False
    
    def test_end_to_end_evaluator_initialization(self):
        """End-to-End í‰ê°€ê¸° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        evaluator = EndToEndPipelineEvaluator(pipeline=None)
        
        assert evaluator.pipeline is None
        assert evaluator.memory_monitor is not None
        assert evaluator.classification_evaluator is not None
        assert evaluator.detection_evaluator is not None


class TestMemoryMonitoring:
    """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸"""
    
    def test_memory_monitor_initialization(self):
        """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        monitor = GPUMemoryMonitor(target_memory_gb=8.0)
        
        assert monitor.target_memory_gb == 8.0
        assert monitor.warning_threshold == 0.9
        assert monitor.critical_threshold == 0.95
    
    def test_memory_usage_reporting(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë³´ê³  í…ŒìŠ¤íŠ¸ (ê°•í™”ëœ ê²€ì¦)"""
        monitor = GPUMemoryMonitor()
        
        usage = monitor.get_current_usage()
        
        # ê¸°ë³¸ í‚¤ ì¡´ì¬ ê²€ì¦
        required_keys = ['used_gb', 'total_gb', 'utilization_percent', 'usage_ratio']
        for key in required_keys:
            assert key in usage, f"í•„ìˆ˜ í‚¤ '{key}'ê°€ ëˆ„ë½ë¨"
        
        # ê°’ ë²”ìœ„ ê²€ì¦
        assert usage['used_gb'] >= 0, "ì‚¬ìš© ë©”ëª¨ë¦¬ëŠ” 0 ì´ìƒì´ì–´ì•¼ í•¨"
        assert usage['total_gb'] > 0, "ì „ì²´ ë©”ëª¨ë¦¬ëŠ” 0ë³´ë‹¤ ì»¤ì•¼ í•¨"
        assert 0 <= usage['utilization_percent'] <= 100, "ì‚¬ìš©ë¥ ì€ 0-100% ë²”ìœ„ì—¬ì•¼ í•¨"
        assert 0 <= usage['usage_ratio'] <= 1.0, "ì‚¬ìš© ë¹„ìœ¨ì€ 0-1 ë²”ìœ„ì—¬ì•¼ í•¨"
        assert usage['used_gb'] <= usage['total_gb'], "ì‚¬ìš© ë©”ëª¨ë¦¬ëŠ” ì „ì²´ ë©”ëª¨ë¦¬ë¥¼ ì´ˆê³¼í•  ìˆ˜ ì—†ìŒ"
        
        # ê³„ì‚° ì¼ê´€ì„± ê²€ì¦
        expected_ratio = usage['used_gb'] / usage['total_gb'] if usage['total_gb'] > 0 else 0
        assert abs(usage['usage_ratio'] - expected_ratio) < 0.01, "ì‚¬ìš© ë¹„ìœ¨ ê³„ì‚° ì˜¤ë¥˜"
    
    def test_memory_cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬ í…ŒìŠ¤íŠ¸"""
        monitor = GPUMemoryMonitor()
        
        cleanup_result = monitor.force_memory_cleanup()
        
        assert 'freed_gb' in cleanup_result
        assert cleanup_result['freed_gb'] >= 0


class TestIntegrationScenarios:
    """í†µí•© ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
    
    def test_training_pipeline_integration(self):
        """í•™ìŠµ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸"""
        with tempfile.TemporaryDirectory() as temp_dir:
            # ìƒíƒœ ê´€ë¦¬ì
            state_manager = TrainingStateManager(
                experiment_name="integration_test",
                checkpoint_dir=temp_dir
            )
            
            # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°
            memory_monitor = GPUMemoryMonitor(target_memory_gb=4.0)
            
            # ë¶„ë¥˜ í•™ìŠµê¸°
            trainer = ClassificationStageTrainer(
                num_classes=5,
                target_accuracy=0.7,
                device="cpu"
            )
            
            # ëª¨ë¸ ì„¤ì •
            trainer.setup_model_and_optimizers(mixed_precision=False)
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            checkpoint_path = state_manager.save_checkpoint(
                epoch=1,
                model=trainer.model,
                optimizer=trainer.optimizer,
                metric_value=0.75,
                is_best=True
            )
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            memory_usage = memory_monitor.get_current_usage()
            
            # ê²€ì¦
            assert Path(checkpoint_path).exists()
            assert memory_usage['used_gb'] >= 0
            assert trainer.model is not None
    
    def test_evaluation_pipeline_integration(self):
        """í‰ê°€ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸"""
        # ê²€ì¶œ í‰ê°€ê¸°
        detection_evaluator = DetectionMetricsEvaluator(num_classes=1)
        
        # End-to-End í‰ê°€ê¸°
        e2e_evaluator = EndToEndPipelineEvaluator(pipeline=None)
        
        # ë”ë¯¸ ê²°ê³¼ë¡œ í‰ê°€
        dummy_detection_results = {
            'metrics': {
                'mAP_0.5': 0.42,
                'precision': 0.45,
                'recall': 0.40
            },
            'num_images': 100
        }
        
        detection_metrics = detection_evaluator.evaluate_yolo_results(dummy_detection_results)
        
        # Stage 1 ëª©í‘œ ë‹¬ì„± í™•ì¸
        achievement = detection_evaluator.evaluate_stage_target_achievement(detection_metrics, stage=1)
        
        assert isinstance(detection_metrics, DetectionMetrics)
        assert 'stage1_detection_completed' in achievement


class TestStrictValidation:
    """ì—„ê²©í•œ ê²€ì¦ í…ŒìŠ¤íŠ¸ (ìƒì—…ìš© ìˆ˜ì¤€)"""
    
    def test_performance_requirements_validation(self):
        """ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ ì—„ê²© ê²€ì¦"""
        from src.training.batch_size_auto_tuner import BatchSizeAutoTuner, create_sample_batch_generator
        import time
        
        # ìµœì†Œ ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­
        MIN_THROUGHPUT = 10.0  # samples/sec
        MAX_TUNING_TIME = 30.0  # seconds
        
        model = torch.nn.Linear(50, 10)
        batch_generator = create_sample_batch_generator((50,), 10)
        tuner = BatchSizeAutoTuner(max_batch_size=32)
        
        start_time = time.time()
        result = tuner.find_optimal_batch_size(model, batch_generator)
        actual_time = time.time() - start_time
        
        # ì—„ê²©í•œ ì„±ëŠ¥ ê²€ì¦
        assert result.throughput_samples_per_sec >= MIN_THROUGHPUT, \
            f"ì²˜ë¦¬ëŸ‰ ë¯¸ë‹¬: {result.throughput_samples_per_sec:.2f} < {MIN_THROUGHPUT}"
        assert actual_time <= MAX_TUNING_TIME, \
            f"íŠœë‹ ì‹œê°„ ì´ˆê³¼: {actual_time:.2f}s > {MAX_TUNING_TIME}s"
        assert result.optimal_batch_size >= 4, "ìµœì  ë°°ì¹˜ê°€ ë„ˆë¬´ ì‘ìŒ"
    
    def test_memory_efficiency_validation(self):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì—„ê²© ê²€ì¦"""
        from src.training.memory_monitor_gpu_usage import GPUMemoryMonitor
        
        monitor = GPUMemoryMonitor()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì—°ì† ì¸¡ì •
        measurements = []
        for _ in range(3):
            usage = monitor.get_current_usage()
            measurements.append(usage)
            time.sleep(0.1)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì•ˆì •ì„± ê²€ì¦
        used_values = [m['used_gb'] for m in measurements]
        max_variance = max(used_values) - min(used_values)
        assert max_variance < 1.0, f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë³€ë™ì´ ë„ˆë¬´ í¼: {max_variance:.2f}GB"
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê²€ì¦
        for usage in measurements:
            assert usage['usage_ratio'] < 0.95, "ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ 95%ë¥¼ ì´ˆê³¼í•¨"
    
    def test_error_handling_robustness(self):
        """ì—ëŸ¬ ì²˜ë¦¬ ê°•ê±´ì„± í…ŒìŠ¤íŠ¸"""
        from src.training.training_state_manager import TrainingStateManager
        import tempfile
        import os
        
        with tempfile.TemporaryDirectory() as temp_dir:
            manager = TrainingStateManager("error_test", checkpoint_dir=temp_dir)
            
            # ì˜ëª»ëœ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œë„
            fake_checkpoint = Path(temp_dir) / "fake_checkpoint.pt"
            fake_checkpoint.write_text("invalid data")
            
            model = torch.nn.Linear(5, 3)
            
            # ì˜ˆì™¸ê°€ ì ì ˆíˆ ë°œìƒí•˜ëŠ”ì§€ í™•ì¸
            with pytest.raises(Exception):
                manager.load_checkpoint(model, checkpoint_path=str(fake_checkpoint))
    
    def test_stage1_performance_targets(self):
        """Stage 1 ì„±ëŠ¥ ëª©í‘œ ì—„ê²© ê²€ì¦"""
        from src.evaluation.evaluate_detection_metrics import DetectionMetricsEvaluator, DetectionMetrics
        
        evaluator = DetectionMetricsEvaluator()
        
        # Stage 1 ìµœì†Œ ìš”êµ¬ì‚¬í•­ë³´ë‹¤ ì•½ê°„ ë‚®ì€ ì„±ëŠ¥
        borderline_metrics = DetectionMetrics(
            map_50=0.29,  # ëª©í‘œ: 0.30
            map_50_95=0.20,
            precision=0.34,  # ëª©í‘œ: 0.35
            recall=0.29,  # ëª©í‘œ: 0.30
            f1_score=0.31,
            ap_per_class=[0.29],
            precision_per_class=[0.34],
            recall_per_class=[0.29],
            num_classes=1,
            num_images=1000
        )
        
        achievement = evaluator.evaluate_stage_target_achievement(borderline_metrics, stage=1)
        
        # ì—„ê²©í•˜ê²Œ ëª©í‘œ ë¯¸ë‹¬ì„± í™•ì¸
        assert achievement['stage1_detection_completed'] == False, \
            "ê²½ê³„ì„  ì„±ëŠ¥ì—ì„œ ëª©í‘œ ë‹¬ì„±ìœ¼ë¡œ ì˜ëª» íŒì •ë¨"


# pytest ì‹¤í–‰ì„ ìœ„í•œ main í•¨ìˆ˜
def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ§ª ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ ì»´í¬ë„ŒíŠ¸ í†µí•© í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # pytest ì‹¤í–‰
    test_file = __file__
    exit_code = pytest.main([
        test_file,
        "-v",
        "--tb=short",
        "--durations=10"
    ])
    
    if exit_code == 0:
        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
    else:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ (Exit code: {exit_code})")
    
    return exit_code


if __name__ == "__main__":
    main()