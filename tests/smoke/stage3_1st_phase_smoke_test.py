#!/usr/bin/env python3
"""
PillSnap ML Stage 3 1ë‹¨ê³„ ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ (1ë‹¨ê³„ í•„ìˆ˜)

5ê°€ì§€ ê²€ì¦ í•­ëª©:
1. Config ë¡œë”ê°€ ì¤‘ë³µ í‚¤ ë°œê²¬ ì‹œ ì‹¤íŒ¨í•˜ëŠ”ì§€ (ì˜ë„ì  ë”ë¯¸ë¡œ í…ŒìŠ¤íŠ¸)
2. ì‘ì€ per-GPU ë°°ì¹˜ ì„¤ì •ì—ì„œ 1~2 epoch ì™„ì£¼ (OOM ì—†ìŒ)
3. sanity ê²€ì¦(ë§¤ epoch 100 batch)ê°€ ëŒì•„ê°€ê³  ë„ë©”ì¸ ë¶„ë¦¬ ì§€í‘œê°€ ì €ì¥ë˜ëŠ”ì§€
4. auto-confidenceê°€ ì„ íƒë˜ì–´ "ì¶”ë¡  ì„¤ì •/ì²´í¬í¬ì¸íŠ¸/ë¦¬í¬íŠ¸" 3ê³³ì— ì¼ê´€ ë°˜ì˜ë˜ëŠ”ì§€
5. ë ˆì´í„´ì‹œ ë¶„í•´(det/crop/cls/total)ì™€ VRAM peakê°€ ë¦¬í¬íŠ¸ì— ì°íˆëŠ”ì§€

RTX 5080 ìµœì í™”
"""

import os
import sys
import tempfile
import shutil
from pathlib import Path
import time
import traceback
from typing import Dict, Any, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parents[2]))

import torch
import pandas as pd
from torch.utils.data import Dataset, DataLoader

# 1ë‹¨ê³„ êµ¬í˜„ëœ ëª¨ë“ˆë“¤ import
from src.utils.core import ConfigLoader, PillSnapLogger
from src.utils.cuda_oom_guard import CUDAOOMGuard, OOMGuardConfig
from src.training.interleave_scheduler import InterleaveScheduler, InterleaveConfig, TaskType
from src.data.domain_mixed_sampler import DomainMixedSampler, DomainMixConfig
from src.evaluation.confidence_tuner import ConfidenceTuner, ConfidenceTuningConfig
from src.monitoring.minimal_logger import MinimalLogger, MinimalLoggingConfig


class SmokeTestResults:
    """ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìˆ˜ì§‘"""
    
    def __init__(self):
        self.results = {}
        self.errors = {}
        self.warnings = []
        self.logger = PillSnapLogger(__name__)
    
    def add_result(self, test_name: str, success: bool, details: str = "") -> None:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶”ê°€"""
        self.results[test_name] = {
            "success": success,
            "details": details,
            "timestamp": time.time()
        }
        
        status = "âœ…" if success else "âŒ"
        self.logger.info(f"{status} {test_name}: {details}")
    
    def add_error(self, test_name: str, error: Exception) -> None:
        """ì—ëŸ¬ ì¶”ê°€"""
        self.errors[test_name] = {
            "error": str(error),
            "traceback": traceback.format_exc(),
            "timestamp": time.time()
        }
        
        self.logger.error(f"âŒ {test_name} ì‹¤íŒ¨: {error}")
    
    def add_warning(self, message: str) -> None:
        """ê²½ê³  ì¶”ê°€"""
        self.warnings.append({
            "message": message,
            "timestamp": time.time()
        })
        
        self.logger.warning(f"âš ï¸ {message}")
    
    def get_summary(self) -> Dict[str, Any]:
        """ê²°ê³¼ ìš”ì•½"""
        total_tests = len(self.results)
        passed_tests = sum(1 for r in self.results.values() if r["success"])
        
        return {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": total_tests - passed_tests,
            "success_rate": passed_tests / total_tests if total_tests > 0 else 0.0,
            "warnings": len(self.warnings),
            "errors": len(self.errors)
        }
    
    def print_summary(self) -> None:
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        summary = self.get_summary()
        
        print("\n" + "=" * 60)
        print("ğŸ§ª 1ë‹¨ê³„ ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ í†µê³„:")
        print(f"  - ì´ í…ŒìŠ¤íŠ¸: {summary['total_tests']}ê°œ")
        print(f"  - í†µê³¼: {summary['passed_tests']}ê°œ")
        print(f"  - ì‹¤íŒ¨: {summary['failed_tests']}ê°œ")
        print(f"  - ì„±ê³µë¥ : {summary['success_rate']:.1%}")
        print(f"  - ê²½ê³ : {summary['warnings']}ê°œ")
        print(f"  - ì—ëŸ¬: {summary['errors']}ê°œ")
        
        print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ì„¸ë¶€ ê²°ê³¼:")
        for test_name, result in self.results.items():
            status = "âœ…" if result["success"] else "âŒ"
            print(f"  {status} {test_name}")
            if result["details"]:
                print(f"      â”” {result['details']}")
        
        if self.warnings:
            print(f"\nâš ï¸ ê²½ê³  ({len(self.warnings)}ê°œ):")
            for warning in self.warnings[-5:]:  # ìµœê·¼ 5ê°œë§Œ
                print(f"  - {warning['message']}")
        
        if self.errors:
            print(f"\nâŒ ì—ëŸ¬ ({len(self.errors)}ê°œ):")
            for test_name, error in self.errors.items():
                print(f"  - {test_name}: {error['error']}")
        
        print("\n" + "=" * 60)
        
        overall_success = summary['failed_tests'] == 0
        if overall_success:
            print("ğŸ‰ ëª¨ë“  1ë‹¨ê³„ ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ í†µê³¼! Stage 3 ì¬í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ.")
        else:
            print(f"ğŸ’¥ {summary['failed_tests']}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ë¬¸ì œ í•´ê²° í›„ ì¬ì‹œë„ í•„ìš”.")


class MockDataset(Dataset):
    """í…ŒìŠ¤íŠ¸ìš© Mock ë°ì´í„°ì…‹"""
    
    def __init__(self, size: int = 200, domains: List[str] = None):
        self.size = size
        if domains is None:
            domains = ["single", "combination"]
        
        # Mock manifest ë°ì´í„°
        domain_data = []
        for i in range(size):
            domain = domains[i % len(domains)]
            domain_data.append({
                'image_path': f'/fake/path/img_{i}.jpg',
                'image_type': domain,
                'mapping_code': f'K{i:06d}'
            })
        
        self.data = pd.DataFrame(domain_data)
    
    def __len__(self):
        return self.size
    
    def __getitem__(self, idx):
        domain = self.data.iloc[idx]['image_type']
        # RGB ì´ë¯¸ì§€ í…ì„œ (3, 224, 224)
        image = torch.randn(3, 224, 224)
        label = idx % 100  # 100ê°œ í´ë˜ìŠ¤
        return image, label, domain


class Stage3SmokeTestRunner:
    """Stage 3 1ë‹¨ê³„ ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.results = SmokeTestResults()
        self.temp_dirs = []
        self.logger = PillSnapLogger(__name__)
        
        print("ğŸ”¥ Stage 3 1ë‹¨ê³„ ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        for temp_dir in self.temp_dirs:
            try:
                if temp_dir.exists():
                    shutil.rmtree(temp_dir)
            except Exception as e:
                self.results.add_warning(f"ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def create_temp_dir(self) -> Path:
        """ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        temp_dir = Path(tempfile.mkdtemp())
        self.temp_dirs.append(temp_dir)
        return temp_dir
    
    def test_1_config_duplicate_key_detection(self) -> None:
        """í…ŒìŠ¤íŠ¸ 1: Config ë¡œë”ê°€ ì¤‘ë³µ í‚¤ ë°œê²¬ ì‹œ ì‹¤íŒ¨í•˜ëŠ”ì§€"""
        try:
            # ì¤‘ë³µ í‚¤ê°€ í¬í•¨ëœ ë”ë¯¸ YAML ìƒì„±
            temp_dir = self.create_temp_dir()
            duplicate_yaml = temp_dir / "duplicate_config.yaml"
            
            duplicate_content = \"\"\"\n# ì˜ë„ì  ì¤‘ë³µ í‚¤ í…ŒìŠ¤íŠ¸\nlogging:\n  enabled: true\n  level: info\n\n# ì¤‘ë³µëœ logging í‚¤ (ì˜ë„ì )\nlogging:\n  enabled: false\n  level: debug\n\ndata:\n  root: \"/fake/path\"\n\"\"\"\n            \n            with open(duplicate_yaml, 'w') as f:\n                f.write(duplicate_content)\n            \n            # ConfigLoaderë¡œ ë¡œë“œ ì‹œë„ (ì‹¤íŒ¨í•´ì•¼ í•¨)\n            try:\n                loader = ConfigLoader(str(duplicate_yaml))\n                config = loader._load_config_instance()\n                \n                # ì‹¤íŒ¨í•´ì•¼ í•˜ëŠ”ë° ì„±ê³µí•˜ë©´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨\n                self.results.add_result(\n                    "test_1_duplicate_key_detection",\n                    False,\n                    \"ì¤‘ë³µ í‚¤ê°€ ê°ì§€ë˜ì§€ ì•ŠìŒ - YAML íŒŒì„œê°€ ì¤‘ë³µì„ í—ˆìš©í•¨\"\n                )\n            \n            except ValueError as e:\n                if \"ì¤‘ë³µ\" in str(e) or \"duplicate\" in str(e).lower():\n                    self.results.add_result(\n                        "test_1_duplicate_key_detection",\n                        True,\n                        f\"ì¤‘ë³µ í‚¤ ì •ìƒ ê°ì§€: {str(e)[:100]}\"\n                    )\n                else:\n                    raise e\n            \n        except Exception as e:\n            self.results.add_error("test_1_duplicate_key_detection", e)\n    \n    def test_2_small_batch_training_completion(self) -> None:\n        \"\"\"í…ŒìŠ¤íŠ¸ 2: ì‘ì€ per-GPU ë°°ì¹˜ ì„¤ì •ì—ì„œ 1~2 epoch ì™„ì£¼ (OOM ì—†ìŒ)\"\"\"\n        try:\n            # OOM ê°€ë“œ ì„¤ì •\n            oom_config = OOMGuardConfig(\n                min_batch_size=1,\n                max_oom_recoveries=3,\n                batch_reduction_factor=0.5\n            )\n            \n            oom_guard = CUDAOOMGuard(oom_config)\n            oom_guard.setup_training_params(batch_size=4, grad_accum_steps=2, learning_rate=1e-4)\n            \n            # Mock ë°ì´í„°ì…‹\n            dataset = MockDataset(size=50)  # ì‘ì€ ë°ì´í„°ì…‹\n            \n            # Mock í•™ìŠµ ë£¨í”„ (2 epoch)\n            completed_epochs = 0\n            oom_occurred = False\n            \n            for epoch in range(2):\n                try:\n                    # Epoch ì‹œì‘\n                    oom_guard.reset_oom_count()\n                    \n                    # ë°°ì¹˜ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜\n                    for batch_idx in range(10):  # 10 ë°°ì¹˜\n                        # Mock í•™ìŠµ step\n                        if torch.cuda.is_available():\n                            # ê°€ìƒ í…ì„œ ì—°ì‚° (VRAM ì‚¬ìš©)\n                            x = torch.randn(4, 3, 224, 224, device='cuda')\n                            y = torch.nn.functional.conv2d(x, torch.randn(64, 3, 3, 3, device='cuda'))\n                            loss = y.sum()\n                            loss.backward()\n                            \n                            # ë©”ëª¨ë¦¬ ì •ë¦¬\n                            del x, y, loss\n                            torch.cuda.empty_cache()\n                    \n                    completed_epochs += 1\n                    \n                except torch.cuda.OutOfMemoryError as oom_e:\n                    oom_occurred = True\n                    # OOM ë³µêµ¬ ì‹œë„\n                    can_recover, new_batch, new_grad_accum, new_lr = oom_guard.handle_oom_error(oom_e, \"training\")\n                    \n                    if can_recover:\n                        self.results.add_warning(f\"OOM ë°œìƒí–ˆì§€ë§Œ ë³µêµ¬ë¨: batch {new_batch}\")\n                        continue\n                    else:\n                        raise oom_e\n            \n            # ê²°ê³¼ í‰ê°€\n            success = completed_epochs >= 1\n            details = f\"{completed_epochs}/2 epoch ì™„ë£Œ\"\n            \n            if oom_occurred:\n                details += \", OOM ë°œìƒí–ˆì§€ë§Œ ë³µêµ¬ë¨\"\n            \n            self.results.add_result(\n                \"test_2_small_batch_training_completion\",\n                success,\n                details\n            )\n            \n        except Exception as e:\n            self.results.add_error(\"test_2_small_batch_training_completion\", e)\n    \n    def test_3_sanity_validation_domain_separation(self) -> None:\n        \"\"\"í…ŒìŠ¤íŠ¸ 3: sanity ê²€ì¦ê³¼ ë„ë©”ì¸ ë¶„ë¦¬ ì§€í‘œ ì €ì¥\"\"\"\n        try:\n            # ë„ë©”ì¸ í˜¼í•© ì„¤ì •\n            domain_config = DomainMixConfig(\n                single_ratio=0.75,\n                combination_ratio=0.25,\n                separate_domain_metrics=True\n            )\n            \n            # Mock ë°ì´í„°ì…‹\n            dataset = MockDataset(size=200, domains=[\"single\", \"combination\"])\n            \n            # ë„ë©”ì¸ í˜¼í•© ìƒ˜í”ŒëŸ¬\n            sampler = DomainMixedSampler(dataset, domain_config, batch_size=8)\n            \n            # 100 ë°°ì¹˜ sanity ê²€ì¦ ì‹œë®¬ë ˆì´ì…˜\n            domain_stats = {\"single\": 0, \"combination\": 0}\n            batches_processed = 0\n            \n            # ìƒ˜í”Œë§ í…ŒìŠ¤íŠ¸\n            sample_iterator = iter(sampler)\n            for _ in range(min(100, len(sampler) // 8)):  # 100 ë°°ì¹˜ ë˜ëŠ” ìµœëŒ€ ê°€ëŠ¥\n                try:\n                    batch_indices = [next(sample_iterator) for _ in range(8)]\n                    \n                    # ë„ë©”ì¸ í†µê³„ ìˆ˜ì§‘\n                    for idx in batch_indices:\n                        if idx < len(dataset):\n                            _, _, domain = dataset[idx]\n                            domain_stats[domain] += 1\n                    \n                    batches_processed += 1\n                    \n                except StopIteration:\n                    break\n            \n            # ë„ë©”ì¸ ë¶„ë¦¬ ì§€í‘œ í™•ì¸\n            total_samples = sum(domain_stats.values())\n            domain_ratios = {\n                domain: count / total_samples if total_samples > 0 else 0\n                for domain, count in domain_stats.items()\n            }\n            \n            # ì„ì‹œ ì €ì¥ ê²½ë¡œ\n            temp_dir = self.create_temp_dir()\n            domain_stats_file = temp_dir / \"domain_validation_stats.json\"\n            \n            import json\n            with open(domain_stats_file, 'w') as f:\n                json.dump({\n                    \"batches_processed\": batches_processed,\n                    \"domain_stats\": domain_stats,\n                    \"domain_ratios\": domain_ratios\n                }, f, indent=2)\n            \n            # ê²°ê³¼ ê²€ì¦\n            success = (\n                batches_processed >= 50 and  # ìµœì†Œ 50 ë°°ì¹˜ ì²˜ë¦¬\n                domain_stats_file.exists() and  # íŒŒì¼ ì €ì¥ë¨\n                len(domain_ratios) == 2  # 2ê°œ ë„ë©”ì¸ ë¶„ë¦¬ë¨\n            )\n            \n            details = f\"{batches_processed}ë°°ì¹˜ ì²˜ë¦¬, ë„ë©”ì¸ ë¹„ìœ¨: {domain_ratios}\"\n            \n            self.results.add_result(\n                \"test_3_sanity_validation_domain_separation\",\n                success,\n                details\n            )\n            \n        except Exception as e:\n            self.results.add_error(\"test_3_sanity_validation_domain_separation\", e)\n    \n    def test_4_auto_confidence_three_way_reflection(self) -> None:\n        \"\"\"í…ŒìŠ¤íŠ¸ 4: auto-confidenceê°€ 3ê³³ì— ì¼ê´€ ë°˜ì˜ë˜ëŠ”ì§€\"\"\"\n        try:\n            # Confidence íŠœë‹ ì„¤ì •\n            tuning_config = ConfidenceTuningConfig(\n                conf_min=0.20,\n                conf_max=0.30,\n                conf_step=0.05,  # í° ìŠ¤í…ìœ¼ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸\n                domains=[\"single\", \"combination\"]\n            )\n            \n            tuner = ConfidenceTuner(tuning_config)\n            \n            # Mock ì˜ˆì¸¡ ë°ì´í„°\n            import random\n            mock_predictions = []\n            mock_ground_truths = []\n            \n            for i in range(100):\n                conf = random.uniform(0.15, 0.35)\n                pred_class = random.randint(0, 10) if conf > 0.22 else -1\n                \n                mock_predictions.append({\n                    'confidence': conf,\n                    'predicted_class': pred_class\n                })\n                \n                mock_ground_truths.append({\n                    'true_class': random.randint(0, 10)\n                })\n            \n            # Mock ë„ë©”ì¸ ë§ˆìŠ¤í¬\n            mock_domain_masks = {\n                'single': torch.tensor([i < 75 for i in range(100)]),\n                'combination': torch.tensor([i >= 75 for i in range(100)])\n            }\n            \n            # Confidence íŠœë‹ ì‹¤í–‰\n            best_confidences = tuner.tune_confidence(\n                mock_predictions,\n                mock_ground_truths,\n                mock_domain_masks\n            )\n            \n            # Mock ì²´í¬í¬ì¸íŠ¸ ìƒì„± ë° ì ìš©\n            mock_checkpoint = {'model_state_dict': {}, 'epoch': 1}\n            updated_checkpoint = tuner.apply_to_checkpoint(mock_checkpoint)\n            \n            # 3ê³³ ë°˜ì˜ í™•ì¸\n            reflections = {\n                \"inference_config\": False,  # ì¶”ë¡  ì„¤ì • (ì‹¤ì œë¡œëŠ” config íŒŒì¼ ì—…ë°ì´íŠ¸)\n                \"checkpoint_meta\": 'optimal_confidences' in updated_checkpoint.get('meta', {}),\n                \"summary_report\": len(tuner.best_confidences) > 0  # ë¦¬í¬íŠ¸ ìƒì„±\n            }\n            \n            # ì¶”ë¡  ì„¤ì • ë°˜ì˜ í™•ì¸ (ê°„ì ‘ì )\n            try:\n                # ì‹¤ì œ config ì—…ë°ì´íŠ¸ëŠ” íŒŒì¼ ì“°ê¸°ê°€ í•„ìš”í•˜ë¯€ë¡œ \n                # best_confidences ì¡´ì¬ ì—¬ë¶€ë¡œ ê°„ì ‘ í™•ì¸\n                reflections[\"inference_config\"] = len(best_confidences) > 0\n            except Exception:\n                pass\n            \n            # ê²°ê³¼ ê²€ì¦\n            success = all(reflections.values())\n            reflection_details = \", \".join([\n                f\"{k}:{v}\" for k, v in reflections.items()\n            ])\n            \n            details = f\"Confidence: {best_confidences}, ë°˜ì˜: {reflection_details}\"\n            \n            self.results.add_result(\n                \"test_4_auto_confidence_three_way_reflection\",\n                success,\n                details\n            )\n            \n        except Exception as e:\n            self.results.add_error(\"test_4_auto_confidence_three_way_reflection\", e)\n    \n    def test_5_latency_breakdown_vram_peak_reporting(self) -> None:\n        \"\"\"í…ŒìŠ¤íŠ¸ 5: ë ˆì´í„´ì‹œ ë¶„í•´ì™€ VRAM peak ë¦¬í¬íŒ…\"\"\"\n        try:\n            # ìµœì†Œì…‹ ë¡œê¹… ì„¤ì •\n            logging_config = MinimalLoggingConfig(\n                pipeline_metrics=[\"det_ms\", \"crop_ms\", \"cls_ms\", \"total_ms\"],\n                system_metrics=[\"vram_current\", \"vram_peak\"],\n                track_percentiles=True\n            )\n            \n            # ì„ì‹œ ë¡œê¹… ë””ë ‰í† ë¦¬\n            temp_dir = self.create_temp_dir()\n            minimal_logger = MinimalLogger(logging_config, save_dir=str(temp_dir))\n            \n            # íŒŒì´í”„ë¼ì¸ íƒ€ì´ë° ì‹œë®¬ë ˆì´ì…˜\n            pipeline_operations = [\"det\", \"crop\", \"cls\"]\n            \n            for i in range(20):  # 20íšŒ ë°˜ë³µìœ¼ë¡œ í†µê³„ ìƒì„±\n                total_start = time.perf_counter()\n                \n                for op in pipeline_operations:\n                    minimal_logger.start_pipeline_timer(op)\n                    time.sleep(0.001)  # 1ms ì‹œë®¬ë ˆì´ì…˜\n                    minimal_logger.end_pipeline_timer(op)\n                \n                total_elapsed = (time.perf_counter() - total_start) * 1000\n                minimal_logger.record_pipeline_timing(\"total\", total_elapsed)\n            \n            # Mock ë©”íŠ¸ë¦­ìœ¼ë¡œ ë¡œê¹…\n            mock_metrics = {\n                \"classification\": {\"top1\": 0.75, \"macro_f1\": 0.68},\n                \"detection\": {\"map_0_5\": 0.45, \"recall\": 0.62},\n                \"loss\": 1.23\n            }\n            \n            minimal_logger.log_step(step=10, epoch=1, metrics=mock_metrics, force_log=True)\n            \n            # Gradient norm ê¸°ë¡\n            minimal_logger.record_grad_norm(1.45, before_clipping=False)\n            \n            # ìš”ì•½ í†µê³„ í™•ì¸\n            summary_stats = minimal_logger.get_summary_stats()\n            \n            # ê²°ê³¼ ê²€ì¦\n            pipeline_stats_exist = len(summary_stats.get(\"pipeline_stats\", {})) > 0\n            vram_peak_recorded = summary_stats.get(\"vram_peak_gb\", 0) >= 0\n            \n            # ë ˆì´í„´ì‹œ ë¶„í•´ í™•ì¸\n            pipeline_stats = summary_stats.get(\"pipeline_stats\", {})\n            latency_breakdown_complete = all(\n                op in pipeline_stats for op in [\"det\", \"crop\", \"cls\"]\n            )\n            \n            success = (\n                pipeline_stats_exist and\n                vram_peak_recorded and\n                latency_breakdown_complete\n            )\n            \n            details = (\n                f\"íŒŒì´í”„ë¼ì¸ í†µê³„: {len(pipeline_stats)}ê°œ, \"\n                f\"VRAM peak: {summary_stats.get('vram_peak_gb', 0):.2f}GB, \"\n                f\"ë ˆì´í„´ì‹œ ë¶„í•´: {latency_breakdown_complete}\"\n            )\n            \n            self.results.add_result(\n                \"test_5_latency_breakdown_vram_peak_reporting\",\n                success,\n                details\n            )\n            \n        except Exception as e:\n            self.results.add_error(\"test_5_latency_breakdown_vram_peak_reporting\", e)\n    \n    def run_all_tests(self) -> SmokeTestResults:\n        \"\"\"ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰\"\"\"\n        tests = [\n            self.test_1_config_duplicate_key_detection,\n            self.test_2_small_batch_training_completion,\n            self.test_3_sanity_validation_domain_separation,\n            self.test_4_auto_confidence_three_way_reflection,\n            self.test_5_latency_breakdown_vram_peak_reporting\n        ]\n        \n        for i, test_func in enumerate(tests, 1):\n            print(f\"\\nğŸ§ª í…ŒìŠ¤íŠ¸ {i}/5 ì‹¤í–‰: {test_func.__name__}\")\n            try:\n                test_func()\n            except Exception as e:\n                self.logger.error(f\"í…ŒìŠ¤íŠ¸ {i} ì¹˜ëª…ì  ì˜¤ë¥˜: {e}\")\n                self.results.add_error(test_func.__name__, e)\n        \n        return self.results\n\n\ndef main():\n    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n    with Stage3SmokeTestRunner() as runner:\n        results = runner.run_all_tests()\n        \n        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n        results.print_summary()\n        \n        # ìµœì¢… ì„±ê³µ ì—¬ë¶€ ë°˜í™˜\n        summary = results.get_summary()\n        return summary['failed_tests'] == 0\n\n\nif __name__ == \"__main__\":\n    success = main()\n    \n    if success:\n        print(\"\\nğŸ‰ 1ë‹¨ê³„ ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ì™„ì „ í†µê³¼!\")\n        print(\"   Stage 3 ì¬í•™ìŠµì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n        sys.exit(0)\n    else:\n        print(\"\\nğŸ’¥ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n        print(\"   ë¬¸ì œë¥¼ í•´ê²°í•œ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.\")\n        sys.exit(1)