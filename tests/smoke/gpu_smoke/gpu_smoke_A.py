#!/usr/bin/env python3
"""
GPU-A ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸: ìˆœìˆ˜ PyTorch í•©ì„± ë°ì´í„° (RTX 5080)
ëª©ì : PyTorch 2.7.0+cu128ì—ì„œ ê¸°ë³¸ GPU ì—°ì‚° ë° sm_120 ìµœì í™” ê²€ì¦
"""
import time, json, torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from pathlib import Path

# RTX 5080 ìµœì í™” ì„¤ì •
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True

class GPUSynthDataset(Dataset):
    def __init__(self, n=64, num_classes=19, img_size=224):
        self.n, self.C, self.size = n, num_classes, img_size
    
    def __len__(self): 
        return self.n
    
    def __getitem__(self, i):
        # RTX 5080 ë©”ëª¨ë¦¬ í™œìš©ì„ ìœ„í•´ ë” í° ì´ë¯¸ì§€
        x = torch.rand(3, self.size, self.size)
        y = torch.randint(0, self.C, ())
        return x, y

class GPUNet(nn.Module):
    """RTX 5080 ìµœì í™” ëª¨ë¸ (EfficientNet-B0 í¬ê¸°)"""
    def __init__(self, num_classes=19):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),
            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.BatchNorm2d(128), nn.ReLU(),
            nn.Conv2d(128, 256, 3, stride=2, padding=1), nn.BatchNorm2d(256), nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        return self.classifier(x)

def main():
    print("ğŸš€ GPU-A Smoke Test (RTX 5080 + PyTorch 2.7.0)")
    
    # GPU í™•ì¸
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA not available")
    
    device = torch.device("cuda")
    gpu_name = torch.cuda.get_device_name(0)
    capability = torch.cuda.get_device_capability(0)
    print(f"ğŸ”§ Device: {gpu_name} (sm_{capability[0]}{capability[1]})")
    
    start_time = time.time()
    run_tag = f"GPU_A_synth_{int(time.time())}"
    output_dir = Path(f"artifacts/gpu_runs/{run_tag}")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ë°ì´í„°ì…‹ (RTX 5080 ë©”ëª¨ë¦¬ í™œìš©)
    dataset = GPUSynthDataset(n=128, num_classes=19, img_size=224)
    loader = DataLoader(dataset, batch_size=16, shuffle=True, 
                       num_workers=4, pin_memory=True)
    
    # ëª¨ë¸
    model = GPUNet(19).to(device)
    
    # torch.compile ìµœì í™” (PyTorch 2.7.0) - ê°œë°œí™˜ê²½ì—ì„œ í—¤ë” íŒŒì¼ ì´ìŠˆë¡œ ë¹„í™œì„±í™”
    # try:
    #     model = torch.compile(model, mode='max-autotune')
    #     print("âœ… torch.compile enabled")
    # except Exception as e:
    #     print(f"âš ï¸ torch.compile failed: {e}")
    print("â„¹ï¸ torch.compile disabled (dev environment)")
    
    optimizer = optim.AdamW(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()
    
    # Mixed Precision í•™ìŠµ
    scaler = torch.amp.GradScaler('cuda')
    
    model.train()
    losses = []
    
    print("ğŸƒ GPU training started...")
    for i, (images, targets) in enumerate(loader):
        images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)
        
        optimizer.zero_grad(set_to_none=True)
        
        with torch.amp.autocast('cuda'):
            outputs = model(images)
            loss = criterion(outputs, targets)
        
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        losses.append(loss.item())
        print(f"  Batch {i+1}/{len(loader)}: loss={loss.item():.4f}, GPU_mem={torch.cuda.memory_allocated()/1e9:.1f}GB")
        
        if i >= 4:  # 5ë°°ì¹˜ í•™ìŠµ
            break
    
    avg_loss = sum(losses) / len(losses)
    
    # GPU ë©”ëª¨ë¦¬ ì •ë³´
    memory_stats = {
        "allocated": torch.cuda.memory_allocated() / 1e9,
        "max_allocated": torch.cuda.max_memory_allocated() / 1e9,
        "reserved": torch.cuda.memory_reserved() / 1e9,
        "max_reserved": torch.cuda.max_memory_reserved() / 1e9,
    }
    
    # ì €ì¥
    torch.save({
        "model_state_dict": model.state_dict(),
        "avg_loss": avg_loss,
        "device": str(device),
        "gpu_name": gpu_name,
        "capability": capability,
        "memory_stats": memory_stats
    }, output_dir / "gpu_checkpoint.pt")
    
    metrics = {
        "success": True,
        "device": str(device),
        "gpu_name": gpu_name,
        "compute_capability": f"sm_{capability[0]}{capability[1]}",
        "pytorch_version": torch.__version__,
        "cuda_version": torch.version.cuda,
        "avg_loss": avg_loss,
        "batches_processed": len(losses),
        "memory_peak_gb": memory_stats["max_allocated"],
        "elapsed_seconds": time.time() - start_time,
        "run_tag": run_tag,
        "torch_compile": "enabled" if hasattr(model, '_compiled') else "disabled"
    }
    
    (output_dir / "metrics.json").write_text(json.dumps(metrics, indent=2))
    
    elapsed = time.time() - start_time
    print(f"âœ… GPU-A test completed in {elapsed:.1f}s")
    print(f"ğŸ“Š Average loss: {avg_loss:.4f}")
    print(f"ğŸ¯ Peak GPU memory: {memory_stats['max_allocated']:.1f}GB")
    print(f"ğŸ“ Results: {output_dir}")
    
    return metrics

if __name__ == "__main__":
    result = main()
    print(f"ğŸ‰ SUCCESS: {result['run_tag']}")