"""
Stage 3 GPU ë©”ëª¨ë¦¬ ì•ˆì •ì„± ë° ëˆ„ìˆ˜ íƒì§€ í…ŒìŠ¤íŠ¸

RTX 5080 16GB í™˜ê²½ì—ì„œì˜ ì² ì €í•œ GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ê²€ì¦:
- CUDA ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€
- OOM(Out of Memory) ë°©ì§€ ë©”ì»¤ë‹ˆì¦˜ ê²€ì¦
- ì¥ì‹œê°„ í•™ìŠµ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
- Mixed Precision ë° torch.compile ìµœì í™” ê²€ì¦
- ë©”ëª¨ë¦¬ Fragment ë°©ì§€
- ìë™ ë°°ì¹˜ í¬ê¸° ì¡°ì • ê²€ì¦
"""

import pytest
import torch
import torch.nn as nn
import torch.optim as optim
import gc
import time
import threading
import psutil
import numpy as np
from pathlib import Path
from unittest.mock import patch, MagicMock
import warnings
import contextlib

import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent))


class CUDAMemoryTracker:
    """CUDA ë©”ëª¨ë¦¬ ì¶”ì ê¸°"""
    
    def __init__(self):
        self.snapshots = []
        self.baseline = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
    
    def take_snapshot(self, label=""):
        """ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ì´¬ì˜"""
        if not torch.cuda.is_available():
            return None
            
        snapshot = {
            'label': label,
            'timestamp': time.time(),
            'allocated': torch.cuda.memory_allocated(),
            'reserved': torch.cuda.memory_reserved(),
            'max_allocated': torch.cuda.max_memory_allocated(),
            'max_reserved': torch.cuda.max_memory_reserved()
        }
        
        self.snapshots.append(snapshot)
        
        if self.baseline is None:
            self.baseline = snapshot
            
        return snapshot
    
    def get_memory_increase(self):
        """ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ ê³„ì‚°"""
        if not self.snapshots or not torch.cuda.is_available():
            return 0, 0
            
        current = self.snapshots[-1]
        allocated_increase = current['allocated'] - self.baseline['allocated']
        reserved_increase = current['reserved'] - self.baseline['reserved']
        
        return allocated_increase, reserved_increase
    
    def detect_memory_leak(self, threshold_mb=100):
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€"""
        if len(self.snapshots) < 2:
            return False, "ìŠ¤ëƒ…ìƒ· ë¶€ì¡±"
            
        # ìµœê·¼ 10ê°œ ìŠ¤ëƒ…ìƒ·ì—ì„œ ì¶”ì„¸ ë¶„ì„
        recent_snapshots = self.snapshots[-10:]
        allocated_values = [s['allocated'] for s in recent_snapshots]
        
        if len(allocated_values) < 2:
            return False, "ë°ì´í„° ë¶€ì¡±"
            
        # ì„ í˜• íšŒê·€ë¡œ ì¶”ì„¸ ê³„ì‚°
        x = np.arange(len(allocated_values))
        slope = np.polyfit(x, allocated_values, 1)[0]
        
        threshold_bytes = threshold_mb * 1024 * 1024
        leak_detected = slope > threshold_bytes
        
        return leak_detected, f"ë©”ëª¨ë¦¬ ì¦ê°€ ì¶”ì„¸: {slope/1024/1024:.1f}MB/ìŠ¤ëƒ…ìƒ·"


class MockEfficientNetV2:
    """EfficientNetV2-L ëª¨ì˜ ëª¨ë¸"""
    
    def __init__(self, num_classes=1000):
        if torch.cuda.is_available():
            device = torch.device('cuda')
        else:
            device = torch.device('cpu')
            
        # EfficientNetV2-Lê³¼ ìœ ì‚¬í•œ í¬ê¸°ì˜ ëª¨ë¸ ìƒì„±
        self.model = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(64, num_classes)
        ).to(device)
        
        self.device = device
    
    def __call__(self, x):
        return self.model(x)
    
    def parameters(self):
        return self.model.parameters()
    
    def train(self):
        return self.model.train()
    
    def eval(self):
        return self.model.eval()


class TestStage3GPUMemoryStability:
    """Stage 3 GPU ë©”ëª¨ë¦¬ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def memory_tracker(self):
        """ë©”ëª¨ë¦¬ ì¶”ì ê¸°"""
        tracker = CUDAMemoryTracker()
        tracker.take_snapshot("test_start")
        yield tracker
        
        # ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    @pytest.fixture
    def mock_model(self):
        """ëª¨ì˜ ëª¨ë¸"""
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")
            
        model = MockEfficientNetV2()
        yield model
        
        # ì •ë¦¬
        del model
        torch.cuda.empty_cache()
    
    def test_baseline_memory_usage(self, memory_tracker, mock_model):
        """ê¸°ë³¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
        print("ğŸ“Š ê¸°ë³¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •")
        
        # ëª¨ë¸ ë¡œë”© í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory_tracker.take_snapshot("model_loaded")
        
        # ì˜µí‹°ë§ˆì´ì € ìƒì„±
        optimizer = optim.AdamW(mock_model.parameters(), lr=1e-4)
        memory_tracker.take_snapshot("optimizer_created")
        
        # Mixed Precision Scaler
        scaler = torch.amp.GradScaler()
        memory_tracker.take_snapshot("scaler_created")
        
        # ë² ì´ìŠ¤ë¼ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
        snapshots = memory_tracker.snapshots
        
        for i, snapshot in enumerate(snapshots):
            allocated_mb = snapshot['allocated'] / 1024 / 1024
            reserved_mb = snapshot['reserved'] / 1024 / 1024
            print(f"  {snapshot['label']}: í• ë‹¹={allocated_mb:.1f}MB, ì˜ˆì•½={reserved_mb:.1f}MB")
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ RTX 5080 16GBì˜ 80% ì´í•˜ì—¬ì•¼ í•¨
        final_allocated = snapshots[-1]['allocated']
        max_allowed = 16 * 1024 * 1024 * 1024 * 0.8  # 12.8GB
        
        assert final_allocated < max_allowed, f"ë² ì´ìŠ¤ë¼ì¸ ë©”ëª¨ë¦¬ ê³¼ë‹¤: {final_allocated/1e9:.1f}GB > 12.8GB"
        print(f"âœ… ë² ì´ìŠ¤ë¼ì¸ ë©”ëª¨ë¦¬ ê²€ì¦ í†µê³¼: {final_allocated/1e9:.1f}GB")
    
    def test_training_batch_memory_management(self, memory_tracker, mock_model):
        """í•™ìŠµ ë°°ì¹˜ ë©”ëª¨ë¦¬ ê´€ë¦¬ í…ŒìŠ¤íŠ¸"""
        print("ğŸ”„ í•™ìŠµ ë°°ì¹˜ ë©”ëª¨ë¦¬ ê´€ë¦¬ í…ŒìŠ¤íŠ¸")
        
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")
        
        device = mock_model.device
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(mock_model.parameters(), lr=1e-4)
        scaler = torch.amp.GradScaler()
        
        batch_sizes = [4, 8, 16, 32]  # ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸
        memory_results = {}
        
        for batch_size in batch_sizes:
            try:
                print(f"  ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸...")
                
                # ë°°ì¹˜ ë°ì´í„° ìƒì„±
                images = torch.randn(batch_size, 3, 384, 384, device=device, dtype=torch.float16)
                labels = torch.randint(0, 1000, (batch_size,), device=device)
                
                memory_tracker.take_snapshot(f"batch_{batch_size}_created")
                
                # Forward pass with Mixed Precision
                with torch.amp.autocast(device_type='cuda'):
                    outputs = mock_model(images)
                    loss = criterion(outputs, labels)
                
                memory_tracker.take_snapshot(f"batch_{batch_size}_forward")
                
                # Backward pass
                scaler.scale(loss).backward()
                
                memory_tracker.take_snapshot(f"batch_{batch_size}_backward")
                
                # Optimizer step
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                
                memory_tracker.take_snapshot(f"batch_{batch_size}_optimizer")
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡
                current_allocated = torch.cuda.memory_allocated()
                current_reserved = torch.cuda.memory_reserved()
                
                memory_results[batch_size] = {
                    'allocated_mb': current_allocated / 1024 / 1024,
                    'reserved_mb': current_reserved / 1024 / 1024,
                    'success': True
                }
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del images, labels, outputs, loss
                torch.cuda.empty_cache()
                
                print(f"    âœ… ë°°ì¹˜ í¬ê¸° {batch_size}: {current_allocated/1024/1024:.1f}MB ì‚¬ìš©")
                
            except torch.cuda.OutOfMemoryError:
                memory_results[batch_size] = {'success': False, 'error': 'OOM'}
                print(f"    âŒ ë°°ì¹˜ í¬ê¸° {batch_size}: CUDA OOM")
                torch.cuda.empty_cache()
                
            except Exception as e:
                memory_results[batch_size] = {'success': False, 'error': str(e)}
                print(f"    âŒ ë°°ì¹˜ í¬ê¸° {batch_size}: {e}")
                torch.cuda.empty_cache()
        
        # ê²°ê³¼ ë¶„ì„
        successful_batches = [bs for bs, result in memory_results.items() if result['success']]
        max_successful_batch = max(successful_batches) if successful_batches else 0
        
        print(f"  ìµœëŒ€ ì„±ê³µ ë°°ì¹˜ í¬ê¸°: {max_successful_batch}")
        
        # ìµœì†Œ ë°°ì¹˜ í¬ê¸° 8ì€ ì„±ê³µí•´ì•¼ í•¨ (Stage 3 ìš”êµ¬ì‚¬í•­)
        assert 8 in successful_batches, "ë°°ì¹˜ í¬ê¸° 8 ì²˜ë¦¬ ì‹¤íŒ¨"
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê²€ì¦
        if 8 in memory_results:
            batch8_memory = memory_results[8]['allocated_mb']
            assert batch8_memory < 8000, f"ë°°ì¹˜ í¬ê¸° 8 ë©”ëª¨ë¦¬ ê³¼ë‹¤: {batch8_memory:.1f}MB > 8000MB"
        
        print(f"âœ… ë°°ì¹˜ ë©”ëª¨ë¦¬ ê´€ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼")
    
    def test_memory_leak_detection(self, memory_tracker, mock_model):
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€ í…ŒìŠ¤íŠ¸"""
        print("ğŸ” ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€ í…ŒìŠ¤íŠ¸")
        
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")
        
        device = mock_model.device
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(mock_model.parameters(), lr=1e-4)
        scaler = torch.amp.GradScaler()
        
        # ë°˜ë³µì  í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ (100 ì´í„°ë ˆì´ì…˜)
        batch_size = 8
        num_iterations = 100
        
        for iteration in range(num_iterations):
            # ë°°ì¹˜ ë°ì´í„° ìƒì„±
            images = torch.randn(batch_size, 3, 384, 384, device=device, dtype=torch.float16)
            labels = torch.randint(0, 1000, (batch_size,), device=device)
            
            # í•™ìŠµ ìŠ¤í…
            optimizer.zero_grad()
            
            with torch.amp.autocast(device_type='cuda'):
                outputs = mock_model(images)
                loss = criterion(outputs, labels)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del images, labels, outputs, loss
            
            # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·
            if (iteration + 1) % 10 == 0:
                memory_tracker.take_snapshot(f"iteration_{iteration+1}")
                
                # ì¤‘ê°„ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                if (iteration + 1) % 50 == 0:
                    gc.collect()
                    torch.cuda.empty_cache()
        
        # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¶„ì„
        leak_detected, message = memory_tracker.detect_memory_leak(threshold_mb=50)  # 50MB ì„ê³„ê°’
        
        print(f"  ì´ ì´í„°ë ˆì´ì…˜: {num_iterations}")
        print(f"  ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¶„ì„: {message}")
        
        assert not leak_detected, f"ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€ë¨: {message}"
        print(f"âœ… ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€ í…ŒìŠ¤íŠ¸ í†µê³¼")
    
    @pytest.mark.slow
    def test_long_term_stability(self, memory_tracker, mock_model):
        """ì¥ì‹œê°„ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"""
        print("ğŸ• ì¥ì‹œê°„ GPU ë©”ëª¨ë¦¬ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ (5ë¶„)")
        
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")
        
        device = mock_model.device
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(mock_model.parameters(), lr=1e-4)
        scaler = torch.amp.GradScaler()
        
        start_time = time.time()
        test_duration = 300  # 5ë¶„
        iteration = 0
        stability_violations = 0
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ
        monitoring_active = threading.Event()
        monitoring_active.set()
        memory_violations = []
        
        def memory_monitor():
            """ë°±ê·¸ë¼ìš´ë“œ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§"""
            while monitoring_active.is_set():
                if torch.cuda.is_available():
                    allocated = torch.cuda.memory_allocated()
                    reserved = torch.cuda.memory_reserved()
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ 14GBë¥¼ ì´ˆê³¼í•˜ë©´ ìœ„ë°˜
                    if allocated > 14 * 1024 * 1024 * 1024:
                        memory_violations.append({
                            'time': time.time() - start_time,
                            'allocated_gb': allocated / 1e9
                        })
                
                time.sleep(10)  # 10ì´ˆë§ˆë‹¤ ì²´í¬
        
        monitor_thread = threading.Thread(target=memory_monitor, daemon=True)
        monitor_thread.start()
        
        try:
            while time.time() - start_time < test_duration:
                iteration += 1
                
                # ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸ (í˜„ì‹¤ì  ë³€ë™ì„±)
                batch_size = 4 if iteration % 10 == 0 else 8
                
                images = torch.randn(batch_size, 3, 384, 384, device=device, dtype=torch.float16)
                labels = torch.randint(0, 1000, (batch_size,), device=device)
                
                optimizer.zero_grad()
                
                with torch.amp.autocast(device_type='cuda'):
                    outputs = mock_model(images)
                    loss = criterion(outputs, labels)
                
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
                
                del images, labels, outputs, loss
                
                # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ê´€ë¦¬
                if iteration % 100 == 0:
                    torch.cuda.empty_cache()
                    memory_tracker.take_snapshot(f"stability_iter_{iteration}")
                    
                    elapsed = time.time() - start_time
                    print(f"    ì§„í–‰: {iteration:,} ì´í„°ë ˆì´ì…˜, {elapsed:.0f}ì´ˆ ê²½ê³¼")
                
                # CPU ì‚¬ìš©ë¥ ë„ ì²´í¬
                if iteration % 1000 == 0:
                    cpu_percent = psutil.cpu_percent()
                    if cpu_percent > 95:
                        stability_violations += 1
        
        except Exception as e:
            print(f"    âš ï¸ ì¥ì‹œê°„ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜ˆì™¸: {e}")
            stability_violations += 1
        
        finally:
            monitoring_active.clear()
            monitor_thread.join(timeout=1)
        
        elapsed_time = time.time() - start_time
        
        print(f"  ì´ ì‹¤í–‰ ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
        print(f"  ì´ ì´í„°ë ˆì´ì…˜: {iteration:,}")
        print(f"  ì•ˆì •ì„± ìœ„ë°˜: {stability_violations}")
        print(f"  ë©”ëª¨ë¦¬ ìœ„ë°˜: {len(memory_violations)}")
        
        # ì•ˆì •ì„± ê²€ì¦
        assert stability_violations == 0, f"ì•ˆì •ì„± ìœ„ë°˜ ë°œìƒ: {stability_violations}ê±´"
        assert len(memory_violations) == 0, f"ë©”ëª¨ë¦¬ í•œê³„ ì´ˆê³¼: {len(memory_violations)}ê±´"
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê²€ì‚¬
        final_leak, leak_message = memory_tracker.detect_memory_leak(threshold_mb=100)
        assert not final_leak, f"ì¥ì‹œê°„ ì‹¤í–‰ í›„ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜: {leak_message}"
        
        print(f"âœ… ì¥ì‹œê°„ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ í†µê³¼")
    
    def test_oom_recovery_mechanism(self, memory_tracker):
        """OOM ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸"""
        print("ğŸ›¡ï¸ OOM ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸")
        
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")
        
        device = torch.device('cuda')
        recovery_successful = False
        
        # ì˜ë„ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ë¶€ì¡± ìƒí™© ë°œìƒ
        try:
            # ë§¤ìš° í° í…ì„œ ìƒì„±ìœ¼ë¡œ OOM ìœ ë°œ
            huge_tensor = torch.randn(10000, 10000, 10000, device=device)
            
        except torch.cuda.OutOfMemoryError:
            print("    OOM ì˜ˆì™¸ ë°œìƒ (ì˜ˆìƒë¨)")
            
            # ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ ì‹¤í–‰
            try:
                torch.cuda.empty_cache()
                gc.collect()
                
                # ë³µêµ¬ í›„ ì •ìƒ ì‘ì—… ìˆ˜í–‰
                normal_tensor = torch.randn(100, 100, device=device)
                result = normal_tensor.sum()
                
                recovery_successful = True
                print(f"    ë³µêµ¬ ì„±ê³µ: í…ì„œ ìƒì„± ë° ì—°ì‚° ì™„ë£Œ")
                
                del normal_tensor
                
            except Exception as recovery_error:
                print(f"    ë³µêµ¬ ì‹¤íŒ¨: {recovery_error}")
        
        except Exception as e:
            print(f"    ì˜ˆìƒì¹˜ ëª»í•œ ì˜ˆì™¸: {e}")
        
        # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        torch.cuda.empty_cache()
        memory_tracker.take_snapshot("after_oom_test")
        
        current_allocated = torch.cuda.memory_allocated()
        print(f"  OOM í…ŒìŠ¤íŠ¸ í›„ ë©”ëª¨ë¦¬: {current_allocated/1024/1024:.1f}MB")
        
        # ë³µêµ¬ ì„±ê³µ ì—¬ë¶€ ê²€ì¦
        assert recovery_successful, "OOM ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ ì‹¤íŒ¨"
        
        # ë©”ëª¨ë¦¬ê°€ ì •ìƒì ìœ¼ë¡œ í•´ì œë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert current_allocated < 1024 * 1024 * 1024, "OOM í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨"  # 1GB ì´í•˜
        
        print(f"âœ… OOM ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸ í†µê³¼")
    
    def test_mixed_precision_memory_efficiency(self, memory_tracker, mock_model):
        """Mixed Precision ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸"""
        print("âš¡ Mixed Precision ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸")
        
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")
        
        device = mock_model.device
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(mock_model.parameters(), lr=1e-4)
        
        batch_size = 16
        
        # FP32 ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        print("  FP32 ëª¨ë“œ ë©”ëª¨ë¦¬ ì¸¡ì •...")
        
        images_fp32 = torch.randn(batch_size, 3, 384, 384, device=device, dtype=torch.float32)
        labels = torch.randint(0, 1000, (batch_size,), device=device)
        
        optimizer.zero_grad()
        outputs = mock_model(images_fp32)
        loss = criterion(outputs, labels)
        loss.backward()
        
        memory_tracker.take_snapshot("fp32_peak")
        fp32_memory = torch.cuda.memory_allocated()
        
        del images_fp32, labels, outputs, loss
        torch.cuda.empty_cache()
        
        # Mixed Precision ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        print("  Mixed Precision ëª¨ë“œ ë©”ëª¨ë¦¬ ì¸¡ì •...")
        
        scaler = torch.amp.GradScaler()
        images_fp16 = torch.randn(batch_size, 3, 384, 384, device=device, dtype=torch.float16)
        labels = torch.randint(0, 1000, (batch_size,), device=device)
        
        optimizer.zero_grad()
        
        with torch.amp.autocast(device_type='cuda'):
            outputs = mock_model(images_fp16)
            loss = criterion(outputs, labels)
        
        scaler.scale(loss).backward()
        
        memory_tracker.take_snapshot("mixed_precision_peak")
        mixed_precision_memory = torch.cuda.memory_allocated()
        
        del images_fp16, labels, outputs, loss
        torch.cuda.empty_cache()
        
        # ë©”ëª¨ë¦¬ ì ˆì•½ëŸ‰ ê³„ì‚°
        memory_savings = fp32_memory - mixed_precision_memory
        savings_percent = (memory_savings / fp32_memory) * 100
        
        print(f"  FP32 ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {fp32_memory/1024/1024:.1f}MB")
        print(f"  Mixed Precision ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {mixed_precision_memory/1024/1024:.1f}MB")
        print(f"  ë©”ëª¨ë¦¬ ì ˆì•½: {memory_savings/1024/1024:.1f}MB ({savings_percent:.1f}%)")
        
        # Mixed Precisionì´ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•´ì•¼ í•¨
        assert memory_savings > 0, "Mixed Precision ë©”ëª¨ë¦¬ ì ˆì•½ íš¨ê³¼ ì—†ìŒ"
        assert savings_percent > 10, f"Mixed Precision íš¨ìœ¨ì„± ë¶€ì¡±: {savings_percent:.1f}% < 10%"
        
        print(f"âœ… Mixed Precision ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸ í†µê³¼")
    
    def test_torch_compile_memory_impact(self, memory_tracker, mock_model):
        """torch.compile ë©”ëª¨ë¦¬ ì˜í–¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ”§ torch.compile ë©”ëª¨ë¦¬ ì˜í–¥ í…ŒìŠ¤íŠ¸")
        
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")
        
        device = mock_model.device
        
        # ì»´íŒŒì¼ ì „ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory_tracker.take_snapshot("before_compile")
        baseline_memory = torch.cuda.memory_allocated()
        
        try:
            # ëª¨ë¸ ì»´íŒŒì¼
            compiled_model = torch.compile(mock_model.model, mode="reduce-overhead")
            memory_tracker.take_snapshot("after_compile")
            
            # ì»´íŒŒì¼ëœ ëª¨ë¸ë¡œ ì¶”ë¡ 
            with torch.no_grad():
                test_input = torch.randn(4, 3, 384, 384, device=device, dtype=torch.float16)
                _ = compiled_model(test_input)
            
            memory_tracker.take_snapshot("after_compiled_inference")
            final_memory = torch.cuda.memory_allocated()
            
            memory_increase = final_memory - baseline_memory
            
            print(f"  ì»´íŒŒì¼ ì „ ë©”ëª¨ë¦¬: {baseline_memory/1024/1024:.1f}MB")
            print(f"  ì»´íŒŒì¼ í›„ ë©”ëª¨ë¦¬: {final_memory/1024/1024:.1f}MB")
            print(f"  ë©”ëª¨ë¦¬ ì¦ê°€: {memory_increase/1024/1024:.1f}MB")
            
            # torch.compileë¡œ ì¸í•œ ë©”ëª¨ë¦¬ ì¦ê°€ê°€ í•©ë¦¬ì  ë²”ìœ„ ë‚´ì—¬ì•¼ í•¨
            max_allowed_increase = 2 * 1024 * 1024 * 1024  # 2GB
            assert memory_increase < max_allowed_increase, f"torch.compile ë©”ëª¨ë¦¬ ì¦ê°€ ê³¼ë‹¤: {memory_increase/1e9:.1f}GB"
            
            print(f"âœ… torch.compile ë©”ëª¨ë¦¬ ì˜í–¥ í…ŒìŠ¤íŠ¸ í†µê³¼")
            
        except Exception as e:
            print(f"  torch.compile ì§€ì›ë˜ì§€ ì•ŠìŒ ë˜ëŠ” ì˜¤ë¥˜: {e}")
            pytest.skip("torch.compile not supported")
    
    def test_gradient_accumulation_memory(self, memory_tracker, mock_model):
        """Gradient Accumulation ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸"""
        print("ğŸ“ˆ Gradient Accumulation ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸")
        
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")
        
        device = mock_model.device
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(mock_model.parameters(), lr=1e-4)
        scaler = torch.amp.GradScaler()
        
        # ë‹¤ì–‘í•œ accumulation stepì— ëŒ€í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        accumulation_steps = [1, 2, 4]
        memory_results = {}
        
        for accum_steps in accumulation_steps:
            print(f"  Accumulation Steps {accum_steps} í…ŒìŠ¤íŠ¸...")
            
            torch.cuda.empty_cache()
            memory_tracker.take_snapshot(f"accum_{accum_steps}_start")
            
            optimizer.zero_grad()
            peak_memory = 0
            
            for step in range(accum_steps):
                batch_size = 8 // accum_steps  # ì´ effective batch sizeëŠ” 8ë¡œ ë™ì¼
                
                images = torch.randn(batch_size, 3, 384, 384, device=device, dtype=torch.float16)
                labels = torch.randint(0, 1000, (batch_size,), device=device)
                
                with torch.amp.autocast(device_type='cuda'):
                    outputs = mock_model(images)
                    loss = criterion(outputs, labels) / accum_steps  # ì •ê·œí™”
                
                scaler.scale(loss).backward()
                
                current_memory = torch.cuda.memory_allocated()
                peak_memory = max(peak_memory, current_memory)
                
                del images, labels, outputs, loss
            
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            
            memory_tracker.take_snapshot(f"accum_{accum_steps}_peak")
            memory_results[accum_steps] = peak_memory
            
            print(f"    Peak ë©”ëª¨ë¦¬: {peak_memory/1024/1024:.1f}MB")
        
        # Gradient Accumulationì´ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ëŠ”ì§€ í™•ì¸
        base_memory = memory_results[1]
        accum4_memory = memory_results[4]
        
        memory_savings = base_memory - accum4_memory
        
        print(f"  1 step vs 4 steps ë©”ëª¨ë¦¬ ì ˆì•½: {memory_savings/1024/1024:.1f}MB")
        
        # 4-step accumulationì´ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•´ì•¼ í•¨
        assert memory_savings > 0, "Gradient Accumulation ë©”ëª¨ë¦¬ ì ˆì•½ íš¨ê³¼ ì—†ìŒ"
        
        print(f"âœ… Gradient Accumulation ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s", "--tb=short"])