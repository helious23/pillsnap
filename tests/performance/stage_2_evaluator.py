#!/usr/bin/env python3
"""
Stage 2 Performance Evaluator
ì„±ëŠ¥ ê¸°ì¤€ì„  í™•ë¦½ì„ ìœ„í•œ í‰ê°€ ì‹œìŠ¤í…œ

ëª©í‘œ:
- Classification accuracy â‰¥ 0.60 (250í´ë˜ìŠ¤)
- Detection mAP@0.5 â‰¥ 0.50 
- Auto Batch íŠœë‹ ì„±ê³µ í™•ì¸
- 128GB RAM ìµœì í™” ê²€ì¦
"""

import os
import json
import time
import torch
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass

from src.utils.core import PillSnapLogger


@dataclass
class Stage2Targets:
    """Stage 2 ì„±ëŠ¥ ëª©í‘œ"""
    # í•„ìˆ˜ ì²´í¬ (ì‹¤ì œ ì¤‘ìš”í•œ ê²ƒë§Œ)
    mandatory_checks = [
        "training_completed",
        "model_saved",
        "memory_optimization_working"
    ]
    
    # ì„±ëŠ¥ ëª©í‘œ (Stage 2ëŠ” 250í´ë˜ìŠ¤ ê¸°ì¤€ì„ )
    classification_accuracy = 0.40  # 40% (Stage 2 ëª©í‘œ)
    classification_macro_f1 = 0.35
    classification_top5_accuracy = 0.60
    
    # ê²€ì¶œ ì„±ëŠ¥ (ë‚˜ì¤‘ì— êµ¬í˜„ë  ì˜ˆì •)
    detection_map_0_5 = 0.50  # mAP@0.5
    detection_precision = 0.45
    detection_recall = 0.40
    
    # ì‹œìŠ¤í…œ ì„±ëŠ¥
    batch_size_minimum = 32
    gpu_memory_limit_gb = 14.0
    throughput_img_s = 80


class OptimizationAdvisor:
    """Stage 2 ìµœì í™” ê¶Œì¥ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = PillSnapLogger(__name__)
        self.targets = Stage2Targets()
    
    def evaluate_stage_2(self, exp_dir: str = "/home/max16/pillsnap_data/exp/exp01") -> Dict[str, Any]:
        """Stage 2 ì¢…í•© í‰ê°€"""
        
        self.logger.step("Stage 2 í‰ê°€ ì‹œì‘", "ì„±ëŠ¥ ê¸°ì¤€ì„  í™•ë¦½ ê²€ì¦")
        
        # 1. í•„ìˆ˜ ì²´í¬ ìˆ˜í–‰
        mandatory_results = self._check_mandatory_requirements(exp_dir)
        
        # 2. ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘
        performance_results = self._collect_performance_metrics(exp_dir)
        
        # 3. ì‹œìŠ¤í…œ ì•ˆì •ì„± í™•ì¸
        system_results = self._check_system_stability(exp_dir)
        
        # 4. ì¢…í•© í‰ê°€ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendation = self._generate_recommendation(
            mandatory_results, performance_results, system_results
        )
        
        # 5. í‰ê°€ ê²°ê³¼ ì €ì¥
        evaluation_report = {
            "stage": 2,
            "timestamp": time.time(),
            "mandatory_checks": mandatory_results,
            "performance_metrics": performance_results,
            "system_metrics": system_results,
            "recommendation": recommendation,
            "targets": {
                "classification_accuracy": self.targets.classification_accuracy,
                "detection_map_0_5": self.targets.detection_map_0_5,
                "throughput_target": self.targets.throughput_img_s
            }
        }
        
        self._save_evaluation_report(evaluation_report, exp_dir)
        
        # 6. ì‚¬ìš©ìì—ê²Œ ê¶Œì¥ì‚¬í•­ í‘œì‹œ
        self._present_recommendation_to_user(recommendation)
        
        return evaluation_report
    
    def _check_mandatory_requirements(self, exp_dir: str) -> Dict[str, bool]:
        """í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ ì²´í¬ - ì‹¤ì œ ì¤‘ìš”í•œ ê²ƒë§Œ"""
        
        results = {}
        
        # 1. í•™ìŠµ ì™„ë£Œ í™•ì¸ (ê°€ì¥ ì¤‘ìš”)
        try:
            # Stage 2 ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
            artifacts_dir = Path("artifacts/models/classification")
            stage2_model = artifacts_dir / "best_classifier_250classes.pt"
            
            if stage2_model.exists():
                # ëª¨ë¸ ë¡œë“œí•´ì„œ ì •í™•ë„ í™•ì¸
                checkpoint = torch.load(stage2_model, map_location='cpu')
                best_acc = checkpoint.get("best_accuracy", 0.0)
                results["training_completed"] = best_acc > 0.1  # ìµœì†Œí•œì˜ í•™ìŠµì´ ì´ë£¨ì–´ì¡ŒëŠ”ì§€
                self.logger.info(f"Stage 2 ëª¨ë¸ ì •í™•ë„: {best_acc:.1%}")
            else:
                results["training_completed"] = False
                self.logger.warning("Stage 2 ëª¨ë¸ íŒŒì¼ ì—†ìŒ")
        except Exception as e:
            self.logger.warning(f"í•™ìŠµ ì™„ë£Œ ì²´í¬ ì‹¤íŒ¨: {e}")
            results["training_completed"] = False
        
        # 2. ëª¨ë¸ ì €ì¥ í™•ì¸
        results["model_saved"] = results["training_completed"]
        
        # 3. ë©”ëª¨ë¦¬ ìµœì í™” ë™ì‘ í™•ì¸
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
                results["memory_optimization_working"] = memory_allocated <= self.targets.gpu_memory_limit_gb
                self.logger.info(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_allocated:.2f}GB")
            else:
                results["memory_optimization_working"] = True  # CPU ëª¨ë“œì—ì„œëŠ” í†µê³¼
        except:
            results["memory_optimization_working"] = False
        
        return results
    
    def _collect_performance_metrics(self, exp_dir: str) -> Dict[str, float]:
        """ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘ - ì‹¤ì œ ì €ì¥ëœ ëª¨ë¸ì—ì„œ"""
        
        metrics = {}
        
        try:
            # artifacts ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤ì œ ì €ì¥ëœ ëª¨ë¸ í™•ì¸
            artifacts_dir = Path("artifacts/models/classification")
            stage2_model = artifacts_dir / "best_classifier_250classes.pt"
            
            if stage2_model.exists():
                checkpoint = torch.load(stage2_model, map_location='cpu')
                
                # ì‹¤ì œ ì €ì¥ëœ ì„±ëŠ¥ ì§€í‘œë“¤
                metrics["classification_accuracy"] = checkpoint.get("best_accuracy", 0.0)
                metrics["classification_top3_accuracy"] = checkpoint.get("top3_accuracy", 0.0)
                metrics["classification_top5_accuracy"] = checkpoint.get("top5_accuracy", 0.0)
                metrics["classification_f1_macro"] = checkpoint.get("f1_macro", 0.0)
                
                # í•™ìŠµ ë©”íƒ€ë°ì´í„°
                metrics["total_epochs_completed"] = checkpoint.get("epoch", 0)
                metrics["training_time_minutes"] = checkpoint.get("training_time", 0.0)
                
                self.logger.info(f"ëª¨ë¸ì—ì„œ ì¶”ì¶œí•œ ì„±ëŠ¥: {metrics['classification_accuracy']:.1%}")
            else:
                self.logger.warning("Stage 2 ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                metrics["classification_accuracy"] = 0.0
                metrics["classification_top3_accuracy"] = 0.0
                metrics["classification_top5_accuracy"] = 0.0
                metrics["classification_f1_macro"] = 0.0
        except Exception as e:
            self.logger.error(f"ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            metrics["classification_accuracy"] = 0.0
            metrics["classification_top3_accuracy"] = 0.0
            metrics["classification_top5_accuracy"] = 0.0
            metrics["classification_f1_macro"] = 0.0
        
        # ì‹œìŠ¤í…œ ì„±ëŠ¥ ì§€í‘œ
        try:
            if torch.cuda.is_available():
                metrics["gpu_memory_used_gb"] = torch.cuda.memory_allocated() / 1024**3
                # GPU ì •ë³´
                gpu_name = torch.cuda.get_device_name(0)
                metrics["gpu_memory_total_gb"] = torch.cuda.get_device_properties(0).total_memory / 1024**3
                self.logger.info(f"GPU: {gpu_name}, ë©”ëª¨ë¦¬: {metrics['gpu_memory_used_gb']:.2f}GB")
            else:
                metrics["gpu_memory_used_gb"] = 0.0
                metrics["gpu_memory_total_gb"] = 0.0
        except:
            metrics["gpu_memory_used_gb"] = 0.0
            metrics["gpu_memory_total_gb"] = 0.0
        
        return metrics
    
    def _check_system_stability(self, exp_dir: str) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì•ˆì •ì„± í™•ì¸"""
        
        results = {
            "no_crashes": True,
            "memory_stable": True,
            "data_loading_stable": True
        }
        
        try:
            # í•™ìŠµ ë¡œê·¸ì—ì„œ ì˜¤ë¥˜ íŒ¨í„´ í™•ì¸
            error_log = Path(exp_dir) / "logs" / "train.err"
            if error_log.exists():
                with open(error_log) as f:
                    error_content = f.read()
                    
                    # OOM ë˜ëŠ” í¬ë˜ì‹œ íŒ¨í„´ í™•ì¸
                    if "OutOfMemoryError" in error_content or "CUDA out of memory" in error_content:
                        results["memory_stable"] = False
                    
                    if "Traceback" in error_content or "Exception" in error_content:
                        results["no_crashes"] = False
        except:
            pass
        
        return results
    
    def _calculate_performance_score(self, performance_metrics: Dict) -> float:
        """ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° (0~1)"""
        
        scores = []
        
        # ë¶„ë¥˜ ì •í™•ë„ ì ìˆ˜
        if performance_metrics.get("classification_accuracy", 0) > 0:
            accuracy_score = min(1.0, performance_metrics["classification_accuracy"] / self.targets.classification_accuracy)
            scores.append(accuracy_score)
        
        # ê²€ì¶œ ì„±ëŠ¥ ì ìˆ˜ (ë‚˜ì¤‘ì— êµ¬í˜„)
        if performance_metrics.get("detection_map_0_5", 0) > 0:
            detection_score = min(1.0, performance_metrics["detection_map_0_5"] / self.targets.detection_map_0_5)
            scores.append(detection_score)
        
        # ì²˜ë¦¬ëŸ‰ ì ìˆ˜
        if performance_metrics.get("estimated_throughput_img_s", 0) > 0:
            throughput_score = min(1.0, performance_metrics["estimated_throughput_img_s"] / self.targets.throughput_img_s)
            scores.append(throughput_score)
        
        return sum(scores) / len(scores) if scores else 0.0
    
    def _generate_recommendation(
        self, 
        mandatory_results: Dict, 
        performance_results: Dict, 
        system_results: Dict
    ) -> Dict[str, Any]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        # í•„ìˆ˜ ì²´í¬ í†µê³¼ ì—¬ë¶€
        mandatory_pass = all(mandatory_results.values())
        
        # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
        performance_score = self._calculate_performance_score(performance_results)
        
        # ì‹œìŠ¤í…œ ì•ˆì •ì„±
        system_stable = all(system_results.values())
        
        # ê¶Œì¥ì‚¬í•­ ê²°ì •
        if not mandatory_pass:
            recommendation = {
                "decision": "WARN_STOP",
                "color": "ğŸ”´",
                "message": "í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ ë¯¸ì¶©ì¡±",
                "performance_score": performance_score,
                "reason": "mandatory_checks_failed",
                "failed_checks": [k for k, v in mandatory_results.items() if not v]
            }
        elif performance_score >= 1.0 and system_stable:
            recommendation = {
                "decision": "RECOMMEND_PROCEED", 
                "color": "ğŸŸ¢",
                "message": "Stage 2 ëª¨ë“  ëª©í‘œ ë‹¬ì„±!",
                "performance_score": performance_score,
                "reason": "all_targets_met"
            }
        elif performance_score >= 0.7 and system_stable:
            recommendation = {
                "decision": "RECOMMEND_PROCEED",
                "color": "ğŸŸ¢", 
                "message": "Stage 2 ì¶©ë¶„í•œ ì„±ëŠ¥ ë‹¬ì„±",
                "performance_score": performance_score,
                "reason": "sufficient_performance"
            }
        elif performance_score >= 0.5:
            recommendation = {
                "decision": "SUGGEST_OPTIMIZE",
                "color": "ğŸŸ¡",
                "message": "Stage 2 ì„±ëŠ¥ ë¯¸ë‹¬. ìµœì í™” ê¶Œì¥",
                "performance_score": performance_score,
                "reason": "performance_below_target"
            }
        else:
            recommendation = {
                "decision": "WARN_STOP",
                "color": "ğŸ”´",
                "message": "Stage 2 ì„±ëŠ¥ì´ ë§¤ìš° ë‚®ìŒ", 
                "performance_score": performance_score,
                "reason": "performance_too_low"
            }
        
        # êµ¬ì²´ì  ì œì•ˆì‚¬í•­ ì¶”ê°€
        recommendation["suggestions"] = self._generate_optimization_suggestions(
            performance_results, mandatory_results
        )
        
        # ì‚¬ìš©ì ì„ íƒ ì˜µì…˜
        if recommendation["decision"] == "RECOMMEND_PROCEED":
            recommendation["user_options"] = [
                "[1] Stage 3ìœ¼ë¡œ ì§„í–‰",
                "[2] í˜„ì¬ Stageì—ì„œ ì¶”ê°€ ìµœì í™”",
                "[3] ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"
            ]
        elif recommendation["decision"] == "SUGGEST_OPTIMIZE":
            recommendation["user_options"] = [
                "[1] ê¶Œì¥ì‚¬í•­ ì ìš© í›„ ì¬ì‹œë„", 
                "[2] í˜„ì¬ ì„±ëŠ¥ìœ¼ë¡œ ì§„í–‰ (ìœ„í—˜)",
                "[3] ìƒì„¸ ë””ë²„ê¹… ëª¨ë“œ"
            ]
        else:
            recommendation["user_options"] = [
                "[1] ê°•ë ¥í•œ ìµœì í™” ì ìš© í›„ ì¬ì‹œë„",
                "[2] ì•„í‚¤í…ì²˜ ì¬ê²€í† ", 
                "[3] ë°ì´í„° í’ˆì§ˆ ì ê²€"
            ]
        
        return recommendation
    
    def _generate_optimization_suggestions(
        self, 
        performance_results: Dict, 
        mandatory_results: Dict
    ) -> List[str]:
        """ìµœì í™” ì œì•ˆì‚¬í•­ ìƒì„±"""
        
        suggestions = []
        
        # ì„±ëŠ¥ ê¸°ë°˜ ì œì•ˆ
        classification_acc = performance_results.get("classification_accuracy", 0)
        if classification_acc < self.targets.classification_accuracy * 0.8:
            suggestions.append("í•™ìŠµë¥  ì¡°ì • (2e-4 â†’ 1e-4)")
            suggestions.append("ë“œë¡­ì•„ì›ƒ ì¦ê°€ (0.3 â†’ 0.4)")
            suggestions.append("ë°ì´í„° ì¦ê°• ê°•í™”")
        
        # ì‹œìŠ¤í…œ ìµœì í™” ì œì•ˆ
        gpu_memory = performance_results.get("gpu_memory_used_gb", 0)
        if gpu_memory > 12:
            suggestions.append("ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ (í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ)")
        elif gpu_memory < 8:
            suggestions.append("ë°°ì¹˜ í¬ê¸° ì¦ê°€ (ë©”ëª¨ë¦¬ ì—¬ìœ  ìˆìŒ)")
        
        # í•„ìˆ˜ ì²´í¬ ì‹¤íŒ¨ì‹œ ì œì•ˆ
        if not mandatory_results.get("auto_batch_tuning_success", True):
            suggestions.append("Auto Batch íŠœë‹ í™œì„±í™” í™•ì¸")
        
        if not mandatory_results.get("tensorboard_logging", True):
            suggestions.append("TensorBoard ë¡œê¹… ì„¤ì • í™•ì¸")
        
        return suggestions[:5]  # ìƒìœ„ 5ê°œë§Œ
    
    def _save_evaluation_report(self, report: Dict, exp_dir: str) -> None:
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        
        try:
            reports_dir = Path(exp_dir) / "reports"
            reports_dir.mkdir(parents=True, exist_ok=True)
            
            report_path = reports_dir / "stage_2_evaluation.json"
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False, default=str)
            
            self.logger.info(f"í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
            
        except Exception as e:
            self.logger.error(f"í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _present_recommendation_to_user(self, recommendation: Dict) -> None:
        """ì‚¬ìš©ìì—ê²Œ ì‹œê°ì ìœ¼ë¡œ ê¶Œì¥ì‚¬í•­ í‘œì‹œ"""
        
        print("\n")
        print("â•" * 70)
        print("ğŸ¯ Stage 2 OptimizationAdvisor í‰ê°€ ê²°ê³¼")
        print("â•" * 70)
        print(f"{recommendation['color']} {recommendation['message']}")
        print()
        print(f"ğŸ“Š ì„±ëŠ¥ ì ìˆ˜: {recommendation['performance_score']:.3f}")
        print(f"ğŸ” íŒë‹¨ ê·¼ê±°: {recommendation.get('reason', 'unknown')}")
        print()
        
        if recommendation.get('failed_checks'):
            print("âŒ ì‹¤íŒ¨í•œ í•„ìˆ˜ ì²´í¬:")
            for check in recommendation['failed_checks']:
                print(f"   â€¢ {check}")
            print()
        
        if recommendation.get('suggestions'):
            print("ğŸ’¡ ì œì•ˆì‚¬í•­:")
            for i, suggestion in enumerate(recommendation['suggestions'], 1):
                print(f"   {i}. {suggestion}")
            print()
        
        print("ğŸ­ ì„ íƒ ì˜µì…˜:")
        for option in recommendation['user_options']:
            print(f"   {option}")
        
        print("â•" * 70)
        
        # ìë™ ì¢…ë£Œ (ëŒ€í™”í˜• ì…ë ¥ ì œê±°)
        if recommendation["decision"] == "RECOMMEND_PROCEED":
            print("\nâœ… ì¶”ì²œ: Stage 3 ì§„í–‰ ëª…ë ¹ì–´")
            print("source .venv/bin/activate && python -m src.training.train_classification_stage --stage 3 --epochs 30")
        else:
            print("\nâš ï¸  ìµœì í™” ì‘ì—… í›„ Stage 2 ì¬ì‹œë„ ê¶Œì¥")
        
        print()
        return  # ëŒ€í™”í˜• ì…ë ¥ ì œê±°


def main():
    """CLI ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Stage 2 Performance Evaluator")
    parser.add_argument("--exp-dir", type=str, 
                       default="/home/max16/pillsnap_data/exp/exp01",
                       help="Experiment directory path")
    parser.add_argument("--save-report", action="store_true",
                       help="Save detailed evaluation report")
    
    args = parser.parse_args()
    
    print("ğŸ¯ Stage 2 OptimizationAdvisor ì‹œì‘")
    print("=" * 60)
    
    advisor = OptimizationAdvisor()
    advisor.evaluate_stage_2(args.exp_dir)  # evaluation_result ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    
    if args.save_report:
        print(f"ğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸: {args.exp_dir}/reports/stage_2_evaluation.json")


if __name__ == "__main__":
    main()