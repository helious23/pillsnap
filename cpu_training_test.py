#!/usr/bin/env python3
"""
CPU ν•™μµ μ¤λ¨ν¬ ν…μ¤νΈ
"""
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
from pathlib import Path
import time
import json

# ν™κ²½ μ„¤μ •
torch.set_num_threads(2)
os.environ["OMP_NUM_THREADS"] = "2"
os.environ["MKL_NUM_THREADS"] = "2"


def main():
    print("π€ CPU Training Smoke Test")
    start_time = time.time()

    # μ¶λ ¥ λ””λ ‰ν† λ¦¬ μƒμ„±
    run_tag = f"cpu_train_{int(time.time())}"
    output_dir = Path(f"artifacts/cpu_runs/{run_tag}")
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"π“ Output: {output_dir}")

    # λ°μ΄ν„°μ…‹ λ΅λ“
    from pillsnap.stage2.dataset_cls import PillsnapClsDataset

    transform = transforms.Compose(
        [
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )

    dataset = PillsnapClsDataset(
        "artifacts/manifest_enriched.csv",
        "artifacts/classes_step11.json",
        transform=transform,
        require_exists=False,
    )

    print(f"π“ Dataset: {len(dataset)} samples")

    # κ°„λ‹¨ν• train/val λ¶„ν• 
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)
    )

    # λ°μ΄ν„°λ΅λ” (μ›μ»¤ 0, μ‘μ€ λ°°μΉ)
    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0)

    print(f"π“ Split: train={len(train_dataset)}, val={len(val_dataset)}")

    # λ¨λΈ μƒμ„±
    from pillsnap.stage2.models import create_efficientnetv2_l

    model = create_efficientnetv2_l(num_classes=19, pretrained=False)

    # μµν‹°λ§μ΄μ € λ° μ†μ‹¤ ν•¨μ
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)
    criterion = nn.CrossEntropyLoss()

    print("πƒ Starting training...")

    # μ§§μ€ ν•™μµ λ£¨ν”„ (1 epoch)
    model.train()
    total_loss = 0.0
    num_batches = 0

    for batch_idx, (images, targets) in enumerate(train_loader):
        if batch_idx >= 5:  # μµλ€ 5λ°°μΉλ§ ν•™μµ
            break

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        num_batches += 1

        print(f"  Batch {batch_idx+1}: loss={loss.item():.4f}")

    avg_loss = total_loss / max(1, num_batches)
    print(f"β… Training completed: avg_loss={avg_loss:.4f}")

    # κ°„λ‹¨ν• κ²€μ¦
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for batch_idx, (images, targets) in enumerate(val_loader):
            if batch_idx >= 3:  # μµλ€ 3λ°°μΉλ§ κ²€μ¦
                break

            outputs = model(images)
            loss = criterion(outputs, targets)
            val_loss += loss.item()

            predictions = outputs.argmax(dim=1)
            correct += (predictions == targets).sum().item()
            total += targets.size(0)

            print(f"  Val batch {batch_idx+1}: loss={loss.item():.4f}")

    val_acc = correct / max(1, total)
    print(f"β… Validation: loss={val_loss/3:.4f}, acc={val_acc:.3f}")

    # μ²΄ν¬ν¬μΈνΈ μ €μ¥
    checkpoint = {
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "train_loss": avg_loss,
        "val_loss": val_loss / 3,
        "val_accuracy": val_acc,
        "epoch": 1,
        "num_classes": 19,
    }

    torch.save(checkpoint, output_dir / "checkpoint.pt")

    # λ©”νΈλ¦­ μ €μ¥
    metrics = {
        "training_completed": True,
        "device": "cpu",
        "train_samples": len(train_dataset),
        "val_samples": len(val_dataset),
        "train_loss": avg_loss,
        "val_loss": val_loss / 3,
        "val_accuracy": val_acc,
        "elapsed_seconds": time.time() - start_time,
        "model_parameters": sum(p.numel() for p in model.parameters()),
        "run_tag": run_tag,
    }

    with open(output_dir / "metrics.json", "w") as f:
        json.dump(metrics, f, indent=2)

    elapsed = time.time() - start_time
    print(f"π CPU training test completed in {elapsed:.1f}s")
    print(f"π“ Results saved to: {output_dir}")

    return metrics


if __name__ == "__main__":
    result = main()
    print(f"β… SUCCESS: {result['run_tag']}")
