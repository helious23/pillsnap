"""
Batch Size Auto Tuner
ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì • ì‹œìŠ¤í…œ

RTX 5080 16GB GPU ìµœì í™”:
- OOM ë°©ì§€ ìë™ ë°°ì¹˜ í¬ê¸° ì¡°ì •
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ë™ì  íŠœë‹
- ì²˜ë¦¬ëŸ‰ ìµœì í™” (throughput optimization)
"""

import time
import torch
import gc
from typing import Dict, List, Optional, Tuple, Callable, Any
from dataclasses import dataclass

from src.training.memory_monitor_gpu_usage import GPUMemoryMonitor
from src.utils.core import PillSnapLogger


@dataclass
class BatchSizeTuningResult:
    """ë°°ì¹˜ í¬ê¸° íŠœë‹ ê²°ê³¼"""
    optimal_batch_size: int
    max_tested_batch_size: int
    memory_usage_at_optimal: float
    throughput_samples_per_sec: float
    tuning_time_seconds: float
    oom_threshold_batch_size: Optional[int] = None


class BatchSizeAutoTuner:
    """ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •ê¸°"""
    
    def __init__(
        self,
        initial_batch_size: int = 32,
        max_batch_size: int = 256,
        memory_threshold_gb: float = 14.0,
        safety_margin: float = 0.1  # 10% ì•ˆì „ ë§ˆì§„
    ):
        self.initial_batch_size = initial_batch_size
        self.max_batch_size = max_batch_size
        self.memory_threshold_gb = memory_threshold_gb
        self.safety_margin = safety_margin
        self.logger = PillSnapLogger(__name__)
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°
        self.memory_monitor = GPUMemoryMonitor(target_memory_gb=memory_threshold_gb)
        
        # íŠœë‹ íˆìŠ¤í† ë¦¬
        self.tuning_history = []
        
        self.logger.info(f"BatchSizeAutoTuner ì´ˆê¸°í™”")
        self.logger.info(f"  ì´ˆê¸° ë°°ì¹˜: {initial_batch_size}")
        self.logger.info(f"  ìµœëŒ€ ë°°ì¹˜: {max_batch_size}")
        self.logger.info(f"  ë©”ëª¨ë¦¬ ì„ê³„: {memory_threshold_gb:.1f}GB")
    
    def find_optimal_batch_size(
        self,
        model: torch.nn.Module,
        sample_batch_generator: Callable[[int], Tuple[torch.Tensor, ...]],
        device: torch.device = None
    ) -> BatchSizeTuningResult:
        """ìµœì  ë°°ì¹˜ í¬ê¸° íƒìƒ‰"""
        
        self.logger.step("ë°°ì¹˜ í¬ê¸° ìµœì í™”", "ì´ì§„ íƒìƒ‰ìœ¼ë¡œ ìµœì  ë°°ì¹˜ í¬ê¸° íƒìƒ‰")
        
        if device is None:
            device = next(model.parameters()).device
        
        start_time = time.time()
        
        try:
            # ì´ˆê¸° ë©”ëª¨ë¦¬ ì •ë¦¬
            self._clear_gpu_memory()
            
            # ì´ì§„ íƒìƒ‰ìœ¼ë¡œ ìµœëŒ€ ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
            max_working_batch = self._binary_search_max_batch(
                model, sample_batch_generator, device
            )
            
            # ì²˜ë¦¬ëŸ‰ ê¸°ë°˜ ìµœì í™”
            optimal_batch = self._optimize_for_throughput(
                model, sample_batch_generator, device, max_working_batch
            )
            
            # ìµœì  ë°°ì¹˜ì—ì„œ ìµœì¢… ë©”íŠ¸ë¦­ ì¸¡ì •
            final_metrics = self._measure_batch_performance(
                model, sample_batch_generator, device, optimal_batch
            )
            
            tuning_time = time.time() - start_time
            
            result = BatchSizeTuningResult(
                optimal_batch_size=optimal_batch,
                max_tested_batch_size=max_working_batch,
                memory_usage_at_optimal=final_metrics['memory_usage_gb'],
                throughput_samples_per_sec=final_metrics['throughput'],
                tuning_time_seconds=tuning_time,
                oom_threshold_batch_size=max_working_batch + 16 if max_working_batch < self.max_batch_size else None
            )
            
            self.logger.success(f"ë°°ì¹˜ í¬ê¸° ìµœì í™” ì™„ë£Œ")
            self.logger.info(f"  ìµœì  ë°°ì¹˜: {optimal_batch}")
            self.logger.info(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©: {final_metrics['memory_usage_gb']:.1f}GB")
            self.logger.info(f"  ì²˜ë¦¬ëŸ‰: {final_metrics['throughput']:.1f} samples/sec")
            
            return result
            
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ í¬ê¸° ìµœì í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _binary_search_max_batch(
        self,
        model: torch.nn.Module,
        sample_batch_generator: Callable[[int], Tuple[torch.Tensor, ...]],
        device: torch.device
    ) -> int:
        """ì´ì§„ íƒìƒ‰ìœ¼ë¡œ ìµœëŒ€ ë°°ì¹˜ í¬ê¸° ì°¾ê¸°"""
        
        low = 1
        high = self.max_batch_size
        max_working_batch = self.initial_batch_size
        
        self.logger.info(f"ì´ì§„ íƒìƒ‰ ì‹œì‘: [{low}, {high}]")
        
        while low <= high:
            mid = (low + high) // 2
            
            try:
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                self._clear_gpu_memory()
                
                # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                success, memory_used = self._test_batch_size(
                    model, sample_batch_generator, device, mid
                )
                
                if success and memory_used <= self.memory_threshold_gb * (1 - self.safety_margin):
                    max_working_batch = mid
                    low = mid + 1
                    self.logger.info(f"  ë°°ì¹˜ {mid}: âœ… ì„±ê³µ ({memory_used:.1f}GB)")
                else:
                    high = mid - 1
                    if not success:
                        self.logger.info(f"  ë°°ì¹˜ {mid}: âŒ OOM")
                    else:
                        self.logger.info(f"  ë°°ì¹˜ {mid}: âš ï¸ ë©”ëª¨ë¦¬ ì„ê³„ ì´ˆê³¼ ({memory_used:.1f}GB)")
                
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    high = mid - 1
                    self.logger.info(f"  ë°°ì¹˜ {mid}: âŒ OOM")
                else:
                    raise
        
        return max_working_batch
    
    def _test_batch_size(
        self,
        model: torch.nn.Module,
        sample_batch_generator: Callable[[int], Tuple[torch.Tensor, ...]],
        device: torch.device,
        batch_size: int
    ) -> Tuple[bool, float]:
        """íŠ¹ì • ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸"""
        
        try:
            model.train()
            
            # ìƒ˜í”Œ ë°°ì¹˜ ìƒì„±
            batch_data = sample_batch_generator(batch_size)
            batch_data = tuple(tensor.to(device) for tensor in batch_data)
            
            # Forward pass
            if len(batch_data) == 2:
                inputs, targets = batch_data
                outputs = model(inputs)
                
                # ê°„ë‹¨í•œ ì†ì‹¤ ê³„ì‚° (ì‹¤ì œ ì†ì‹¤ í•¨ìˆ˜ëŠ” ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¦„)
                if hasattr(torch.nn.functional, 'cross_entropy') and targets.dtype == torch.long:
                    loss = torch.nn.functional.cross_entropy(outputs, targets)
                else:
                    loss = outputs.mean()  # ë”ë¯¸ ì†ì‹¤
            else:
                inputs = batch_data[0]
                outputs = model(inputs)
                loss = outputs.mean()  # ë”ë¯¸ ì†ì‹¤
            
            # Backward pass
            loss.backward()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            memory_stats = self.memory_monitor.get_current_usage()
            memory_used = memory_stats['used_gb']
            
            return True, memory_used
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                return False, float('inf')
            else:
                raise
        finally:
            # ê·¸ë˜ë””ì–¸íŠ¸ ì •ë¦¬
            model.zero_grad()
    
    def _optimize_for_throughput(
        self,
        model: torch.nn.Module,
        sample_batch_generator: Callable[[int], Tuple[torch.Tensor, ...]],
        device: torch.device,
        max_batch: int
    ) -> int:
        """ì²˜ë¦¬ëŸ‰ ê¸°ë°˜ ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°"""
        
        self.logger.info(f"ì²˜ë¦¬ëŸ‰ ìµœì í™”: ìµœëŒ€ ë°°ì¹˜ {max_batch}ì—ì„œ ì‹œì‘")
        
        # í…ŒìŠ¤íŠ¸í•  ë°°ì¹˜ í¬ê¸°ë“¤ (ìµœëŒ€ ë°°ì¹˜ì—ì„œ ì‹œì‘í•´ì„œ ê°ì†Œ)
        test_batches = []
        current = max_batch
        while current >= 8:  # ìµœì†Œ 8ê¹Œì§€ í…ŒìŠ¤íŠ¸
            test_batches.append(current)
            current = max(current // 2, current - 16)
        
        if test_batches[-1] != 8:
            test_batches.append(8)
        
        best_batch = max_batch
        best_throughput = 0.0
        
        for batch_size in test_batches:
            try:
                self._clear_gpu_memory()
                
                # ì²˜ë¦¬ëŸ‰ ì¸¡ì •
                metrics = self._measure_batch_performance(
                    model, sample_batch_generator, device, batch_size
                )
                
                throughput = metrics['throughput']
                memory_used = metrics['memory_usage_gb']
                
                self.logger.info(f"  ë°°ì¹˜ {batch_size}: {throughput:.1f} samples/sec, {memory_used:.1f}GB")
                
                # ì²˜ë¦¬ëŸ‰ì´ ë” ì¢‹ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ì ˆí•œ ê²½ìš° ì—…ë°ì´íŠ¸
                if throughput > best_throughput and memory_used <= self.memory_threshold_gb:
                    best_throughput = throughput
                    best_batch = batch_size
                
            except Exception as e:
                self.logger.warning(f"ë°°ì¹˜ {batch_size} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                continue
        
        return best_batch
    
    def _measure_batch_performance(
        self,
        model: torch.nn.Module,
        sample_batch_generator: Callable[[int], Tuple[torch.Tensor, ...]],
        device: torch.device,
        batch_size: int,
        num_iterations: int = 5
    ) -> Dict[str, float]:
        """ë°°ì¹˜ ì„±ëŠ¥ ì¸¡ì •"""
        
        model.train()
        
        # ì›Œë°ì—…
        batch_data = sample_batch_generator(batch_size)
        batch_data = tuple(tensor.to(device) for tensor in batch_data)
        _ = model(batch_data[0])
        
        # ì‹¤ì œ ì¸¡ì •
        torch.cuda.synchronize()
        start_time = time.time()
        
        for _ in range(num_iterations):
            batch_data = sample_batch_generator(batch_size)
            batch_data = tuple(tensor.to(device) for tensor in batch_data)
            
            outputs = model(batch_data[0])
            
            # ê°„ë‹¨í•œ ë°±ì›Œë“œ íŒ¨ìŠ¤
            loss = outputs.mean()
            loss.backward()
            model.zero_grad()
        
        torch.cuda.synchronize()
        end_time = time.time()
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        total_time = end_time - start_time
        total_samples = batch_size * num_iterations
        throughput = total_samples / total_time
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory_stats = self.memory_monitor.get_current_usage()
        
        return {
            'throughput': throughput,
            'memory_usage_gb': memory_stats['used_gb'],
            'avg_time_per_batch': total_time / num_iterations
        }
    
    def _clear_gpu_memory(self) -> None:
        """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
        time.sleep(0.1)  # ì •ë¦¬ ì™„ë£Œ ëŒ€ê¸°
    
    def suggest_batch_size_for_inference(
        self,
        model: torch.nn.Module,
        sample_input_generator: Callable[[int], torch.Tensor],
        target_latency_ms: float = 50.0
    ) -> int:
        """ì¶”ë¡ ìš© ë°°ì¹˜ í¬ê¸° ì œì•ˆ"""
        
        self.logger.step("ì¶”ë¡  ë°°ì¹˜ ìµœì í™”", f"ëª©í‘œ ì§€ì—°ì‹œê°„ {target_latency_ms}ms")
        
        model.eval()
        device = next(model.parameters()).device
        
        # ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸
        test_batches = [1, 2, 4, 8, 16, 32, 64]
        suitable_batches = []
        
        with torch.no_grad():
            for batch_size in test_batches:
                try:
                    self._clear_gpu_memory()
                    
                    # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
                    sample_input = sample_input_generator(batch_size).to(device)
                    
                    # ì›Œë°ì—…
                    _ = model(sample_input)
                    
                    # ì‹¤ì œ ì¸¡ì •
                    torch.cuda.synchronize()
                    start_time = time.time()
                    
                    for _ in range(10):
                        _ = model(sample_input)
                    
                    torch.cuda.synchronize()
                    end_time = time.time()
                    
                    avg_time_ms = (end_time - start_time) * 1000 / 10
                    per_sample_ms = avg_time_ms / batch_size
                    
                    self.logger.info(f"  ë°°ì¹˜ {batch_size}: {avg_time_ms:.1f}ms total, {per_sample_ms:.1f}ms/sample")
                    
                    if per_sample_ms <= target_latency_ms:
                        suitable_batches.append((batch_size, per_sample_ms))
                
                except Exception as e:
                    self.logger.warning(f"ì¶”ë¡  ë°°ì¹˜ {batch_size} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        if suitable_batches:
            # ëª©í‘œ ì§€ì—°ì‹œê°„ì„ ë§Œì¡±í•˜ëŠ” ê°€ì¥ í° ë°°ì¹˜ í¬ê¸° ì„ íƒ
            optimal_batch = max(suitable_batches, key=lambda x: x[0])[0]
            self.logger.success(f"ì¶”ë¡  ìµœì  ë°°ì¹˜: {optimal_batch}")
            return optimal_batch
        else:
            self.logger.warning("ëª©í‘œ ì§€ì—°ì‹œê°„ì„ ë§Œì¡±í•˜ëŠ” ë°°ì¹˜ í¬ê¸° ì—†ìŒ, ë°°ì¹˜ 1 ì‚¬ìš©")
            return 1


def create_sample_batch_generator(input_shape: Tuple[int, ...], num_classes: int):
    """ìƒ˜í”Œ ë°°ì¹˜ ìƒì„±ê¸° íŒ©í† ë¦¬"""
    
    def generator(batch_size: int) -> Tuple[torch.Tensor, torch.Tensor]:
        # ë”ë¯¸ ì…ë ¥ ë° ë¼ë²¨ ìƒì„±
        inputs = torch.randn(batch_size, *input_shape)
        labels = torch.randint(0, num_classes, (batch_size,))
        return inputs, labels
    
    return generator


def main():
    """ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì • í…ŒìŠ¤íŠ¸"""
    print("ğŸ”§ Batch Size Auto Tuner Test")
    print("=" * 50)
    
    try:
        # ë”ë¯¸ ëª¨ë¸ ìƒì„±
        model = torch.nn.Sequential(
            torch.nn.Linear(784, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, 10)
        )
        
        if torch.cuda.is_available():
            model = model.cuda()
            device = torch.device('cuda')
        else:
            device = torch.device('cpu')
        
        # ë°°ì¹˜ ìƒì„±ê¸°
        batch_generator = create_sample_batch_generator((784,), 10)
        
        # ìë™ íŠœë„ˆ ìƒì„±
        tuner = BatchSizeAutoTuner(
            initial_batch_size=32,
            max_batch_size=128,
            memory_threshold_gb=14.0
        )
        
        # ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
        result = tuner.find_optimal_batch_size(model, batch_generator, device)
        
        print(f"âœ… ë°°ì¹˜ í¬ê¸° ìµœì í™” ì™„ë£Œ")
        print(f"   ìµœì  ë°°ì¹˜: {result.optimal_batch_size}")
        print(f"   ìµœëŒ€ í…ŒìŠ¤íŠ¸ ë°°ì¹˜: {result.max_tested_batch_size}")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©: {result.memory_usage_at_optimal:.1f}GB")
        print(f"   ì²˜ë¦¬ëŸ‰: {result.throughput_samples_per_sec:.1f} samples/sec")
        print(f"   íŠœë‹ ì‹œê°„: {result.tuning_time_seconds:.1f}ì´ˆ")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()