#!/usr/bin/env python3
"""
TensorBoard Integration Patch for Stage 3 Training
ê¸°ì¡´ í•™ìŠµ ì½”ë“œì— TensorBoardë¥¼ ì¶”ê°€í•˜ëŠ” í—¬í¼ ëª¨ë“ˆ
"""

import os
import sys
import torch
from pathlib import Path
from typing import Dict, Any, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.utils.tensorboard_logger import TensorBoardLogger


class TensorBoardIntegration:
    """ê¸°ì¡´ í•™ìŠµ ì½”ë“œì— TensorBoard í†µí•©ì„ ìœ„í•œ ë˜í¼"""
    
    def __init__(self, trainer_instance):
        """
        Args:
            trainer_instance: TwoStageTrainer ì¸ìŠ¤í„´ìŠ¤
        """
        self.trainer = trainer_instance
        
        # TensorBoard ë¡œê±° ì´ˆê¸°í™”
        exp_dir = self.trainer.exp_dir if hasattr(self.trainer, 'exp_dir') else '/home/max16/pillsnap_data/exp/exp01'
        tb_dir = os.path.join(exp_dir, 'tensorboard')
        
        self.tb_logger = TensorBoardLogger(
            log_dir=tb_dir,
            experiment_name=f"stage3_resume",
            comment="two_stage",
            flush_secs=30
        )
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…
        self._log_hyperparameters()
        
        print(f"âœ… TensorBoard í†µí•© ì™„ë£Œ")
        print(f"   ğŸ“Š ì‹¤í–‰: tensorboard --logdir {tb_dir}")
        print(f"   ğŸŒ ë¸Œë¼ìš°ì €: http://localhost:6006")
    
    def _log_hyperparameters(self):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…"""
        hparams = {}
        
        # Training config
        if hasattr(self.trainer, 'training_config'):
            cfg = self.trainer.training_config
            hparams['epochs'] = cfg.epochs
            hparams['batch_size'] = cfg.batch_size
            hparams['lr_classifier'] = cfg.lr_classifier
            hparams['lr_detector'] = cfg.lr_detector
            hparams['mixed_precision'] = cfg.mixed_precision
            hparams['num_classes'] = cfg.num_classes
        
        # í…ìŠ¤íŠ¸ë¡œ ë¡œê¹…
        hparam_text = "\n".join([f"- {k}: {v}" for k, v in hparams.items()])
        self.tb_logger.log_text("hyperparameters", hparam_text, 0)
    
    def log_classification_batch(
        self,
        epoch: int,
        batch_idx: int,
        total_batches: int,
        loss: float,
        accuracy: Optional[float] = None,
        learning_rate: Optional[float] = None
    ):
        """Classification ë°°ì¹˜ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        step = epoch * total_batches + batch_idx
        
        self.tb_logger.log_scalar('train/classification/batch_loss', loss, step)
        
        if accuracy is not None:
            self.tb_logger.log_scalar('train/classification/batch_accuracy', accuracy, step)
        
        if learning_rate is not None:
            self.tb_logger.log_scalar('train/learning_rate', learning_rate, step)
    
    def log_classification_epoch(
        self,
        epoch: int,
        train_loss: float,
        train_accuracy: float,
        val_loss: Optional[float] = None,
        val_accuracy: Optional[float] = None,
        val_top5_accuracy: Optional[float] = None
    ):
        """Classification ì—í¬í¬ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        # Train metrics
        self.tb_logger.log_classification_metrics(
            loss=train_loss,
            accuracy=train_accuracy,
            step=epoch,
            phase="train"
        )
        
        # Validation metrics
        if val_loss is not None and val_accuracy is not None:
            self.tb_logger.log_classification_metrics(
                loss=val_loss,
                accuracy=val_accuracy,
                top5_accuracy=val_top5_accuracy,
                step=epoch,
                phase="val"
            )
    
    def log_detection_epoch(
        self,
        epoch: int,
        box_loss: float,
        cls_loss: float,
        dfl_loss: float,
        map50: float,
        map50_95: Optional[float] = None
    ):
        """Detection ì—í¬í¬ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        self.tb_logger.log_detection_metrics(
            box_loss=box_loss,
            cls_loss=cls_loss,
            dfl_loss=dfl_loss,
            map50=map50,
            map50_95=map50_95,
            step=epoch,
            phase="train"
        )
    
    def log_system_metrics(self, epoch: int):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        if torch.cuda.is_available():
            gpu_memory_used = torch.cuda.memory_allocated() / (1024**3)
            gpu_memory_peak = torch.cuda.max_memory_allocated() / (1024**3)
            
            self.tb_logger.log_system_metrics(
                gpu_memory_used=gpu_memory_used,
                gpu_memory_peak=gpu_memory_peak,
                step=epoch
            )
    
    def close(self):
        """TensorBoard ë¡œê±° ì¢…ë£Œ"""
        self.tb_logger.close()


def patch_trainer_with_tensorboard(trainer_class):
    """ê¸°ì¡´ Trainer í´ë˜ìŠ¤ì— TensorBoard ë©”ì†Œë“œ íŒ¨ì¹˜"""
    
    # ì¤‘ë³µ íŒ¨ì¹˜ ë°©ì§€ (idempotent guard)
    if hasattr(trainer_class, '_tb_patched'):
        print("âš ï¸ TensorBoard patch already applied, skipping...")
        return trainer_class
    
    original_init = trainer_class.__init__
    original_train = trainer_class.train if hasattr(trainer_class, 'train') else None
    
    def new_init(self, *args, **kwargs):
        original_init(self, *args, **kwargs)
        # TensorBoard í†µí•© ì¶”ê°€
        self.tb_integration = TensorBoardIntegration(self)
    
    def new_train(self, *args, **kwargs):
        try:
            # original_trainì´ ì—†ìœ¼ë©´ ê¸°ë³¸ train ë©”ì†Œë“œ í˜¸ì¶œ
            if original_train:
                result = original_train(self, *args, **kwargs)
            else:
                # train ë©”ì†Œë“œê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
                result = None
            return result
        finally:
            # í•™ìŠµ ì¢…ë£Œ ì‹œ TensorBoard ë‹«ê¸°
            if hasattr(self, 'tb_integration'):
                self.tb_integration.close()
    
    # ë©”ì†Œë“œ íŒ¨ì¹˜
    trainer_class.__init__ = new_init
    trainer_class.train = new_train
    
    # TensorBoard ë¡œê¹… ë©”ì†Œë“œ ì¶”ê°€
    def log_tb_classification_batch(self, epoch, batch_idx, total_batches, loss, accuracy=None, lr=None):
        if hasattr(self, 'tb_integration'):
            self.tb_integration.log_classification_batch(
                epoch, batch_idx, total_batches, loss, accuracy, lr
            )
    
    def log_tb_classification_epoch(self, epoch, train_loss, train_acc, val_loss=None, val_acc=None, val_top5=None):
        if hasattr(self, 'tb_integration'):
            self.tb_integration.log_classification_epoch(
                epoch, train_loss, train_acc, val_loss, val_acc, val_top5
            )
    
    def log_tb_detection_epoch(self, epoch, box_loss, cls_loss, dfl_loss, map50, map50_95=None):
        if hasattr(self, 'tb_integration'):
            self.tb_integration.log_detection_epoch(
                epoch, box_loss, cls_loss, dfl_loss, map50, map50_95
            )
    
    def log_tb_system_metrics(self, epoch):
        if hasattr(self, 'tb_integration'):
            self.tb_integration.log_system_metrics(epoch)
    
    trainer_class.log_tb_classification_batch = log_tb_classification_batch
    trainer_class.log_tb_classification_epoch = log_tb_classification_epoch
    trainer_class.log_tb_detection_epoch = log_tb_detection_epoch
    trainer_class.log_tb_system_metrics = log_tb_system_metrics
    
    # íŒ¨ì¹˜ ì™„ë£Œ ë§ˆì»¤
    trainer_class._tb_patched = True
    
    return trainer_class


def log_tb_smoke(experiment="stage3", step=0):
    """TensorBoard ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ - ë”ë¯¸ ê°’ ë¡œê¹…"""
    from src.utils.tensorboard_logger import TensorBoardLogger
    import random
    
    tb_logger = TensorBoardLogger(
        log_dir="artifacts/tensorboard",
        experiment_name=experiment,
        comment="smoke_test"
    )
    
    # ë”ë¯¸ ìŠ¤ì¹¼ë¼ ë¡œê¹…
    tb_logger.log_scalar('smoke/test_metric_1', random.random(), step)
    tb_logger.log_scalar('smoke/test_metric_2', random.random() * 100, step)
    tb_logger.log_scalar('smoke/test_metric_3', random.random() * 0.01, step)
    
    # ë¶„ë¥˜ ë©”íŠ¸ë¦­ ì˜ˆì œ
    tb_logger.log_scalar('train/loss', 2.3 - step * 0.1, step)
    tb_logger.log_scalar('train/lr', 1e-4 * (0.95 ** step), step)
    tb_logger.log_scalar('val/top1', 0.3 + step * 0.02, step)
    tb_logger.log_scalar('val/top5', 0.5 + step * 0.03, step)
    
    # ê²€ì¶œ ë©”íŠ¸ë¦­ ì˜ˆì œ
    tb_logger.log_scalar('det/map50', 0.25 + step * 0.01, step)
    tb_logger.log_scalar('det/box_loss', 0.5 - step * 0.01, step)
    
    # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì˜ˆì œ
    tb_logger.log_scalar('sys/vram_used', 8000 + random.randint(-500, 500), step)
    tb_logger.log_scalar('latency/total', 50 + random.randint(-10, 10), step)
    
    print(f"âœ… Smoke test logged at step {step}")
    print(f"ğŸ“Š View at: tensorboard --logdir artifacts/tensorboard")
    
    tb_logger.close()
    return True


# ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--smoke", action="store_true", help="Run smoke test")
    args = parser.parse_args()
    
    if args.smoke:
        print("ğŸ”¥ Running TensorBoard smoke test...")
        for step in range(5):
            log_tb_smoke("stage3_smoke", step)
        print("âœ… Smoke test complete!")
    else:
        print("TensorBoard Integration Module")
        print("ì´ ëª¨ë“ˆì„ importí•˜ê³  patch_trainer_with_tensorboard()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        print("\nì˜ˆì œ:")
        print("from src.training.tensorboard_integration import patch_trainer_with_tensorboard")
        print("patch_trainer_with_tensorboard(TwoStageTrainer)")
        print("\nìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸:")
        print("python -m src.training.tensorboard_integration --smoke")