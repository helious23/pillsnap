"""
Classification Stage Training Module
ë¶„ë¥˜ ëª¨ë¸ ì „ìš© í•™ìŠµ ëª¨ë“ˆ

EfficientNetV2-S ë¶„ë¥˜ê¸° Stageë³„ í•™ìŠµ:
- Progressive Validation ì§€ì› (Stage 1~4)
- RTX 5080 ìµœì í™” (Mixed Precision, torch.compile)
- ëª©í‘œ ì •í™•ë„ ë‹¬ì„± ìë™ ì²´í¬
"""

import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.amp import GradScaler, autocast
from typing import Dict, Optional, Tuple, Any
from pathlib import Path

from src.models.classifier_efficientnetv2 import PillSnapClassifier, create_pillsnap_classifier
from src.training.memory_monitor_gpu_usage import GPUMemoryMonitor
from src.evaluation.evaluate_classification_metrics import ClassificationMetricsEvaluator
from src.utils.core import PillSnapLogger


class ClassificationStageTrainer:
    """ë¶„ë¥˜ ëª¨ë¸ ì „ìš© í•™ìŠµê¸°"""
    
    def __init__(
        self, 
        num_classes: int,
        target_accuracy: float = 0.40,
        device: str = "cuda"
    ):
        self.num_classes = num_classes
        self.target_accuracy = target_accuracy
        self.device = torch.device(device)
        self.logger = PillSnapLogger(__name__)
        
        # ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
        self.memory_monitor = GPUMemoryMonitor()
        self.metrics_evaluator = ClassificationMetricsEvaluator(num_classes)
        
        # í•™ìŠµ ìƒíƒœ
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.scaler = None
        self.best_accuracy = 0.0
        self.training_history = []
        
        self.logger.info(f"ClassificationStageTrainer ì´ˆê¸°í™”")
        self.logger.info(f"í´ë˜ìŠ¤ ìˆ˜: {num_classes}, ëª©í‘œ ì •í™•ë„: {target_accuracy:.1%}")
    
    def setup_model_and_optimizers(
        self, 
        learning_rate: float = 2e-4,
        weight_decay: float = 1e-4,
        mixed_precision: bool = True
    ) -> None:
        """ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •"""
        
        try:
            # ë¶„ë¥˜ê¸° ìƒì„±
            self.model = create_pillsnap_classifier(
                num_classes=self.num_classes,
                device=str(self.device)
            )
            
            # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
            self.optimizer = optim.AdamW(
                self.model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay
            )
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, T_max=50  # ìµœëŒ€ 50 ì—í¬í¬ ê°€ì •
            )
            
            # Mixed Precision ì„¤ì •
            if mixed_precision and torch.cuda.is_available():
                self.scaler = GradScaler()
                self.logger.info("Mixed Precision í™œì„±í™”")
            
            self.logger.success("ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    def train_epoch(
        self, 
        train_loader: DataLoader,
        epoch: int
    ) -> Dict[str, float]:
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        
        if self.model is None:
            raise RuntimeError("ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ. setup_model_and_optimizers() ë¨¼ì € í˜¸ì¶œ")
        
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        criterion = nn.CrossEntropyLoss()
        
        for batch_idx, (images, labels) in enumerate(train_loader):
            images, labels = images.to(self.device), labels.to(self.device)
            
            self.optimizer.zero_grad()
            
            # Mixed Precision í•™ìŠµ
            if self.scaler is not None:
                with autocast():
                    outputs = self.model(images)
                    loss = criterion(outputs, labels)
                
                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                outputs = self.model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                self.optimizer.step()
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # ì§„í–‰ ìƒí™© ë¡œê¹…
            if batch_idx % 100 == 0:
                self.logger.info(f"Epoch {epoch} Batch {batch_idx}: Loss {loss.item():.4f}")
        
        epoch_loss = total_loss / len(train_loader)
        epoch_accuracy = correct / total
        
        return {
            'loss': epoch_loss,
            'accuracy': epoch_accuracy,
            'correct': correct,
            'total': total
        }
    
    def validate_epoch(
        self, 
        val_loader: DataLoader
    ) -> Dict[str, float]:
        """ê²€ì¦ ì—í¬í¬"""
        
        if self.model is None:
            raise RuntimeError("ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        criterion = nn.CrossEntropyLoss()
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(self.device), labels.to(self.device)
                
                outputs = self.model(images)
                loss = criterion(outputs, labels)
                
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
                
                all_predictions.extend(predicted.cpu())
                all_labels.extend(labels.cpu())
        
        epoch_loss = total_loss / len(val_loader)
        epoch_accuracy = correct / total
        
        # ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚°
        try:
            y_true = torch.tensor(all_labels)
            y_pred_logits = torch.zeros(len(all_predictions), self.num_classes)
            # ê°„ë‹¨í•œ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ë¡œì§“ ì‹œë®¬ë ˆì´ì…˜
            for i, pred in enumerate(all_predictions):
                y_pred_logits[i, pred] = 1.0
            
            detailed_metrics = self.metrics_evaluator.evaluate_predictions(y_true, y_pred_logits)
            
        except Exception as e:
            self.logger.warning(f"ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            detailed_metrics = None
        
        return {
            'loss': epoch_loss,
            'accuracy': epoch_accuracy,
            'correct': correct,
            'total': total,
            'detailed_metrics': detailed_metrics
        }
    
    def train_stage(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        max_epochs: int = 10,
        early_stopping_patience: int = 5
    ) -> Dict[str, Any]:
        """ì „ì²´ Stage í•™ìŠµ"""
        
        self.logger.step("ë¶„ë¥˜ Stage í•™ìŠµ", f"{max_epochs} ì—í¬í¬ ëª©í‘œ ì •í™•ë„ {self.target_accuracy:.1%}")
        
        start_time = time.time()
        patience_counter = 0
        
        for epoch in range(1, max_epochs + 1):
            # GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
            memory_stats = self.memory_monitor.get_current_usage()
            self.logger.info(f"Epoch {epoch}/{max_epochs} - GPU: {memory_stats['used_gb']:.1f}GB")
            
            # í•™ìŠµ
            train_results = self.train_epoch(train_loader, epoch)
            
            # ê²€ì¦
            val_results = self.validate_epoch(val_loader)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            if self.scheduler:
                self.scheduler.step()
            
            # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
            if val_results['accuracy'] > self.best_accuracy:
                self.best_accuracy = val_results['accuracy']
                patience_counter = 0
                self.logger.metric("best_accuracy", self.best_accuracy, "%")
                
                # ëª¨ë¸ ì €ì¥
                self._save_best_model()
            else:
                patience_counter += 1
            
            # í•™ìŠµ íˆìŠ¤í† ë¦¬ ê¸°ë¡
            epoch_history = {
                'epoch': epoch,
                'train_loss': train_results['loss'],
                'train_accuracy': train_results['accuracy'],
                'val_loss': val_results['loss'],
                'val_accuracy': val_results['accuracy'],
                'learning_rate': self.optimizer.param_groups[0]['lr'] if self.optimizer else 0
            }
            self.training_history.append(epoch_history)
            
            self.logger.info(f"Epoch {epoch} - Train: {train_results['accuracy']:.1%}, "
                           f"Val: {val_results['accuracy']:.1%}")
            
            # ëª©í‘œ ë‹¬ì„± ì²´í¬
            if val_results['accuracy'] >= self.target_accuracy:
                self.logger.success(f"ğŸ‰ ëª©í‘œ ì •í™•ë„ ë‹¬ì„±! {val_results['accuracy']:.1%} >= {self.target_accuracy:.1%}")
                break
            
            # Early Stopping
            if patience_counter >= early_stopping_patience:
                self.logger.warning(f"Early stopping at epoch {epoch}")
                break
        
        total_time = time.time() - start_time
        
        # ìµœì¢… ê²°ê³¼
        final_results = {
            'best_accuracy': self.best_accuracy,
            'target_achieved': self.best_accuracy >= self.target_accuracy,
            'epochs_completed': len(self.training_history),
            'total_time_minutes': total_time / 60,
            'training_history': self.training_history,
            'final_val_results': val_results
        }
        
        self.logger.success(f"ë¶„ë¥˜ í•™ìŠµ ì™„ë£Œ - ìµœê³  ì •í™•ë„: {self.best_accuracy:.1%}")
        return final_results
    
    def _save_best_model(self) -> None:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥"""
        try:
            save_dir = Path("artifacts/models/classification")
            save_dir.mkdir(parents=True, exist_ok=True)
            
            model_path = save_dir / f"best_classifier_{self.num_classes}classes.pt"
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict() if self.optimizer else None,
                'best_accuracy': self.best_accuracy,
                'num_classes': self.num_classes
            }, model_path)
            
            self.logger.info(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {model_path}")
            
        except Exception as e:
            self.logger.warning(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë¶„ë¥˜ Stage í•™ìŠµ í…ŒìŠ¤íŠ¸"""
    print("ğŸ”§ Classification Stage Trainer Test")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    trainer = ClassificationStageTrainer(num_classes=50, target_accuracy=0.40)
    trainer.setup_model_and_optimizers()
    
    # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ (ì‹¤ì œë¡œëŠ” DataLoader ì „ë‹¬)
    print("âœ… Classification Stage Trainer ì´ˆê¸°í™” ì™„ë£Œ")
    print("ì‹¤ì œ í•™ìŠµì„ ìœ„í•´ì„œëŠ” DataLoaderê°€ í•„ìš”í•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()