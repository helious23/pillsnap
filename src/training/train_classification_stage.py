"""
Classification Stage Training Module
ë¶„ë¥˜ ëª¨ë¸ ì „ìš© í•™ìŠµ ëª¨ë“ˆ

EfficientNetV2-S ë¶„ë¥˜ê¸° Stageë³„ í•™ìŠµ:
- Progressive Validation ì§€ì› (Stage 1~4)
- RTX 5080 ìµœì í™” (Mixed Precision, torch.compile)
- ëª©í‘œ ì •í™•ë„ ë‹¬ì„± ìë™ ì²´í¬
"""

import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.amp import GradScaler, autocast
from typing import Dict, Optional, Tuple, Any
from pathlib import Path

from src.models.classifier_efficientnetv2 import PillSnapClassifier, create_pillsnap_classifier
from src.training.memory_monitor_gpu_usage import GPUMemoryMonitor
from src.evaluation.evaluate_classification_metrics import ClassificationMetricsEvaluator
from src.utils.core import PillSnapLogger


class ClassificationStageTrainer:
    """ë¶„ë¥˜ ëª¨ë¸ ì „ìš© í•™ìŠµê¸°"""
    
    def __init__(
        self, 
        num_classes: int,
        target_accuracy: float = 0.40,
        device: str = "cuda"
    ):
        self.num_classes = num_classes
        self.target_accuracy = target_accuracy
        self.device = torch.device(device)
        self.logger = PillSnapLogger(__name__)
        
        # ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
        self.memory_monitor = GPUMemoryMonitor()
        self.metrics_evaluator = ClassificationMetricsEvaluator(num_classes)
        
        # í•™ìŠµ ìƒíƒœ
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.scaler = None
        self.best_accuracy = 0.0
        self.training_history = []
        
        self.logger.info(f"ClassificationStageTrainer ì´ˆê¸°í™”")
        self.logger.info(f"í´ë˜ìŠ¤ ìˆ˜: {num_classes}, ëª©í‘œ ì •í™•ë„: {target_accuracy:.1%}")
    
    def setup_model_and_optimizers(
        self, 
        learning_rate: float = 2e-4,
        weight_decay: float = 1e-4,
        mixed_precision: bool = True
    ) -> None:
        """ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •"""
        
        try:
            # ë¶„ë¥˜ê¸° ìƒì„±
            self.model = create_pillsnap_classifier(
                num_classes=self.num_classes,
                device=str(self.device)
            )
            
            # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
            self.optimizer = optim.AdamW(
                self.model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay
            )
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, T_max=50  # ìµœëŒ€ 50 ì—í¬í¬ ê°€ì •
            )
            
            # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
            self.criterion = nn.CrossEntropyLoss()
            
            # Mixed Precision ì„¤ì •
            if mixed_precision and torch.cuda.is_available():
                self.scaler = GradScaler()
                self.logger.info("Mixed Precision í™œì„±í™”")
            
            self.logger.success("ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    def train_epoch(
        self, 
        train_loader: DataLoader,
        epoch: int
    ) -> Dict[str, float]:
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        
        if self.model is None:
            raise RuntimeError("ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ. setup_model_and_optimizers() ë¨¼ì € í˜¸ì¶œ")
        
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        criterion = nn.CrossEntropyLoss()
        
        for batch_idx, (images, labels) in enumerate(train_loader):
            images, labels = images.to(self.device), labels.to(self.device)
            
            self.optimizer.zero_grad()
            
            # Mixed Precision í•™ìŠµ
            if self.scaler is not None:
                with autocast(device_type='cuda'):
                    outputs = self.model(images)
                    loss = criterion(outputs, labels)
                
                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                outputs = self.model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                self.optimizer.step()
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # ì§„í–‰ ìƒí™© ë¡œê¹… (ë” ìì£¼ ì¶œë ¥)
            if batch_idx % 5 == 0 or batch_idx == len(train_loader) - 1:
                current_acc = correct / total if total > 0 else 0
                print(f"  Batch {batch_idx+1}/{len(train_loader)}: Loss={loss.item():.4f}, Acc={current_acc:.2%}")
            elif batch_idx % 100 == 0:
                self.logger.info(f"Epoch {epoch} Batch {batch_idx}: Loss {loss.item():.4f}")
        
        epoch_loss = total_loss / len(train_loader)
        epoch_accuracy = correct / total
        
        return {
            'loss': epoch_loss,
            'accuracy': epoch_accuracy,
            'correct': correct,
            'total': total
        }
    
    def validate_epoch(
        self, 
        val_loader: DataLoader
    ) -> Dict[str, float]:
        """ê²€ì¦ ì—í¬í¬"""
        
        if self.model is None:
            raise RuntimeError("ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        criterion = nn.CrossEntropyLoss()
        all_predictions = []
        all_labels = []
        
        print(f"  ğŸ“Š ê²€ì¦ ì¤‘... ({len(val_loader)} ë°°ì¹˜)")
        with torch.no_grad():
            for batch_idx, (images, labels) in enumerate(val_loader):
                images, labels = images.to(self.device), labels.to(self.device)
                
                outputs = self.model(images)
                loss = criterion(outputs, labels)
                
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
                
                # ê²€ì¦ ì§„í–‰ìƒí™© ì¶œë ¥
                if batch_idx % 3 == 0 or batch_idx == len(val_loader) - 1:
                    current_acc = correct / total if total > 0 else 0
                    print(f"    Val Batch {batch_idx+1}/{len(val_loader)}: Acc={current_acc:.2%}")
                
                all_predictions.extend(predicted.cpu())
                all_labels.extend(labels.cpu())
        
        epoch_loss = total_loss / len(val_loader)
        epoch_accuracy = correct / total
        
        # ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚°
        try:
            y_true = torch.tensor(all_labels)
            y_pred_logits = torch.zeros(len(all_predictions), self.num_classes)
            # ê°„ë‹¨í•œ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ë¡œì§“ ì‹œë®¬ë ˆì´ì…˜
            for i, pred in enumerate(all_predictions):
                y_pred_logits[i, pred] = 1.0
            
            detailed_metrics = self.metrics_evaluator.evaluate_predictions(y_true, y_pred_logits)
            
        except Exception as e:
            self.logger.warning(f"ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            detailed_metrics = None
        
        return {
            'loss': epoch_loss,
            'accuracy': epoch_accuracy,
            'correct': correct,
            'total': total,
            'detailed_metrics': detailed_metrics
        }
    
    def train_stage(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        max_epochs: int = 10,
        early_stopping_patience: int = 5
    ) -> Dict[str, Any]:
        """ì „ì²´ Stage í•™ìŠµ"""
        
        self.logger.step("ë¶„ë¥˜ Stage í•™ìŠµ", f"{max_epochs} ì—í¬í¬ ëª©í‘œ ì •í™•ë„ {self.target_accuracy:.1%}")
        
        start_time = time.time()
        patience_counter = 0
        
        for epoch in range(1, max_epochs + 1):
            # GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
            memory_stats = self.memory_monitor.get_current_usage()
            self.logger.info(f"Epoch {epoch}/{max_epochs} - GPU: {memory_stats['used_gb']:.1f}GB")
            
            # í•™ìŠµ
            train_results = self.train_epoch(train_loader, epoch)
            
            # ê²€ì¦
            val_results = self.validate_epoch(val_loader)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            if self.scheduler:
                self.scheduler.step()
            
            # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
            if val_results['accuracy'] > self.best_accuracy:
                self.best_accuracy = val_results['accuracy']
                patience_counter = 0
                self.logger.metric("best_accuracy", self.best_accuracy, "%")
                
                # ëª¨ë¸ ì €ì¥
                self._save_best_model()
            else:
                patience_counter += 1
            
            # í•™ìŠµ íˆìŠ¤í† ë¦¬ ê¸°ë¡
            epoch_history = {
                'epoch': epoch,
                'train_loss': train_results['loss'],
                'train_accuracy': train_results['accuracy'],
                'val_loss': val_results['loss'],
                'val_accuracy': val_results['accuracy'],
                'learning_rate': self.optimizer.param_groups[0]['lr'] if self.optimizer else 0
            }
            self.training_history.append(epoch_history)
            
            self.logger.info(f"Epoch {epoch} - Train: {train_results['accuracy']:.1%}, "
                           f"Val: {val_results['accuracy']:.1%}")
            
            # ëª©í‘œ ë‹¬ì„± ì²´í¬
            if val_results['accuracy'] >= self.target_accuracy:
                self.logger.success(f"ğŸ‰ ëª©í‘œ ì •í™•ë„ ë‹¬ì„±! {val_results['accuracy']:.1%} >= {self.target_accuracy:.1%}")
                break
            
            # Early Stopping
            if patience_counter >= early_stopping_patience:
                self.logger.warning(f"Early stopping at epoch {epoch}")
                break
        
        total_time = time.time() - start_time
        
        # ìµœì¢… ê²°ê³¼
        final_results = {
            'best_accuracy': self.best_accuracy,
            'target_achieved': self.best_accuracy >= self.target_accuracy,
            'epochs_completed': len(self.training_history),
            'total_time_minutes': total_time / 60,
            'training_history': self.training_history,
            'final_val_results': val_results
        }
        
        self.logger.success(f"ë¶„ë¥˜ í•™ìŠµ ì™„ë£Œ - ìµœê³  ì •í™•ë„: {self.best_accuracy:.1%}")
        return final_results
    
    def _save_best_model(self) -> None:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥"""
        try:
            save_dir = Path("artifacts/models/classification")
            save_dir.mkdir(parents=True, exist_ok=True)
            
            model_path = save_dir / f"best_classifier_{self.num_classes}classes.pt"
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'best_accuracy': self.best_accuracy,
                'num_classes': self.num_classes
            }, model_path)
            
            self.logger.info(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {model_path}")
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    


def main():
    """CLIë¥¼ í†µí•œ ë¶„ë¥˜ Stage í•™ìŠµ ì‹¤í–‰"""
    import argparse
    import os
    
    parser = argparse.ArgumentParser(description="PillSnap Classification Stage Training")
    parser.add_argument("--stage", type=int, default=1, help="Progressive Validation Stage (1-4)")
    parser.add_argument("--epochs", type=int, default=5, help="Number of training epochs")
    parser.add_argument("--batch-size", type=int, default=32, help="Batch size for training")
    parser.add_argument("--learning-rate", type=float, default=1e-3, help="Learning rate")
    parser.add_argument("--device", type=str, default="cuda", help="Device (cuda/cpu)")
    parser.add_argument("--dry-run", action="store_true", help="Dry run without actual training")
    parser.add_argument("--resume", action="store_true", help="Resume from checkpoint if available")
    
    args = parser.parse_args()
    
    print(f"ğŸš€ Classification Stage {args.stage} Training")
    print("=" * 60)
    print(f"ğŸ“Š Parameters: epochs={args.epochs}, batch_size={args.batch_size}, lr={args.learning_rate}")
    print(f"ğŸ–¥ï¸  Device: {args.device}")
    
    # Stageë³„ í´ë˜ìŠ¤ ìˆ˜ ì„¤ì •
    stage_classes = {1: 50, 2: 250, 3: 1000, 4: 4523}
    num_classes = stage_classes.get(args.stage, 50)
    
    if args.dry_run:
        print("ğŸ” Dry Run Mode - ì„¤ì • ê²€ì¦ë§Œ ìˆ˜í–‰")
        trainer = ClassificationStageTrainer(
            num_classes=num_classes, 
            target_accuracy=0.40,
            device=args.device
        )
        trainer.setup_model_and_optimizers(learning_rate=args.learning_rate)
        print("âœ… ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
        print("ì‹¤ì œ í•™ìŠµì„ ìœ„í•´ì„œëŠ” --dry-run ì—†ì´ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    # Stage 1 ìƒ˜í”Œ ë°ì´í„° í™•ì¸
    stage1_sample_path = "artifacts/stage1/sampling/stage1_sample.json"
    if not os.path.exists(stage1_sample_path):
        print(f"âŒ Stage 1 ìƒ˜í”Œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {stage1_sample_path}")
        print("ë¨¼ì € Progressive Validation ìƒ˜í”Œë§ì„ ì‹¤í–‰í•˜ì„¸ìš”:")
        print("./scripts/python_safe.sh -m src.data.progressive_validation_sampler")
        return
    
    print("âœ… Stage 1 ìƒ˜í”Œ ë°ì´í„° í™•ì¸ë¨")
    
    # ì‹¤ì œ í•™ìŠµ ì‹œì‘
    print("ğŸš€ ì‹¤ì œ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    
    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    trainer = ClassificationStageTrainer(
        num_classes=num_classes,
        target_accuracy=0.40,
        device=args.device
    )
    trainer.setup_model_and_optimizers(learning_rate=args.learning_rate)
    
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    print("ğŸ“Š ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...")
    from src.data.dataloader_single_pill_training import SinglePillTrainingDataLoader
    
    dataloader_manager = SinglePillTrainingDataLoader(
        stage=args.stage,
        batch_size=args.batch_size
        # num_workersëŠ” ì‹œìŠ¤í…œ ìµœì í™”ë¥¼ í†µí•´ ìë™ ì„¤ì •
    )
    
    train_loader, val_loader, metadata = dataloader_manager.get_stage_dataloaders()
    
    print(f"âœ… ë°ì´í„°ë¡œë” ì¤€ë¹„ ì™„ë£Œ")
    print(f"   í´ë˜ìŠ¤ ìˆ˜: {metadata['num_classes']}")
    print(f"   í•™ìŠµ ë°ì´í„°: {metadata['train_size']}ê°œ")
    print(f"   ê²€ì¦ ë°ì´í„°: {metadata['val_size']}ê°œ")
    
    # ì‹¤ì œ í•™ìŠµ ì‹¤í–‰
    print(f"ğŸ‹ï¸ í•™ìŠµ ì‹œì‘ - {args.epochs} epochs")
    
    try:
        # ì‹œìŠ¤í…œ ìµœì í™”ëœ DataLoader ì‚¬ìš©
        print("ğŸ”§ ì‹œìŠ¤í…œ ìµœì í™”ëœ DataLoader ì¬ìƒì„±")
        dataloader_manager_optimized = SinglePillTrainingDataLoader(
            stage=args.stage,
            batch_size=args.batch_size
            # num_workersëŠ” ìë™ ìµœì í™”ë¨
        )
        
        train_loader, val_loader, metadata = dataloader_manager_optimized.get_stage_dataloaders()
        print(f"âœ… ìµœì í™”ëœ ë°ì´í„°ë¡œë” ì¤€ë¹„ ì™„ë£Œ")
        
        # ì›ë˜ train_stage() í˜¸ì¶œ
        print("ğŸš€ ì›ë˜ train_stage() ë©”ì„œë“œ í˜¸ì¶œ")
        results = trainer.train_stage(
            train_loader=train_loader,
            val_loader=val_loader,
            max_epochs=args.epochs,
            early_stopping_patience=5
        )
        
        print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
        print(f"   ìµœê³  ì •í™•ë„: {results['best_accuracy']:.1%}")
        print(f"   ëª©í‘œ ë‹¬ì„±: {results['target_achieved']}")
        print(f"   ì™„ë£Œ ì—í¬í¬: {results['epochs_completed']}")
        print(f"   ì†Œìš” ì‹œê°„: {results['total_time_minutes']:.1f}ë¶„")
            
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()