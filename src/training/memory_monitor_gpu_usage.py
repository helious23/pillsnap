"""
GPU Memory Usage Monitor
GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

RTX 5080 16GB ìµœì í™”:
- ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
- OOM ì˜ˆë°© ë° ê²½ê³  ì‹œìŠ¤í…œ
- ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì • ì§€ì›
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë¶„ì„
"""

import time
import gc
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path

import torch
import psutil
import GPUtil

from src.utils.core import PillSnapLogger


@dataclass
class MemorySnapshot:
    """ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ë°ì´í„°"""
    timestamp: float
    gpu_used_gb: float
    gpu_total_gb: float
    gpu_utilization_percent: float
    system_ram_used_gb: float
    system_ram_total_gb: float
    torch_cache_gb: float
    torch_allocated_gb: float


class GPUMemoryMonitor:
    """GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, target_memory_gb: float = 14.0):
        self.target_memory_gb = target_memory_gb
        self.logger = PillSnapLogger(__name__)
        self.snapshots: List[MemorySnapshot] = []
        self.warning_threshold = 0.9  # 90% ì‚¬ìš© ì‹œ ê²½ê³ 
        self.critical_threshold = 0.95  # 95% ì‚¬ìš© ì‹œ ìœ„í—˜
        
        # GPU ì •ë³´ ì´ˆê¸°í™”
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
        
        if self.gpu_count > 0:
            self.gpu_name = torch.cuda.get_device_name(0)
            self.gpu_total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            self.logger.info(f"GPU ëª¨ë‹ˆí„°ë§ ì´ˆê¸°í™”: {self.gpu_name} ({self.gpu_total_memory:.1f}GB)")
        else:
            self.logger.warning("CUDA GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - CPU ëª¨ë“œ")
    
    def get_current_usage(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            if not torch.cuda.is_available():
                return self._get_cpu_memory_stats()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë³´
            gpu_stats = torch.cuda.memory_stats(self.device)
            allocated_gb = gpu_stats['allocated_bytes.all.current'] / (1024**3)
            cached_gb = gpu_stats['reserved_bytes.all.current'] / (1024**3)
            
            # GPU ì‚¬ìš©ë¥  (GPUtil ì‚¬ìš©)
            try:
                gpus = GPUtil.getGPUs()
                gpu = gpus[0] if gpus else None
                gpu_used_gb = gpu.memoryUsed / 1024 if gpu else allocated_gb
                gpu_total_gb = gpu.memoryTotal / 1024 if gpu else self.gpu_total_memory
                gpu_utilization = gpu.load * 100 if gpu else 0.0
            except:
                gpu_used_gb = allocated_gb
                gpu_total_gb = self.gpu_total_memory
                gpu_utilization = 0.0
            
            # ì‹œìŠ¤í…œ RAM ì •ë³´
            system_memory = psutil.virtual_memory()
            system_used_gb = (system_memory.total - system_memory.available) / (1024**3)
            system_total_gb = system_memory.total / (1024**3)
            
            # ìŠ¤ëƒ…ìƒ· ìƒì„±
            snapshot = MemorySnapshot(
                timestamp=time.time(),
                gpu_used_gb=gpu_used_gb,
                gpu_total_gb=gpu_total_gb,
                gpu_utilization_percent=gpu_utilization,
                system_ram_used_gb=system_used_gb,
                system_ram_total_gb=system_total_gb,
                torch_cache_gb=cached_gb,
                torch_allocated_gb=allocated_gb
            )
            
            # ìŠ¤ëƒ…ìƒ· ì €ì¥ (ìµœê·¼ 100ê°œë§Œ)
            self.snapshots.append(snapshot)
            if len(self.snapshots) > 100:
                self.snapshots.pop(0)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê²€ì‚¬
            self._check_memory_thresholds(snapshot)
            
            return {
                'used_gb': gpu_used_gb,
                'total_gb': gpu_total_gb,
                'utilization_percent': gpu_utilization,
                'allocated_gb': allocated_gb,
                'cached_gb': cached_gb,
                'usage_ratio': gpu_used_gb / gpu_total_gb if gpu_total_gb > 0 else 0,
                'system_ram_used_gb': system_used_gb,
                'system_ram_total_gb': system_total_gb
            }
            
        except Exception as e:
            self.logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self._get_fallback_stats()
    
    def _get_cpu_memory_stats(self) -> Dict[str, float]:
        """CPU ëª¨ë“œ ë©”ëª¨ë¦¬ í†µê³„"""
        system_memory = psutil.virtual_memory()
        return {
            'used_gb': 0.0,
            'total_gb': 0.0,
            'utilization_percent': 0.0,
            'allocated_gb': 0.0,
            'cached_gb': 0.0,
            'usage_ratio': 0.0,
            'system_ram_used_gb': (system_memory.total - system_memory.available) / (1024**3),
            'system_ram_total_gb': system_memory.total / (1024**3)
        }
    
    def _get_fallback_stats(self) -> Dict[str, float]:
        """í´ë°± í†µê³„ (ì—ëŸ¬ ì‹œ)"""
        return {
            'used_gb': 0.0,
            'total_gb': 16.0,  # RTX 5080 ê¸°ë³¸ê°’
            'utilization_percent': 0.0,
            'allocated_gb': 0.0,
            'cached_gb': 0.0,
            'usage_ratio': 0.0,
            'system_ram_used_gb': 0.0,
            'system_ram_total_gb': 128.0  # ì‹œìŠ¤í…œ ê¸°ë³¸ê°’
        }
    
    def _check_memory_thresholds(self, snapshot: MemorySnapshot) -> None:
        """ë©”ëª¨ë¦¬ ì„ê³„ê°’ ê²€ì‚¬ ë° ê²½ê³ """
        usage_ratio = snapshot.gpu_used_gb / snapshot.gpu_total_gb
        
        if usage_ratio >= self.critical_threshold:
            self.logger.error(f"ğŸš¨ GPU ë©”ëª¨ë¦¬ ìœ„í—˜ ìˆ˜ì¤€: {usage_ratio:.1%} "
                             f"({snapshot.gpu_used_gb:.1f}GB/{snapshot.gpu_total_gb:.1f}GB)")
            self.logger.error("ì¦‰ì‹œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ ë˜ëŠ” ë©”ëª¨ë¦¬ ì •ë¦¬ í•„ìš”!")
            
        elif usage_ratio >= self.warning_threshold:
            self.logger.warning(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ê²½ê³  ìˆ˜ì¤€: {usage_ratio:.1%} "
                               f"({snapshot.gpu_used_gb:.1f}GB/{snapshot.gpu_total_gb:.1f}GB)")
            self.logger.warning("ë°°ì¹˜ í¬ê¸° ì¡°ì • ê¶Œì¥")
        
        # ëª©í‘œ ë©”ëª¨ë¦¬ ì´ˆê³¼ í™•ì¸
        if snapshot.gpu_used_gb > self.target_memory_gb:
            self.logger.warning(f"ëª©í‘œ ë©”ëª¨ë¦¬ ì´ˆê³¼: {snapshot.gpu_used_gb:.1f}GB > {self.target_memory_gb:.1f}GB")
    
    def suggest_optimal_batch_size(self, current_batch_size: int, model_memory_estimate: float) -> int:
        """ìµœì  ë°°ì¹˜ í¬ê¸° ì œì•ˆ"""
        try:
            current_stats = self.get_current_usage()
            available_memory = current_stats['total_gb'] - current_stats['used_gb']
            
            # ì•ˆì „ ë§ˆì§„ (20% ì—¬ìœ  ê³µê°„)
            safe_memory = available_memory * 0.8
            
            # ë°°ì¹˜ë‹¹ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
            memory_per_batch = model_memory_estimate
            max_safe_batches = int(safe_memory / memory_per_batch)
            
            # ìµœì†Œ 1, ìµœëŒ€ í˜„ì¬ ë°°ì¹˜ í¬ê¸°ì˜ 2ë°°
            suggested_batch_size = max(1, min(max_safe_batches, current_batch_size * 2))
            
            if suggested_batch_size != current_batch_size:
                self.logger.info(f"ë°°ì¹˜ í¬ê¸° ì œì•ˆ: {current_batch_size} â†’ {suggested_batch_size}")
                self.logger.info(f"ê·¼ê±°: ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬ {available_memory:.1f}GB, "
                                f"ë°°ì¹˜ë‹¹ {memory_per_batch:.1f}GB")
            
            return suggested_batch_size
            
        except Exception as e:
            self.logger.warning(f"ë°°ì¹˜ í¬ê¸° ìµœì í™” ì‹¤íŒ¨: {e}")
            return current_batch_size
    
    def force_memory_cleanup(self) -> Dict[str, float]:
        """ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        self.logger.info("GPU ë©”ëª¨ë¦¬ ê°•ì œ ì •ë¦¬ ì‹œì‘...")
        
        try:
            # ì‚¬ì „ ìƒíƒœ ê¸°ë¡
            before_stats = self.get_current_usage()
            
            # PyTorch ìºì‹œ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            
            # ì‚¬í›„ ìƒíƒœ í™•ì¸
            time.sleep(0.5)  # ì •ë¦¬ ì™„ë£Œ ëŒ€ê¸°
            after_stats = self.get_current_usage()
            
            freed_memory = before_stats['used_gb'] - after_stats['used_gb']
            
            if freed_memory > 0.1:  # 100MB ì´ìƒ ì •ë¦¬ë¨
                self.logger.success(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {freed_memory:.1f}GB í™•ë³´")
            else:
                self.logger.info("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ (ë³€í™” ë¯¸ë¯¸)")
            
            return {
                'before_used_gb': before_stats['used_gb'],
                'after_used_gb': after_stats['used_gb'],
                'freed_gb': freed_memory
            }
            
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {'freed_gb': 0.0}
    
    def get_memory_efficiency_report(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë¦¬í¬íŠ¸"""
        if len(self.snapshots) < 10:
            return {"error": "ì¶©ë¶„í•œ ë°ì´í„° ì—†ìŒ (ìµœì†Œ 10ê°œ ìŠ¤ëƒ…ìƒ· í•„ìš”)"}
        
        try:
            recent_snapshots = self.snapshots[-10:]  # ìµœê·¼ 10ê°œ
            
            # í‰ê·  ì‚¬ìš©ëŸ‰
            avg_usage = sum(s.gpu_used_gb for s in recent_snapshots) / len(recent_snapshots)
            
            # ìµœëŒ€/ìµœì†Œ ì‚¬ìš©ëŸ‰
            max_usage = max(s.gpu_used_gb for s in recent_snapshots)
            min_usage = min(s.gpu_used_gb for s in recent_snapshots)
            
            # ì‚¬ìš©ëŸ‰ ë³€ë™ì„±
            usage_variance = sum((s.gpu_used_gb - avg_usage) ** 2 for s in recent_snapshots) / len(recent_snapshots)
            
            # ëª©í‘œ ë©”ëª¨ë¦¬ ëŒ€ë¹„ íš¨ìœ¨ì„±
            target_efficiency = avg_usage / self.target_memory_gb if self.target_memory_gb > 0 else 0
            
            # ì „ì²´ ë©”ëª¨ë¦¬ ëŒ€ë¹„ íš¨ìœ¨ì„±
            total_memory = recent_snapshots[0].gpu_total_gb
            total_efficiency = avg_usage / total_memory if total_memory > 0 else 0
            
            report = {
                'average_usage_gb': avg_usage,
                'max_usage_gb': max_usage,
                'min_usage_gb': min_usage,
                'usage_variance': usage_variance,
                'target_efficiency_ratio': target_efficiency,
                'total_efficiency_ratio': total_efficiency,
                'stability_score': 1.0 - (usage_variance / avg_usage) if avg_usage > 0 else 0
            }
            
            # íš¨ìœ¨ì„± í‰ê°€
            if target_efficiency < 0.7:
                efficiency_status = "ìš°ìˆ˜ (ì—¬ìœ  ì¶©ë¶„)"
            elif target_efficiency < 0.9:
                efficiency_status = "ì–‘í˜¸ (ì ì • ìˆ˜ì¤€)"
            elif target_efficiency < 1.0:
                efficiency_status = "ì£¼ì˜ (ëª©í‘œ ê·¼ì ‘)"
            else:
                efficiency_status = "ìœ„í—˜ (ëª©í‘œ ì´ˆê³¼)"
            
            report['efficiency_status'] = efficiency_status
            
            return report
            
        except Exception as e:
            self.logger.error(f"íš¨ìœ¨ì„± ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def save_monitoring_report(self) -> str:
        """ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ ì €ì¥"""
        try:
            report_dir = Path("artifacts/reports/performance_benchmark_reports")
            report_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            report_file = report_dir / f"gpu_memory_monitoring_{timestamp}.json"
            
            # ë¦¬í¬íŠ¸ ë°ì´í„° ìƒì„±
            efficiency_report = self.get_memory_efficiency_report()
            current_stats = self.get_current_usage()
            
            report_data = {
                'timestamp': timestamp,
                'gpu_name': getattr(self, 'gpu_name', 'Unknown'),
                'target_memory_gb': self.target_memory_gb,
                'current_stats': current_stats,
                'efficiency_report': efficiency_report,
                'snapshot_count': len(self.snapshots),
                'recent_snapshots': [
                    {
                        'timestamp': s.timestamp,
                        'gpu_used_gb': s.gpu_used_gb,
                        'gpu_utilization_percent': s.gpu_utilization_percent
                    } for s in self.snapshots[-20:]  # ìµœê·¼ 20ê°œ
                ]
            }
            
            import json
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
            return str(report_file)
            
        except Exception as e:
            self.logger.error(f"ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""


def main():
    """GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„° í…ŒìŠ¤íŠ¸"""
    print("ğŸ” GPU Memory Monitor Test")
    print("=" * 50)
    
    monitor = GPUMemoryMonitor(target_memory_gb=14.0)
    
    # í˜„ì¬ ìƒíƒœ í™•ì¸
    stats = monitor.get_current_usage()
    print(f"GPU ë©”ëª¨ë¦¬: {stats['used_gb']:.1f}GB / {stats['total_gb']:.1f}GB ({stats['usage_ratio']:.1%})")
    print(f"ì‹œìŠ¤í…œ RAM: {stats['system_ram_used_gb']:.1f}GB / {stats['system_ram_total_gb']:.1f}GB")
    
    # íš¨ìœ¨ì„± ë¦¬í¬íŠ¸ (10ì´ˆê°„ ëª¨ë‹ˆí„°ë§)
    print("\n10ì´ˆê°„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§...")
    for i in range(10):
        monitor.get_current_usage()
        time.sleep(1)
    
    efficiency_report = monitor.get_memory_efficiency_report()
    if 'error' not in efficiency_report:
        print(f"\nğŸ“Š íš¨ìœ¨ì„± ë¶„ì„:")
        print(f"  í‰ê·  ì‚¬ìš©ëŸ‰: {efficiency_report['average_usage_gb']:.1f}GB")
        print(f"  íš¨ìœ¨ì„± ìƒíƒœ: {efficiency_report['efficiency_status']}")
        print(f"  ì•ˆì •ì„± ì ìˆ˜: {efficiency_report['stability_score']:.3f}")
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    report_file = monitor.save_monitoring_report()
    if report_file:
        print(f"\nğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥ë¨: {report_file}")
    
    print("\nâœ… GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    main()