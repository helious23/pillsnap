#!/usr/bin/env python3
"""
Stage 3 Two-Stage Pipeline í•™ìŠµê¸°

Detection + Classification í†µí•© í•™ìŠµ:
- YOLOv11x Detection (Stage 4 ì¤€ë¹„ìš© ê¸°ëŠ¥ ê²€ì¦)
- EfficientNetV2-L Classification (ë†’ì€ ì„±ëŠ¥ ìœ ì§€)  
- êµì°¨ í•™ìŠµ (Interleaved Training)
- RTX 5080 ìµœì í™” (Mixed Precision, torch.compile)
- ëª©í‘œ: Detection mAP@0.5 â‰¥ 0.30, Classification Accuracy â‰¥ 85%
"""

import os
import sys
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.amp import GradScaler
import torch.amp
from pathlib import Path
from typing import Dict, Optional, Tuple, Any, List
import pandas as pd
from dataclasses import dataclass

from src.models.classifier_efficientnetv2 import PillSnapClassifier, create_pillsnap_classifier
from src.models.detector_yolo11m import PillSnapYOLODetector, create_pillsnap_detector
from src.data.dataloader_manifest_training import ManifestDataset, ManifestTrainingDataLoader
from src.training.memory_monitor_gpu_usage import GPUMemoryMonitor  
from src.evaluation.evaluate_classification_metrics import ClassificationMetricsEvaluator
from src.evaluation.evaluate_detection_metrics import DetectionMetricsEvaluator
from src.utils.core import PillSnapLogger, load_config
from src.training.tensorboard_integration import patch_trainer_with_tensorboard


@dataclass
class TwoStageTrainingConfig:
    """Two-Stage í•™ìŠµ ì„¤ì • - RTX 5080 Native Linux ìµœì í™”"""
    
    # í•™ìŠµ ê¸°ë³¸ ì„¤ì •
    max_epochs: int = 20
    learning_rate_classifier: float = 2e-4
    learning_rate_detector: float = 1e-3
    batch_size: int = 16
    
    # êµì°¨ í•™ìŠµ ì„¤ì • - ë¶„ë¥˜ê¸° ì¤‘ì‹¬
    interleaved_training: bool = True
    classifier_epochs_per_cycle: int = 1  # ì—í¬í¬ë‹¹ 1íšŒ í•™ìŠµ (ì •ìƒ ë™ì‘)
    detector_epochs_per_cycle: int = 1    # ê²€ì¶œê¸°ë„ 1íšŒ í•™ìŠµ
    
    # Native Linux ìµœì í™” ì„¤ì •
    mixed_precision: bool = True
    torch_compile: bool = True
    channels_last: bool = True
    
    # íƒ€ê²Ÿ ì§€í‘œ
    target_classification_accuracy: float = 0.85
    target_detection_map: float = 0.30


class Stage3TwoStageTrainer:
    """Stage 3 Two-Stage Pipeline í•™ìŠµê¸°"""
    
    def __init__(
        self,
        config_path: str = "config.yaml",
        manifest_train: str = "artifacts/stage3/manifest_train.csv",
        manifest_val: str = "artifacts/stage3/manifest_val.csv",
        device: str = "cuda",
        resume_checkpoint: str = None
    ):
        self.device = torch.device(device)
        self.logger = PillSnapLogger(__name__)
        
        # ì„¤ì • ë¡œë“œ
        self.config = load_config(config_path)
        self.manifest_train = Path(manifest_train)
        self.manifest_val = Path(manifest_val)
        
        # Stage 3 ì„¤ì • í™•ì¸
        self.stage_config = self.config['progressive_validation']['stage_configs']['stage_3']
        
        # í•™ìŠµ ì„¤ì •
        self.training_config = TwoStageTrainingConfig()
        self.seed = 42
        torch.manual_seed(self.seed)
        
        # Resume ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì €ì¥
        self._resume_checkpoint_path = resume_checkpoint
        
        # torch.compile ì›Œì»¤ ìˆ˜ ì„¤ì • (Smoke Test ê²€ì¦ëœ 8ê°œ)
        os.environ["TORCH_COMPILE_MAX_PARALLEL_COMPILE_JOBS"] = "8"
        
        # ëª¨ë¸ ë° ë„êµ¬ ì´ˆê¸°í™”
        self.classifier = None
        self.detector = None
        self.classification_dataloader = None
        self.detection_dataloader = None
        self.memory_monitor = GPUMemoryMonitor()
        
        # Detection ì‹¤ì¸¡ì¹˜ ì¶”ì 
        self.last_detection_map = 0.0
        
        # DataLoader ìºì‹± (ë§¤ epochë§ˆë‹¤ ì¬ìƒì„± ë°©ì§€)
        self.train_loader_cache = None
        self.val_loader_cache = None
        
        # í•™ìŠµ ìƒíƒœ
        self.best_classification_accuracy = 0.0
        self.best_classification_top5_accuracy = 0.0
        self.best_detection_map = 0.0
        self.training_history = []
        
        self.logger.info("Stage 3 Two-Stage Pipeline Trainer ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ëª©í‘œ - Classification: {self.training_config.target_classification_accuracy:.1%}")
        self.logger.info(f"ëª©í‘œ - Detection mAP@0.5: {self.training_config.target_detection_map:.1%}")
    
    def setup_data_loaders(self) -> None:
        """ë°ì´í„° ë¡œë” ì„¤ì •"""
        
        try:
            self.logger.info("ë°ì´í„° ë¡œë” ì„¤ì • ì‹œì‘...")
            
            # Manifest íŒŒì¼ í™•ì¸
            if not self.manifest_train.exists():
                raise FileNotFoundError(f"í•™ìŠµ manifest íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.manifest_train}")
            if not self.manifest_val.exists():
                raise FileNotFoundError(f"ê²€ì¦ manifest íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.manifest_val}")
            
            # Classification ë°ì´í„° ë¡œë” (Single + Combination crop)
            train_manifest_df = pd.read_csv(self.manifest_train)
            val_manifest_df = pd.read_csv(self.manifest_val)
            
            self.logger.info(f"í•™ìŠµ ë°ì´í„°: {len(train_manifest_df)} ìƒ˜í”Œ")
            self.logger.info(f"ê²€ì¦ ë°ì´í„°: {len(val_manifest_df)} ìƒ˜í”Œ")
            
            # Single/Combination ë¹„ìœ¨ í™•ì¸ (ManifestëŠ” image_type ì»¬ëŸ¼ ì‚¬ìš©)
            train_single = train_manifest_df[train_manifest_df['image_type'] == 'single']
            train_combo = train_manifest_df[train_manifest_df['image_type'] == 'combination']
            
            self.logger.info(f"í•™ìŠµ - Single: {len(train_single)} ({len(train_single)/len(train_manifest_df):.1%})")
            self.logger.info(f"í•™ìŠµ - Combination: {len(train_combo)} ({len(train_combo)/len(train_manifest_df):.1%})")
            
            # Classification ë°ì´í„°ë¡œë” (ì „ì²´ ë°ì´í„°)
            self.classification_dataloader = ManifestTrainingDataLoader(
                manifest_train_path=str(self.manifest_train),
                manifest_val_path=str(self.manifest_val),
                batch_size=self.training_config.batch_size,
                image_size=384,  # EfficientNetV2-L
                num_workers=8,  # Native Linux ìµœì í™”
                task="classification"
            )
            
            # Detection ë°ì´í„°ë¡œë” (Combinationë§Œ)
            # Combination ë°ì´í„°ë§Œ ë³„ë„ manifest ìƒì„±
            combo_train_path = "artifacts/stage3/manifest_train_combo.csv"
            combo_val_path = "artifacts/stage3/manifest_val_combo.csv"
            
            train_combo.to_csv(combo_train_path, index=False)
            val_combo = val_manifest_df[val_manifest_df['image_type'] == 'combination']
            val_combo.to_csv(combo_val_path, index=False)
            
            self.detection_dataloader = ManifestTrainingDataLoader(
                manifest_train_path=combo_train_path,
                manifest_val_path=combo_val_path,
                batch_size=max(8, self.training_config.batch_size // 2),  # Detectionì€ ë” ì ì€ ë°°ì¹˜
                image_size=640,  # YOLOv11x
                num_workers=4,
                task="detection"
            )
            
            self.logger.info("ë°ì´í„° ë¡œë” ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ë¡œë” ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    def setup_models(self) -> None:
        """ëª¨ë¸ ì„¤ì •"""
        
        try:
            self.logger.info("ëª¨ë¸ ì„¤ì • ì‹œì‘...")
            
            # í´ë˜ìŠ¤ ìˆ˜ í™•ì¸ (ManifestëŠ” mapping_code ì»¬ëŸ¼ ì‚¬ìš©)
            train_manifest_df = pd.read_csv(self.manifest_train)
            num_classes = train_manifest_df['mapping_code'].nunique()
            self.logger.info(f"ë¶„ë¥˜ í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
            
            # Classification ëª¨ë¸ (EfficientNetV2-L)
            self.classifier = create_pillsnap_classifier(
                num_classes=num_classes,
                model_name="efficientnetv2_l", 
                pretrained=True,
                device=self.device
            )
            
            # Detection ëª¨ë¸ (YOLOv11x) - 1ê°œ í´ë˜ìŠ¤ (pill)
            self.detector = create_pillsnap_detector(
                num_classes=1,  # ì•½í’ˆ ê²€ì¶œìš©
                model_size="yolo11x",  # Stage 3+ ëŒ€í˜• ëª¨ë¸
                input_size=640,
                device=self.device
            )
            
            # ìµœì í™” ì ìš©
            if self.training_config.channels_last:
                self.classifier = self.classifier.to(memory_format=torch.channels_last)
                
            if self.training_config.torch_compile:
                # reduce-overhead ëª¨ë“œ: ì»´íŒŒì¼ ì‹œê°„ ë‹¨ì¶•, ì•ˆì •ì  ì„±ëŠ¥
                self.classifier = torch.compile(self.classifier, mode='reduce-overhead')
                self.logger.info("torch.compile ìµœì í™” ì ìš© (reduce-overhead ëª¨ë“œ)")
            
            self.logger.info("ëª¨ë¸ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    def setup_optimizers(self) -> Tuple[optim.Optimizer, optim.Optimizer]:
        """ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •"""
        
        # ConfigProviderì—ì„œ learning rate ê°€ì ¸ì˜¤ê¸° (ëŸ°íƒ€ì„ ì˜¤ë²„ë¼ì´ë“œ ì§€ì›)
        from src.utils.core import config_provider
        
        lr_classifier = config_provider.get('train.lr_classifier', self.training_config.learning_rate_classifier)
        lr_detector = config_provider.get('train.lr_detector', self.training_config.learning_rate_detector)
        
        # CLI ì¸ìë¡œ ì˜¤ë²„ë¼ì´ë“œ í™•ì¸
        if hasattr(self, '_lr_classifier_override') and self._lr_classifier_override:
            lr_classifier = self._lr_classifier_override
            self.logger.info(f"Classifier LR override: {lr_classifier}")
        
        if hasattr(self, '_lr_detector_override') and self._lr_detector_override:
            lr_detector = self._lr_detector_override
            self.logger.info(f"Detector LR override: {lr_detector}")
        
        # ì˜µí‹°ë§ˆì´ì € ìƒì„±
        classifier_optimizer = optim.AdamW(
            self.classifier.parameters(),
            lr=lr_classifier,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        detector_optimizer = optim.AdamW(
            self.detector.parameters(),
            lr=lr_detector,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„± (CosineAnnealingWarmRestarts)
        self.scheduler_cls = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            classifier_optimizer,
            T_0=10,  # ì²« ë²ˆì§¸ restartê¹Œì§€ epoch ìˆ˜
            T_mult=2,  # restart ì£¼ê¸° ë°°ìˆ˜
            eta_min=1e-6  # ìµœì†Œ learning rate
        )
        
        self.scheduler_det = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            detector_optimizer,
            T_0=10,
            T_mult=2,
            eta_min=1e-6
        )
        
        self.logger.info(f"Optimizers ì„¤ì • ì™„ë£Œ:")
        self.logger.info(f"  - Classifier LR: {lr_classifier}")
        self.logger.info(f"  - Detector LR: {lr_detector}")
        self.logger.info(f"  - Scheduler: CosineAnnealingWarmRestarts (T_0=10, T_mult=2)")
        
        return classifier_optimizer, detector_optimizer
    
    def train_classification_epoch(
        self, 
        optimizer: optim.Optimizer, 
        scaler: GradScaler,
        epoch: int
    ) -> Dict[str, float]:
        """ë¶„ë¥˜ê¸° í•œ ì—í¬í¬ í•™ìŠµ"""
        
        self.classifier.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        # DataLoader ìºì‹±: ì²« ë²ˆì§¸ epochì—ì„œë§Œ ìƒì„±
        if self.train_loader_cache is None:
            self.train_loader_cache = self.classification_dataloader.get_train_loader()
            self.logger.info("Train DataLoader ìºì‹œ ìƒì„± ì™„ë£Œ")
        
        train_loader = self.train_loader_cache
        
        for batch_idx, (images, labels) in enumerate(train_loader):
            images = images.to(self.device, non_blocking=True)
            labels = labels.to(self.device, non_blocking=True)
            
            if self.training_config.channels_last:
                images = images.to(memory_format=torch.channels_last)
            
            optimizer.zero_grad()
            
            with torch.amp.autocast('cuda', enabled=self.training_config.mixed_precision):
                outputs = self.classifier(images)
                loss = nn.CrossEntropyLoss()(outputs, labels)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            if batch_idx % 20 == 0:  # 20 ë°°ì¹˜ë§ˆë‹¤ ì¶œë ¥ (ë” ìì£¼)
                self.logger.info(f"Epoch {epoch} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}")
                
                # TensorBoard ë¡œê¹…
                if hasattr(self, 'log_tb_classification_batch'):
                    current_lr = optimizer.param_groups[0]['lr']
                    batch_accuracy = correct / total if total > 0 else 0
                    self.log_tb_classification_batch(
                        epoch=epoch,
                        batch_idx=batch_idx,
                        total_batches=len(train_loader),
                        loss=loss.item(),
                        accuracy=batch_accuracy,
                        lr=current_lr
                    )
                
            # ì²˜ìŒ ëª‡ ê°œ ë°°ì¹˜ëŠ” ë” ìì£¼ ì¶œë ¥
            if batch_idx < 10:
                self.logger.info(f"ì´ˆê¸° ë°°ì¹˜ {batch_idx} | Loss: {loss.item():.4f}")
        
        accuracy = correct / total
        avg_loss = total_loss / len(train_loader)
        
        # Classification í•™ìŠµ ì™„ë£Œ ë¡œê¹…
        self.logger.info(f"âœ… Classification Epoch {epoch} ì™„ë£Œ | Loss: {avg_loss:.4f} | Train Accuracy: {accuracy:.4f}")
        
        return {
            'classification_loss': avg_loss,
            'classification_accuracy': accuracy
        }
    
    def train_detection_epoch(
        self,
        optimizer: optim.Optimizer,
        epoch: int
    ) -> Dict[str, float]:
        """ê²€ì¶œê¸° í•œ ì—í¿í¬ í•™ìŠµ - Ultralytics YOLO.train() ì‚¬ìš© (ëˆ„ì  í•™ìŠµ + ì‹¤ì œ ë©”íŠ¸ë¦­)"""
        
        import subprocess
        import tempfile
        import time
        
        # ê²€ì¦ ì£¼ê¸° ìƒìˆ˜
        VAL_PERIOD = 3  # 3 ì—í­ë§ˆë‹¤ ê²€ì¦
        YOLO_PROJECT = '/home/max16/pillsnap/artifacts/yolo'
        YOLO_NAME = 'stage3'
        
        try:
            # ëª¨ë¸ ê²½ë¡œ ë° resume ì„¤ì • (ê°œì„ ëœ ë¡œì§)
            last_pt_path = Path(YOLO_PROJECT) / YOLO_NAME / 'weights' / 'last.pt'
            best_pt_path = Path(YOLO_PROJECT) / YOLO_NAME / 'weights' / 'best.pt'
            
            # YOLO ëª¨ë¸ ì´ˆê¸°í™” ì „ëµ
            if epoch == 1:
                # ì²« ì—í¬í¬: resume ì²´í¬í¬ì¸íŠ¸ ë˜ëŠ” pretrained ì‚¬ìš©
                if hasattr(self, '_resume_yolo_checkpoint') and self._resume_yolo_checkpoint:
                    # --resumeìœ¼ë¡œ ì§€ì •ëœ ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©
                    model_path = self._resume_yolo_checkpoint
                    resume = True
                    self.logger.info(f"ğŸ”„ Detection Resume: {model_path}")
                elif last_pt_path.exists():
                    # ì´ì „ ì‹¤í–‰ì˜ last.pt ì¡´ì¬ì‹œ ì‚¬ìš©
                    model_path = str(last_pt_path)
                    resume = True
                    self.logger.info(f"ğŸ”„ Detection ì´ì „ í•™ìŠµ ì¬ê°œ: {model_path}")
                else:
                    # ì²˜ìŒë¶€í„° ì‹œì‘
                    model_path = 'yolo11x.pt'
                    resume = False
                    self.logger.info(f"ğŸ†• Detection í•™ìŠµ ì‹œì‘: {model_path}")
            else:
                # ì´í›„ ì—í¬í¬: í•­ìƒ last.ptì—ì„œ ì´ì–´ì„œ í•™ìŠµ
                if last_pt_path.exists():
                    model_path = str(last_pt_path)
                    resume = True  # í•­ìƒ True (í•™ìŠµ ì§€ì†)
                    self.logger.info(f"âœ… Detection í•™ìŠµ ì§€ì†: {model_path}")
                else:
                    # Fallback: best.pt ë˜ëŠ” pretrained
                    if best_pt_path.exists():
                        model_path = str(best_pt_path)
                        resume = True
                        self.logger.warning(f"âš ï¸ last.pt ì—†ìŒ, best.pt ì‚¬ìš©: {model_path}")
                    else:
                        model_path = 'yolo11x.pt'
                        resume = False
                        self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ, pretrained ì‚¬ìš©: {model_path}")
            
            self.logger.info(f"ğŸ¯ Detection í•™ìŠµ ì‹œì‘ (Epoch {epoch})")
            
            # YOLO ë°ì´í„°ì…‹ ì„¤ì • íŒŒì¼ ìƒì„±
            dataset_yaml = self._create_yolo_dataset_config()
            
            # ê²€ì¦ ì—¬ë¶€ ê²°ì •
            do_validation = (epoch % VAL_PERIOD == 0)
            
            # ì„ì‹œ ë¡œê·¸ íŒŒì¼
            with tempfile.NamedTemporaryFile(mode='w+', suffix='.log', delete=False) as temp_log:
                temp_log_path = temp_log.name
            
            # YOLO í•™ìŠµì„ subprocessë¡œ ì‹¤í–‰í•˜ì—¬ ì¶œë ¥ ìº¡ì²˜
            cmd = [
                sys.executable, '-c',
                f"""
import sys
sys.path.insert(0, '/home/max16/pillsnap')
from ultralytics import YOLO

model = YOLO('{model_path}')
results = model.train(
    data='{dataset_yaml}',
    epochs=1,
    batch={min(8, self.training_config.batch_size)},
    imgsz=640,
    device='{self.device.type}',
    save=True,  # í•­ìƒ ì €ì¥ (last.pt, best.pt)
    save_period=1,  # ë§¤ ì—í¬í¬ë§ˆë‹¤ ì €ì¥
    verbose=True,
    workers=4,
    rect=False,
    cache=False,
    plots=False,
    exist_ok=True,
    project='{YOLO_PROJECT}',
    name='{YOLO_NAME}',
    patience=0,  # Early stopping ë¹„í™œì„±í™”
    val={do_validation},
    resume={resume},  # ë™ì ìœ¼ë¡œ ì„¤ì •ëœ resume ê°’ ì‚¬ìš©
    deterministic=False,
    single_cls=False,
    optimizer='auto',
    seed=0,
    close_mosaic=10,
    copy_paste=0.0,
    auto_augment=None
)

# ê²°ê³¼ ì¶œë ¥
if hasattr(results, 'results_dict'):
    print("RESULTS_DICT:", results.results_dict)
if hasattr(results, 'metrics'):
    print("METRICS:", results.metrics)
"""
            ]
            
            self.logger.info(f"YOLO í•™ìŠµ subprocess ì‹¤í–‰ ì¤‘... (val={do_validation}, resume={resume})")
            
            # subprocess ì‹¤í–‰í•˜ê³  ì¶œë ¥ ì‹¤ì‹œê°„ ë¡œê¹…
            with open(temp_log_path, 'w') as log_file:
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    universal_newlines=True,
                    bufsize=1
                )
                
                # ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥ ì½ê¸°
                for line in iter(process.stdout.readline, ''):
                    if line:
                        line = line.strip()
                        log_file.write(line + '\n')
                        log_file.flush()
                        
                        # ì¤‘ìš”í•œ ë¡œê·¸ë§Œ í•„í„°ë§í•´ì„œ ì¶œë ¥
                        if any(keyword in line for keyword in ['box_loss', 'cls_loss', 'dfl_loss', 'Epoch', 'GPU', 'images']):
                            self.logger.info(f"[YOLO] {line}")
                            # ëª¨ë‹ˆí„°ë§ ë¡œê·¸ íŒŒì¼ì—ë„ ì“°ê¸°
                            with open('/tmp/mini_test.log', 'a') as monitor_log:
                                monitor_log.write(f"{time.strftime('%H:%M:%S')} | INFO     | [YOLO] {line}\n")
                                monitor_log.flush()
                
                process.wait()
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(temp_log_path)
            
            # results.csvì—ì„œ ì‹¤ì œ ë©”íŠ¸ë¦­ ì½ê¸°
            results_csv_path = Path(YOLO_PROJECT) / YOLO_NAME / 'results.csv'
            
            # ë©”íŠ¸ë¦­ ì»¬ëŸ¼ ë§¤í•‘ (ë²„ì „ í˜¸í™˜ì„±)
            METRIC_COLUMNS = {
                'mAP': ['metrics/mAP50(B)', 'metrics/mAP50', 'mAP50'],
                'precision': ['metrics/precision(B)', 'metrics/precision', 'precision'],
                'recall': ['metrics/recall(B)', 'metrics/recall', 'recall'],
                'box_loss': ['train/box_loss', 'box_loss'],
                'cls_loss': ['train/cls_loss', 'cls_loss'],
                'dfl_loss': ['train/dfl_loss', 'dfl_loss']
            }
            
            # ê¸°ë³¸ê°’ ì„¤ì •
            val_map = 0.0
            precision = 0.0
            recall = 0.0
            box_loss = None
            cls_loss = None
            dfl_loss = None
            
            # CSV ì½ê¸° ì‹œë„ (ì¬ì‹œë„ í¬í•¨)
            for attempt in range(3):
                try:
                    if results_csv_path.exists():
                        df = pd.read_csv(results_csv_path)
                        if not df.empty:
                            last_row = df.iloc[-1]
                            
                            # ê° ë©”íŠ¸ë¦­ì„ ì»¬ëŸ¼ í›„ë³´ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì°¾ê¸°
                            for metric_name, column_candidates in METRIC_COLUMNS.items():
                                for col in column_candidates:
                                    if col in last_row:
                                        value = last_row[col]
                                        if pd.notna(value):
                                            if metric_name == 'mAP':
                                                val_map = float(value)
                                            elif metric_name == 'precision':
                                                precision = float(value)
                                            elif metric_name == 'recall':
                                                recall = float(value)
                                            elif metric_name == 'box_loss':
                                                box_loss = float(value)
                                            elif metric_name == 'cls_loss':
                                                cls_loss = float(value)
                                            elif metric_name == 'dfl_loss':
                                                dfl_loss = float(value)
                                            break
                            
                            self.logger.info(f"âœ… CSV ë©”íŠ¸ë¦­ ë¡œë“œ ì„±ê³µ: {results_csv_path}")
                            break
                    else:
                        self.logger.warning(f"results.csv ì—†ìŒ: {results_csv_path}")
                        
                except Exception as e:
                    if attempt < 2:
                        time.sleep(0.5)  # ì§§ì€ ëŒ€ê¸° í›„ ì¬ì‹œë„
                    else:
                        self.logger.warning(f"CSV ì½ê¸° ì‹¤íŒ¨: {e}")
            
            # total_loss ê³„ì‚°
            if box_loss is not None and cls_loss is not None and dfl_loss is not None:
                total_loss = (box_loss + cls_loss + dfl_loss) / 3.0
            else:
                total_loss = None
                self.logger.warning("Loss ê°’ì„ CSVì—ì„œ ì½ì§€ ëª»í•¨, Noneìœ¼ë¡œ ì„¤ì •")
            
            # ìƒì„¸í•œ Detection ë©”íŠ¸ë¦­ ë¡œê¹…
            self.logger.info(f"âœ… Detection Epoch {epoch} ì™„ë£Œ")
            if box_loss is not None:
                self.logger.info(f"[Detection Epoch {epoch}] box_loss: {box_loss:.4f} | cls_loss: {cls_loss:.4f} | dfl_loss: {dfl_loss:.4f}")
            if total_loss is not None:
                self.logger.info(f"[Detection Epoch {epoch}] Total Loss: {total_loss:.4f} | mAP@0.5: {val_map:.3f}")
            
            # ì‹¤ì¸¡ì¹˜ ì €ì¥ (validate_modelsì—ì„œ ì‚¬ìš©)
            self.last_detection_map = val_map
            
            # ëª¨ë‹ˆí„°ë§ íŒŒì„œìš© í‘œì¤€ íƒœê·¸ (DET_SUMMARY)
            # ê°’ ë¨¼ì € ì •í•˜ê³  í¬ë§·íŒ…
            box_loss_val = 0.0 if box_loss is None else float(box_loss)
            cls_loss_val = 0.0 if cls_loss is None else float(cls_loss)
            dfl_loss_val = 0.0 if dfl_loss is None else float(dfl_loss)
            total_loss_val = 0.0 if total_loss is None else float(total_loss)
            
            self.logger.info(
                f"DET_SUMMARY | epoch={epoch} | "
                f"box_loss={box_loss_val:.4f} | "
                f"cls_loss={cls_loss_val:.4f} | "
                f"dfl_loss={dfl_loss_val:.4f} | "
                f"total_loss={total_loss_val:.4f} | "
                f"mAP={val_map:.4f}"
            )
            
            # DET_DETAIL ë¡œê·¸ (confidenceëŠ” 0.0ìœ¼ë¡œ ì„¤ì •, ë‚˜ì¤‘ì— tunerê°€ ë®ì–´ì“¸ ìˆ˜ ìˆìŒ)
            self.logger.info(
                f"DET_DETAIL | recall={recall:.3f} | precision={precision:.3f} | "
                f"det_conf=0.000 | cls_conf=0.000 | single_conf=0.000 | combo_conf=0.000"
            )
            
            return {
                'detection_loss': total_loss if total_loss is not None else 0.0,
                'detection_map': val_map,
                'detection_precision': precision,
                'detection_recall': recall
            }
            
        except Exception as e:
            self.logger.warning(f"Detection í•™ìŠµ ì—ëŸ¬ - ì†ìƒëœ íŒŒì¼ì´ë‚˜ ì¼ì‹œì  ë¬¸ì œë¡œ ìŠ¤í‚µ: {e}")
            # ì—ëŸ¬ ìœ í˜•ë³„ ì²˜ë¦¬
            error_msg = str(e).lower()
            if any(keyword in error_msg for keyword in ['truncated', 'corrupt', 'bad image', 'decode']):
                self.logger.info("ì´ë¯¸ì§€ íŒŒì¼ ê´€ë ¨ ì—ëŸ¬ - ë‹¤ìŒ epochì—ì„œ ìë™ìœ¼ë¡œ ìŠ¤í‚µë©ë‹ˆë‹¤")
            elif 'cuda' in error_msg or 'memory' in error_msg:
                self.logger.warning("GPU ë©”ëª¨ë¦¬ ë¶€ì¡± - ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ë³´ì„¸ìš”")
            else:
                self.logger.warning(f"ê¸°íƒ€ Detection ì—ëŸ¬: {error_msg}")
            
            # ê¸°ë³¸ ë©”íŠ¸ë¦­ ë°˜í™˜ (í•™ìŠµ ê³„ì† ì§„í–‰)
            return {
                'detection_loss': 0.0,
                'detection_map': 0.0,
                'detection_precision': 0.0,
                'detection_recall': 0.0
            }
    
    def _create_yolo_dataset_config(self) -> Path:
        """YOLO ë°ì´í„°ì…‹ ì„¤ì • íŒŒì¼ ìƒì„± - ì‹¤ì œ ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ ì¡°ì •"""
        import yaml
        
        # YOLO ì„¤ì • íŒŒì¼ ê²½ë¡œ
        config_dir = Path("/home/max16/pillsnap_data/yolo_configs")
        config_dir.mkdir(exist_ok=True)
        config_path = config_dir / "stage3_detection.yaml"
        
        # YOLO í˜¸í™˜ ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ìƒì„±
        yolo_dataset_root = config_dir / "yolo_dataset" 
        yolo_images_dir = yolo_dataset_root / "images"
        yolo_labels_dir = yolo_dataset_root / "labels"
        
        yolo_images_dir.mkdir(parents=True, exist_ok=True)
        yolo_labels_dir.mkdir(parents=True, exist_ok=True)
        
        # ê¸°ì¡´ ì‹¬ë³¼ë¦­ ë§í¬ë“¤ ì •ë¦¬
        for f in yolo_images_dir.glob("*"):
            if f.is_file() or f.is_symlink():
                try:
                    f.unlink()
                except Exception:
                    pass
        for f in yolo_labels_dir.glob("*"):
            if f.is_file() or f.is_symlink():
                try:
                    f.unlink()
                except Exception:
                    pass
        
        # ì´ë¯¸ì§€ì™€ ë¼ë²¨ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± (ë§¤ì¹­ë˜ëŠ” ê²ƒë§Œ + ìœ íš¨í•œ íŒŒì¼ë§Œ)
        base_path = Path("/home/max16/pillsnap_data/train/images/combination")
        label_path = Path("/home/max16/pillsnap_data/train/labels/combination_yolo")
        
        linked_count = 0
        skipped_count = 0
        
        for ts_dir in base_path.glob("TS_*_combo"):
            if not ts_dir.is_dir():
                continue
                
            for k_dir in ts_dir.iterdir():
                if not k_dir.is_dir():
                    continue
                    
                # ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ ì°¾ê³  ë§¤ì¹­ë˜ëŠ” ë¼ë²¨ì´ ìˆëŠ” ê²ƒë§Œ ë§í¬ (ìœ íš¨ì„± ê²€ì‚¬ í¬í•¨)
                for img_file in k_dir.glob("*_0_2_0_2_*.png"):
                    label_file = label_path / f"{img_file.stem}.txt"
                    
                    if label_file.exists():
                        # ì´ë¯¸ì§€ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
                        try:
                            # íŒŒì¼ í¬ê¸° ì²´í¬ (ì†ìƒëœ íŒŒì¼ì€ ë³´í†µ ë§¤ìš° ì‘ìŒ)
                            if img_file.stat().st_size < 100:  
                                self.logger.debug(f"ìŠ¤í‚µ: ë„ˆë¬´ ì‘ì€ íŒŒì¼ {img_file.name}")
                                skipped_count += 1
                                continue
                            
                            # ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
                            img_link = yolo_images_dir / img_file.name
                            label_link = yolo_labels_dir / label_file.name
                            
                            if not img_link.exists():
                                img_link.symlink_to(img_file.absolute())
                            if not label_link.exists():
                                label_link.symlink_to(label_file.absolute())
                                
                            linked_count += 1
                            
                        except (OSError, IOError) as e:
                            # íŒŒì¼ ì ‘ê·¼ ì˜¤ë¥˜ ì‹œ ìŠ¤í‚µ
                            self.logger.debug(f"ìŠ¤í‚µ: íŒŒì¼ ì ‘ê·¼ ì˜¤ë¥˜ {img_file.name}: {e}")
                            skipped_count += 1
                            continue
        
        self.logger.info(f"YOLO ë°ì´í„°ì…‹ ì¤€ë¹„: {linked_count}ê°œ ì´ë¯¸ì§€-ë¼ë²¨ ìŒ ë§í¬ ìƒì„±")
        if skipped_count > 0:
            self.logger.info(f"ì†ìƒë˜ê±°ë‚˜ ë¬¸ì œìˆëŠ” íŒŒì¼ {skipped_count}ê°œ ìŠ¤í‚µë¨")
        
        # YOLO ë°ì´í„°ì…‹ ì„¤ì •
        config = {
            'path': str(yolo_dataset_root),  # YOLO ë°ì´í„°ì…‹ ë£¨íŠ¸
            'train': 'images',  # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ (ìƒëŒ€ ê²½ë¡œ)
            'val': 'images',    # ê²€ì¦ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ (ê°™ì€ ê²½ë¡œ ì‚¬ìš©)
            'names': {0: 'pill'},  # í´ë˜ìŠ¤ ì´ë¦„
            'nc': 1  # í´ë˜ìŠ¤ ê°œìˆ˜
        }
        
        # YAML íŒŒì¼ ìƒì„±
        with open(config_path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        
        self.logger.info(f"YOLO ë°ì´í„°ì…‹ ì„¤ì • ìƒì„±: {config_path}")
        self.logger.info(f"  - ë°ì´í„°ì…‹ ê²½ë¡œ: {yolo_dataset_root}")
        self.logger.info(f"  - ì´ë¯¸ì§€: {len(list(yolo_images_dir.glob('*.png')))}ê°œ")
        self.logger.info(f"  - ë¼ë²¨: {len(list(yolo_labels_dir.glob('*.txt')))}ê°œ")
        
        return config_path
    
    def validate_models(self) -> Dict[str, float]:
        """ëª¨ë¸ ê²€ì¦ - Classificationë§Œ ì‹¤ì œ í‰ê°€, Detectionì€ train_detection_epochì˜ ì‹¤ì¸¡ì¹˜ ì‚¬ìš©"""
        
        results = {}
        
        # Classification ê²€ì¦ (ì‹¤ì œ í‰ê°€)
        self.classifier.eval()
        correct = 0
        correct_top5 = 0
        total = 0
        
        # DataLoader ìºì‹±: ì²« ë²ˆì§¸ validationì—ì„œë§Œ ìƒì„±
        if self.val_loader_cache is None:
            self.val_loader_cache = self.classification_dataloader.get_val_loader()
            self.logger.info("Validation DataLoader ìºì‹œ ìƒì„± ì™„ë£Œ")
        
        val_loader = self.val_loader_cache
        
        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(self.device, non_blocking=True)
                labels = labels.to(self.device, non_blocking=True)
                
                if self.training_config.channels_last:
                    images = images.to(memory_format=torch.channels_last)
                
                with torch.amp.autocast('cuda', enabled=self.training_config.mixed_precision):
                    outputs = self.classifier(images)
                
                # Top-1 accuracy
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
                
                # Top-5 accuracy
                _, pred_top5 = outputs.topk(5, 1, True, True)
                pred_top5 = pred_top5.t()
                correct_expanded = labels.view(1, -1).expand_as(pred_top5)
                correct_top5 += pred_top5.eq(correct_expanded).sum().item()
        
        results['val_classification_accuracy'] = correct / total
        results['val_classification_top5_accuracy'] = correct_top5 / total
        
        # ê²€ì¦ ê²°ê³¼ ì¦‰ì‹œ ë¡œê¹…
        self.logger.info(f"ğŸ“Š Validation Results:")
        self.logger.info(f"  - Classification Accuracy: {results['val_classification_accuracy']:.4f} ({correct}/{total})")
        self.logger.info(f"  - Top-5 Accuracy: {results['val_classification_top5_accuracy']:.4f}")
        
        # Detection ë©”íŠ¸ë¦­ì€ train_detection_epoch()ì˜ ì‹¤ì¸¡ì¹˜ ì‚¬ìš©
        # ê°€ì¥ ìµœê·¼ detection ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸°
        if hasattr(self, 'last_detection_map'):
            results['val_detection_map'] = self.last_detection_map
        else:
            # ì´ˆê¸°ê°’ ë˜ëŠ” ì´ì „ ì²´í¬í¬ì¸íŠ¸ ê°’ ì‚¬ìš©
            results['val_detection_map'] = getattr(self, 'best_detection_map', 0.0)
        
        self.logger.info(f"  - Detection mAP@0.5: {results['val_detection_map']:.4f} (from training)")
        
        return results
    
    def train(self, start_epoch: int = 0) -> Dict[str, Any]:
        """Two-Stage êµì°¨ í•™ìŠµ ì‹¤í–‰"""
        
        self.logger.info("Two-Stage Pipeline í•™ìŠµ ì‹œì‘")
        self.current_epoch = start_epoch
        
        # ë°ì´í„° ë° ëª¨ë¸ ì„¤ì •
        self.setup_data_loaders()
        self.setup_models()
        
        # ì˜µí‹°ë§ˆì´ì €ë¥¼ selfì— ì €ì¥í•˜ì—¬ ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œ ê°€ëŠ¥í•˜ê²Œ í•¨
        self.optimizer_cls, self.optimizer_det = self.setup_optimizers()
        
        # Patience counter ì´ˆê¸°í™”
        self.cls_patience_counter = 0
        self.det_patience_counter = 0
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ëª¨ë¸ ì´ˆê¸°í™” í›„ì— ì‹¤í–‰)
        if hasattr(self, '_resume_checkpoint_path') and self._resume_checkpoint_path:
            start_epoch, _ = self.load_checkpoint(self._resume_checkpoint_path)
            self.logger.info(f"ì²´í¬í¬ì¸íŠ¸ì—ì„œ resume: epoch {start_epoch}")
        
        scaler = GradScaler(enabled=self.training_config.mixed_precision)
        
        start_time = time.time()
        
        for epoch in range(start_epoch + 1, self.training_config.max_epochs + 1):
            self.current_epoch = epoch
            epoch_start = time.time()
            
            epoch_results = {'epoch': epoch}
            
            # êµì°¨ í•™ìŠµ: Classification â†’ Detection
            if self.training_config.interleaved_training:
                
                # Classification í•™ìŠµ
                for _ in range(self.training_config.classifier_epochs_per_cycle):
                    cls_results = self.train_classification_epoch(
                        self.optimizer_cls, scaler, epoch
                    )
                    epoch_results.update(cls_results)
                
                # Detection í•™ìŠµ  
                for _ in range(self.training_config.detector_epochs_per_cycle):
                    det_results = self.train_detection_epoch(
                        self.optimizer_det, epoch
                    )
                    epoch_results.update(det_results)
            
            # ê²€ì¦
            val_results = self.validate_models()
            epoch_results.update(val_results)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ step
            if hasattr(self, 'scheduler_cls'):
                self.scheduler_cls.step()
                current_lr_cls = self.scheduler_cls.get_last_lr()[0]
                self.logger.info(f"ğŸ“ˆ Classifier LR updated: {current_lr_cls:.2e}")
                
            if hasattr(self, 'scheduler_det'):
                self.scheduler_det.step()
                current_lr_det = self.scheduler_det.get_last_lr()[0]
                self.logger.info(f"ğŸ“ˆ Detector LR updated: {current_lr_det:.2e}")
            
            # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸ (epsilon ê¸°ì¤€ ì ìš©)
            BEST_EPS = 0.001  # 0.1% ì´ìƒ ê°œì„  ì‹œ ì €ì¥
            
            # Classification best ì—…ë°ì´íŠ¸
            if val_results['val_classification_accuracy'] > self.best_classification_accuracy + BEST_EPS:
                self.best_classification_accuracy = val_results['val_classification_accuracy']
                self.best_classification_top5_accuracy = val_results['val_classification_top5_accuracy']
                self.save_checkpoint('classification', 'best')
                self.logger.info(f"âœ… NEW BEST Classification: {self.best_classification_accuracy:.4f}")
                self.cls_patience_counter = 0  # Patience ì´ˆê¸°í™”
            else:
                self.cls_patience_counter += 1
            
            # Detection best ì—…ë°ì´íŠ¸
            if val_results['val_detection_map'] > self.best_detection_map + BEST_EPS:
                self.best_detection_map = val_results['val_detection_map']
                self.save_checkpoint('detection', 'best')
                self.logger.info(f"âœ… NEW BEST Detection mAP: {self.best_detection_map:.4f}")
                self.det_patience_counter = 0  # Patience ì´ˆê¸°í™”
            else:
                self.det_patience_counter += 1
            
            # ë§¤ epochë§ˆë‹¤ last ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            self.save_checkpoint('classification', 'last')
            self.save_checkpoint('detection', 'last')
            self.logger.info(f"ğŸ’¾ Saved last checkpoints - Cls: {val_results['val_classification_accuracy']:.4f}, Det: {val_results['val_detection_map']:.4f}")
            
            # Patience ê¸°ë°˜ ì£¼ê¸°ì  ì €ì¥ (5 epochs ê°œì„  ì—†ìœ¼ë©´)
            PATIENCE_THRESHOLD = 5
            if self.cls_patience_counter >= PATIENCE_THRESHOLD:
                self.save_checkpoint('classification', 'periodic')
                self.logger.info(f"ğŸ“¦ Periodic save (patience={self.cls_patience_counter}) for Classification")
                self.cls_patience_counter = 0  # Reset after periodic save
                
            if self.det_patience_counter >= PATIENCE_THRESHOLD:
                self.save_checkpoint('detection', 'periodic')
                self.logger.info(f"ğŸ“¦ Periodic save (patience={self.det_patience_counter}) for Detection")
                self.det_patience_counter = 0  # Reset after periodic save
            
            # ìˆ˜ë™ ì €ì¥ íŠ¸ë¦¬ê±° í™•ì¸
            from pathlib import Path
            save_flag_path = Path("artifacts/flags/save_now")
            if save_flag_path.exists():
                self.save_checkpoint('classification', 'manual')
                self.save_checkpoint('detection', 'manual')
                save_flag_path.unlink()
                self.logger.info("ğŸ’¾ Manual checkpoint saved due to save_now flag")
            
            # TensorBoard ì—í¬í¬ ë¡œê¹…
            if hasattr(self, 'log_tb_classification_epoch'):
                self.log_tb_classification_epoch(
                    epoch=epoch,
                    train_loss=epoch_results.get('classification_loss', 0),
                    train_acc=epoch_results.get('classification_accuracy', 0),
                    val_loss=None,  # í˜„ì¬ val loss ì¶”ì  ì•ˆí•¨
                    val_acc=val_results['val_classification_accuracy'],
                    val_top5=val_results['val_classification_top5_accuracy']
                )
            
            if hasattr(self, 'log_tb_detection_epoch'):
                self.log_tb_detection_epoch(
                    epoch=epoch,
                    box_loss=epoch_results.get('detection_box_loss', 0),
                    cls_loss=epoch_results.get('detection_cls_loss', 0),
                    dfl_loss=epoch_results.get('detection_dfl_loss', 0),
                    map50=val_results['val_detection_map'],
                    map50_95=None  # í˜„ì¬ ì¶”ì  ì•ˆí•¨
                )
            
            if hasattr(self, 'log_tb_system_metrics'):
                self.log_tb_system_metrics(epoch)
            
            # ë¡œê·¸ ì¶œë ¥
            epoch_time = time.time() - epoch_start
            self.logger.info(
                f"Epoch {epoch:2d} | "
                f"Cls Acc: {val_results['val_classification_accuracy']:.3f} | "
                f"Top5 Acc: {val_results['val_classification_top5_accuracy']:.3f} | "
                f"Det mAP: {val_results['val_detection_map']:.3f} | "
                f"Time: {epoch_time:.1f}s"
            )
            # ëª¨ë‹ˆí„°ë§ íŒŒì„œìš© í‘œì¤€ íƒœê·¸ ë¡œê·¸
            self.logger.info(
                f"CLS_SUMMARY | epoch={epoch} | top1={val_results['val_classification_accuracy']:.4f} | "
                f"top5={val_results['val_classification_top5_accuracy']:.4f}"
            )
            self.logger.info(
                f"DET_SUMMARY | epoch={epoch} | map50={val_results['val_detection_map']:.4f}"
            )
            
            # ëª©í‘œ ë‹¬ì„± ì²´í¬
            if (val_results['val_classification_accuracy'] >= self.training_config.target_classification_accuracy and 
                val_results['val_detection_map'] >= self.training_config.target_detection_map):
                self.logger.info("ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„±! í•™ìŠµ ì¡°ê¸° ì¢…ë£Œ")
                break
            
            self.training_history.append(epoch_results)
        
        total_time = time.time() - start_time
        
        # ìµœì¢… ê²°ê³¼
        final_results = {
            'training_completed': True,
            'total_training_time_minutes': total_time / 60,
            'best_classification_accuracy': self.best_classification_accuracy,
            'best_classification_top5_accuracy': self.best_classification_top5_accuracy,
            'best_detection_map': self.best_detection_map,
            'epochs_completed': getattr(self, 'current_epoch', start_epoch),
            'target_achieved': {
                'classification': self.best_classification_accuracy >= self.training_config.target_classification_accuracy,
                'detection': self.best_detection_map >= self.training_config.target_detection_map
            }
        }
        
        self.logger.info("Two-Stage Pipeline í•™ìŠµ ì™„ë£Œ")
        self.logger.info(f"ìµœê³  Classification ì •í™•ë„: {self.best_classification_accuracy:.3f}")
        self.logger.info(f"ìµœê³  Classification Top-5 ì •í™•ë„: {self.best_classification_top5_accuracy:.3f}")
        self.logger.info(f"ìµœê³  Detection mAP: {self.best_detection_map:.3f}")
        self.logger.info(f"ì´ í•™ìŠµ ì‹œê°„: {total_time/60:.1f}ë¶„")
        
        return final_results
    
    def save_checkpoint(self, model_type: str, checkpoint_type: str, force_save: bool = False) -> None:
        """
        ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ê°œì„ ëœ ì •ì±…)
        
        Args:
            model_type: 'classification' or 'detection'
            checkpoint_type: 'best', 'last', 'manual', 'periodic'
            force_save: ê°•ì œ ì €ì¥ ì—¬ë¶€
        """
        
        try:
            checkpoint_dir = Path("artifacts/stage3/checkpoints")
            checkpoint_dir.mkdir(parents=True, exist_ok=True)
            
            # ì£¼ê¸°ì  ì €ì¥ ë””ë ‰í† ë¦¬ (epochë³„ ë°±ì—…)
            if checkpoint_type == 'periodic':
                periodic_dir = checkpoint_dir / "periodic"
                periodic_dir.mkdir(exist_ok=True)
            
            if model_type == 'classification':
                if checkpoint_type == 'periodic':
                    checkpoint_path = periodic_dir / f"stage3_classification_epoch_{self.current_epoch:03d}.pt"
                else:
                    checkpoint_path = checkpoint_dir / f"stage3_classification_{checkpoint_type}.pt"
                
                # ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
                checkpoint_data = {
                    'model_state_dict': self.classifier.state_dict(),
                    'optimizer_state_dict': self.optimizer_cls.state_dict() if hasattr(self, 'optimizer_cls') else None,
                    'scheduler_state_dict': self.scheduler_cls.state_dict() if hasattr(self, 'scheduler_cls') else None,
                    'accuracy': self.best_classification_accuracy,
                    'top5_accuracy': self.best_classification_top5_accuracy,
                    'epoch': getattr(self, 'current_epoch', 0),
                    'training_history': getattr(self, 'training_history', {}),
                    'config': self.training_config,
                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                }
                
                torch.save(checkpoint_data, checkpoint_path)
                
            elif model_type == 'detection':
                if checkpoint_type == 'periodic':
                    checkpoint_path = periodic_dir / f"stage3_detection_epoch_{self.current_epoch:03d}.pt"
                else:
                    checkpoint_path = checkpoint_dir / f"stage3_detection_{checkpoint_type}.pt"
                
                # YOLO ëª¨ë¸ ì €ì¥ (Ultralytics ë°©ì‹)
                if hasattr(self.detector, 'model') and hasattr(self.detector.model, 'save'):
                    self.detector.model.save(str(checkpoint_path))
                elif hasattr(self.detector, 'export'):
                    self.detector.export(format='torchscript', file=str(checkpoint_path))
                else:
                    # ëŒ€ì²´ ë°©ë²•: ëª¨ë¸ state_dict ì €ì¥
                    checkpoint_data = {
                        'model_state_dict': self.detector.state_dict() if hasattr(self.detector, 'state_dict') else None,
                        'optimizer_state_dict': self.optimizer_det.state_dict() if hasattr(self, 'optimizer_det') else None,
                        'scheduler_state_dict': self.scheduler_det.state_dict() if hasattr(self, 'scheduler_det') else None,
                        'detection_map': self.best_detection_map,
                        'epoch': getattr(self, 'current_epoch', 0),
                        'training_history': getattr(self, 'detection_history', {}),
                        'config': self.training_config,
                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                    }
                    torch.save(checkpoint_data, checkpoint_path)
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size_mb = checkpoint_path.stat().st_size / (1024 * 1024)
            self.logger.info(f"ğŸ’¾ CHECKPOINT SAVED: {model_type} {checkpoint_type} â†’ {checkpoint_path} ({file_size_mb:.1f}MB)")
            
            # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ (ìµœëŒ€ 5ê°œ ìœ ì§€)
            if checkpoint_type == 'periodic':
                self._cleanup_old_periodic_checkpoints(periodic_dir, model_type, max_keep=5)
            
        except Exception as e:
            self.logger.warning(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _cleanup_old_periodic_checkpoints(self, periodic_dir: Path, model_type: str, max_keep: int = 5):
        """ì˜¤ë˜ëœ ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬"""
        pattern = f"stage3_{model_type}_epoch_*.pt"
        checkpoints = sorted(periodic_dir.glob(pattern))
        
        if len(checkpoints) > max_keep:
            for old_checkpoint in checkpoints[:-max_keep]:
                old_checkpoint.unlink()
                self.logger.info(f"ğŸ—‘ï¸ Old checkpoint removed: {old_checkpoint.name}")
    
    def load_checkpoint(self, checkpoint_path: str) -> Tuple[int, float]:
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        try:
            if not Path(checkpoint_path).exists():
                raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
            
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # Classification model checkpoint
            if 'classification' in checkpoint_path:
                self.classifier.load_state_dict(checkpoint['model_state_dict'])
                if hasattr(self, 'optimizer_cls') and 'optimizer_state_dict' in checkpoint:
                    self.optimizer_cls.load_state_dict(checkpoint['optimizer_state_dict'])
                
                accuracy = checkpoint.get('accuracy', 0.0)
                epoch = checkpoint.get('epoch', 0)
                
                self.logger.info(f"Classification checkpoint loaded: {checkpoint_path}")
                self.logger.info(f"Resumed from epoch {epoch}, best accuracy: {accuracy:.3f}")
                return epoch, accuracy
                
            # Detection model checkpoint 
            elif 'detection' in checkpoint_path:
                if 'model_state_dict' in checkpoint and checkpoint['model_state_dict'] is not None:
                    self.detector.load_state_dict(checkpoint['model_state_dict'])
                    
                detection_map = checkpoint.get('detection_map', 0.0)
                epoch = checkpoint.get('epoch', 0)
                
                self.logger.info(f"Detection checkpoint loaded: {checkpoint_path}")
                self.logger.info(f"Resumed from epoch {epoch}, best mAP: {detection_map:.3f}")
                return epoch, detection_map
            
            return 0, 0.0
            
        except Exception as e:
            self.logger.error(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return 0, 0.0


# TensorBoard í†µí•© íŒ¨ì¹˜ ì ìš© (í´ë˜ìŠ¤ ì •ì˜ í›„, 1íšŒë§Œ)
patch_trainer_with_tensorboard(Stage3TwoStageTrainer)


def main():
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜ - ë©€í‹°í”„ë¡œì„¸ì‹± ì›Œì»¤ì—ì„œ ì‹¤í–‰ë˜ì§€ ì•Šë„ë¡ ë³´í˜¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Stage 3 Two-Stage Pipeline Training")
    parser.add_argument("--config", default="config.yaml", help="Config file path")
    parser.add_argument("--manifest-train", default="artifacts/stage3/manifest_train.csv", help="Train manifest path")
    parser.add_argument("--manifest-val", default="artifacts/stage3/manifest_val.csv", help="Val manifest path")
    parser.add_argument("--device", default="cuda", help="Device to use")
    parser.add_argument("--epochs", type=int, default=5, help="Number of epochs (default: 5 for smoke test)")
    parser.add_argument("--batch-size", type=int, default=20, help="Classification batch size (default: 20)")
    parser.add_argument("--resume", type=str, help="Resume from checkpoint path")
    parser.add_argument("--lr-classifier", type=float, help="Override classifier learning rate")
    parser.add_argument("--lr-detector", type=float, help="Override detector learning rate")
    parser.add_argument("--reset-best", action="store_true", help="Reset best metrics at start")
    
    args = parser.parse_args()
    
    trainer = Stage3TwoStageTrainer(
        config_path=args.config,
        manifest_train=args.manifest_train,
        manifest_val=args.manifest_val,
        device=args.device,
        resume_checkpoint=args.resume
    )
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
    trainer.training_config.max_epochs = args.epochs
    trainer.training_config.batch_size = args.batch_size
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì˜¤ë²„ë¼ì´ë“œ (optimizer ìƒì„± ì „ì— ì„¤ì •)
    if args.lr_classifier:
        trainer._lr_classifier_override = args.lr_classifier
        trainer.training_config.learning_rate_classifier = args.lr_classifier
        trainer.logger.info(f"Classifier learning rate overridden: {args.lr_classifier}")
    
    if args.lr_detector:
        trainer._lr_detector_override = args.lr_detector
        trainer.training_config.learning_rate_detector = args.lr_detector
        trainer.logger.info(f"Detector learning rate overridden: {args.lr_detector}")
    
    # --reset-best ì˜µì…˜ ì²˜ë¦¬
    if args.reset_best:
        trainer.best_classification_accuracy = 0.0
        trainer.best_classification_top5_accuracy = 0.0
        trainer.best_detection_map = 0.0
        trainer.logger.info("âœ… Reset best metrics due to --reset-best")
    
    print(f"ğŸš€ Stage 3 Two-Stage í•™ìŠµ ì‹œì‘")
    print(f"  ì—í¬í¬: {args.epochs}")
    print(f"  ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    
    results = trainer.train(start_epoch=0)
    print(f"âœ… í•™ìŠµ ì™„ë£Œ - Classification: {results['best_classification_accuracy']:.3f}, Detection: {results['best_detection_map']:.3f}")


if __name__ == "__main__":
    # PyTorch ë©€í‹°í”„ë¡œì„¸ì‹± í™˜ê²½ì—ì„œëŠ” ê¸°ë³¸ fork ë°©ì‹ ì‚¬ìš©
    # spawn ë°©ì‹ì€ DataLoaderì™€ ì¶©ëŒ ê°€ëŠ¥ì„± ìˆìŒ
    main()