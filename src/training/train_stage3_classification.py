#!/usr/bin/env python3
"""
Stage 3 Classification ì „ìš© í•™ìŠµê¸°

Classification ì„±ëŠ¥ ê·¹ëŒ€í™” ì „ëµ:
- EfficientNetV2-L ì‚¬ìš© (Stage 3+ Large ëª¨ë¸)
- Single 95% + Combination 5% ë°ì´í„°
- RTX 5080 ìµœì í™” (Mixed Precision, torch.compile)
- ëª©í‘œ: Classification Accuracy 85%
- Detection í•™ìŠµ ì™„ì „ ìƒëµ
"""

import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.amp import GradScaler, autocast
from pathlib import Path
from typing import Dict, Optional, Tuple, Any
import pandas as pd

from src.models.classifier_efficientnetv2 import PillSnapClassifier, create_pillsnap_classifier
from src.data.dataloader_manifest_training import ManifestDataset, ManifestTrainingDataLoader
from src.training.memory_monitor_gpu_usage import GPUMemoryMonitor  
from src.evaluation.evaluate_classification_metrics import ClassificationMetricsEvaluator
from src.utils.core import PillSnapLogger, load_config


class Stage3ClassificationTrainer:
    """Stage 3 Classification ì „ìš© í•™ìŠµê¸°"""
    
    def __init__(
        self,
        config_path: str = "config.yaml",
        manifest_train: str = "artifacts/stage3/manifest_train.csv",
        manifest_val: str = "artifacts/stage3/manifest_val.csv",
        device: str = "cuda"
    ):
        self.device = torch.device(device)
        self.logger = PillSnapLogger(__name__)
        
        # ì„¤ì • ë¡œë“œ
        self.config = load_config(config_path)
        self.manifest_train = Path(manifest_train)
        self.manifest_val = Path(manifest_val)
        
        # Stage 3 ì„¤ì • í™•ì¸
        self.stage_config = self.config['progressive_validation']['stage_configs']['stage_3']
        if not self.stage_config.get('focus') == 'classification_only':
            self.logger.warning("Stage 3ì´ Classification ì „ìš©ìœ¼ë¡œ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        
        # RTX 5080 ìµœì í™”
        if torch.cuda.is_available():
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            self.logger.info("ğŸš€ RTX 5080 GPU ìµœì í™” í™œì„±í™”")
        
        # ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
        self.memory_monitor = GPUMemoryMonitor()
        self.metrics_evaluator = ClassificationMetricsEvaluator(
            num_classes=self.config['data']['num_classes']
        )
        
        # ì¬í˜„ì„± ì„¤ì •
        self.seed = 42
        torch.manual_seed(self.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(self.seed)
        
        # í•™ìŠµ ìƒíƒœ
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.scaler = None
        self.train_loader = None
        self.val_loader = None
        self.best_accuracy = 0.0
        self.best_macro_f1 = 0.0
        self.training_history = []
        
        self.logger.info("Stage 3 Classification ì „ìš© í•™ìŠµê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ëª©í‘œ ì •í™•ë„: {self.stage_config['target_metrics']['classification_accuracy']}")
        self.logger.info(f"Manifest: train={self.manifest_train}, val={self.manifest_val}")
    
    def setup_data_loaders(self) -> None:
        """Manifest ê¸°ë°˜ ë°ì´í„° ë¡œë” ì„¤ì •"""
        
        # Manifest íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not self.manifest_train.exists():
            raise FileNotFoundError(f"Train manifest ì—†ìŒ: {self.manifest_train}")
        if not self.manifest_val.exists():
            raise FileNotFoundError(f"Val manifest ì—†ìŒ: {self.manifest_val}")
        
        # Train/Val manifest ë¡œë“œ
        train_df = pd.read_csv(self.manifest_train)
        val_df = pd.read_csv(self.manifest_val)
        
        self.logger.info(f"Train ìƒ˜í”Œ: {len(train_df):,}ê°œ")
        self.logger.info(f"Val ìƒ˜í”Œ: {len(val_df):,}ê°œ")
        
        # ë°ì´í„° ë¶„í¬ í™•ì¸
        train_single = (train_df['image_type'] == 'single').sum()
        train_combo = (train_df['image_type'] == 'combination').sum()
        single_ratio = train_single / len(train_df)
        
        self.logger.info(f"Train ë¶„í¬: Single {single_ratio:.1%} ({train_single:,}), Combination {1-single_ratio:.1%} ({train_combo:,})")
        
        # ë°°ì¹˜ í¬ê¸° ì„¤ì • (RTX 5080 ìµœì í™”)
        batch_size = self.config.get('train', {}).get('batch_size', 16)  # ê¸°ë³¸ê°’ 16
        if single_ratio >= 0.9:  # Classification ì¤‘ì‹¬ì¸ ê²½ìš° ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê°€ëŠ¥
            batch_size = min(batch_size + 8, 32)  # ìµœëŒ€ 32
            self.logger.info(f"Classification ì¤‘ì‹¬ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ì¦ê°€: {batch_size}")
        
        # ë°ì´í„°ë¡œë” ìƒì„± (ManifestDataset ì§ì ‘ ì‚¬ìš©)
        import torchvision.transforms as transforms
        from torch.utils.data import DataLoader
        
        # ë³€í™˜ ì •ì˜
        train_transform = transforms.Compose([
            transforms.Resize((self.config['data']['img_size']['classification'], 
                              self.config['data']['img_size']['classification'])),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=15),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        val_transform = transforms.Compose([
            transforms.Resize((self.config['data']['img_size']['classification'], 
                              self.config['data']['img_size']['classification'])),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = ManifestDataset(train_df, transform=train_transform)
        val_dataset = ManifestDataset(val_df, transform=val_transform)
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=self.config['dataloader']['num_workers'],
            pin_memory=True,
            drop_last=True
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=self.config['dataloader']['num_workers'],
            pin_memory=True
        )
        
        self.logger.success("ë°ì´í„° ë¡œë” ì„¤ì • ì™„ë£Œ")
    
    def setup_model_and_optimizers(self) -> None:
        """ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •"""
        
        # EfficientNetV2-L ëª¨ë¸ ìƒì„±
        model_name = self.config.get('classification', {}).get('backbone', 'tf_efficientnetv2_l')
        self.model = create_pillsnap_classifier(
            num_classes=self.config['data']['num_classes'],
            model_name=model_name,  # backbone -> model_nameìœ¼ë¡œ ìˆ˜ì •
            device=str(self.device)
        )
        
        # torch.compile ì ìš© (RTX 5080 ìµœì í™”)
        try:
            self.model = torch.compile(self.model, mode="reduce-overhead")
            self.logger.info("torch.compile ì ìš© ì„±ê³µ")
        except Exception as e:
            self.logger.warning(f"torch.compile ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: {e}")
        
        # ì˜µí‹°ë§ˆì´ì €
        train_config = self.config.get('train', {})
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=train_config.get('learning_rate', 1e-4),
            weight_decay=train_config.get('weight_decay', 1e-5),
            fused=True  # RTX 5080 ìµœì í™”
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        steps_per_epoch = len(self.train_loader)
        total_steps = steps_per_epoch * train_config.get('epochs', 50)
        
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=total_steps
        )
        
        # Mixed Precision
        self.scaler = GradScaler()
        
        # ì†ì‹¤ í•¨ìˆ˜
        loss_config = self.config.get('loss', {})
        self.criterion = nn.CrossEntropyLoss(
            label_smoothing=loss_config.get('label_smoothing', 0.0)
        )
        
        self.logger.success("ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì™„ë£Œ")
        model_name = self.config.get('classification', {}).get('backbone', 'unknown')
        self.logger.info(f"ëª¨ë¸: {model_name}")
        self.logger.info(f"íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in self.model.parameters()):,}")
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        start_time = time.time()
        
        for batch_idx, (images, labels) in enumerate(self.train_loader):
            images = images.to(self.device, memory_format=torch.channels_last, non_blocking=True)
            labels = labels.to(self.device, non_blocking=True)
            
            self.optimizer.zero_grad()
            
            # Mixed Precision í•™ìŠµ
            with autocast(device_type='cuda'):
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
            
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.scheduler.step()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # ì§„í–‰ë¥  ë¡œê·¸
            if (batch_idx + 1) % 50 == 0:
                accuracy = 100.0 * correct / total
                self.logger.info(
                    f"Epoch {epoch+1} [{batch_idx+1}/{len(self.train_loader)}] "
                    f"Loss: {loss.item():.4f}, Acc: {accuracy:.2f}%, "
                    f"LR: {self.scheduler.get_last_lr()[0]:.6f}"
                )
        
        # ì—í¬í¬ í†µê³„
        epoch_time = time.time() - start_time
        avg_loss = total_loss / len(self.train_loader)
        accuracy = 100.0 * correct / total
        
        metrics = {
            'loss': avg_loss,
            'accuracy': accuracy,
            'epoch_time': epoch_time
        }
        
        self.logger.info(
            f"Train Epoch {epoch+1} ì™„ë£Œ: Loss={avg_loss:.4f}, "
            f"Acc={accuracy:.2f}%, Time={epoch_time:.1f}s"
        )
        
        return metrics
    
    def validate_epoch(self, epoch: int) -> Dict[str, float]:
        """í•œ ì—í¬í¬ ê²€ì¦"""
        
        self.model.eval()
        total_loss = 0.0
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for images, labels in self.val_loader:
                images = images.to(self.device, memory_format=torch.channels_last, non_blocking=True)
                labels = labels.to(self.device, non_blocking=True)
                
                with autocast(device_type='cuda'):
                    outputs = self.model(images)
                    loss = self.criterion(outputs, labels)
                
                total_loss += loss.item()
                
                _, predicted = outputs.max(1)
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_loss = total_loss / len(self.val_loader)
        metrics = self.metrics_evaluator.compute_metrics(
            y_true=all_labels,
            y_pred=all_predictions
        )
        
        metrics['loss'] = avg_loss
        
        # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
        if metrics['accuracy'] > self.best_accuracy:
            self.best_accuracy = metrics['accuracy']
        
        if metrics['macro_f1'] > self.best_macro_f1:
            self.best_macro_f1 = metrics['macro_f1']
        
        self.logger.info(
            f"Val Epoch {epoch+1}: Loss={avg_loss:.4f}, "
            f"Acc={metrics['accuracy']:.2f}%, F1={metrics['macro_f1']:.4f}"
        )
        
        return metrics
    
    def save_checkpoint(self, epoch: int, metrics: Dict[str, float], is_best: bool = False) -> None:
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        
        exp_dir = Path(self.config.get('paths', {}).get('exp_dir', 'exp/stage3_classification'))
        ckpt_dir = exp_dir / "checkpoints"
        ckpt_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            'epoch': epoch + 1,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'scaler_state_dict': self.scaler.state_dict(),
            'metrics': metrics,
            'config': self.config
        }
        
        # Last checkpoint
        last_path = ckpt_dir / "stage3_classification_last.pt"
        torch.save(checkpoint, last_path)
        
        # Best checkpoint
        if is_best:
            best_path = ckpt_dir / "stage3_classification_best.pt"
            torch.save(checkpoint, best_path)
            self.logger.success(f"Best ëª¨ë¸ ì €ì¥: {best_path}")
    
    def train(self) -> Dict[str, Any]:
        """ì „ì²´ í•™ìŠµ ì‹¤í–‰"""
        
        self.logger.info("=" * 60)
        self.logger.info("Stage 3 Classification í•™ìŠµ ì‹œì‘")
        self.logger.info("=" * 60)
        
        start_time = time.time()
        
        # 1. ë°ì´í„°ë¡œë” ì„¤ì •
        self.setup_data_loaders()
        
        # 2. ëª¨ë¸ ì„¤ì •
        self.setup_model_and_optimizers()
        
        # 3. í•™ìŠµ ë£¨í”„
        target_accuracy = self.stage_config['target_metrics']['classification_accuracy']
        epochs = self.config['train']['epochs']
        
        for epoch in range(epochs):
            # í•™ìŠµ
            train_metrics = self.train_epoch(epoch)
            
            # ê²€ì¦
            val_metrics = self.validate_epoch(epoch)
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            is_best = val_metrics['accuracy'] >= self.best_accuracy
            self.save_checkpoint(epoch, val_metrics, is_best)
            
            # ì´ë ¥ ì €ì¥
            epoch_history = {
                'epoch': epoch + 1,
                'train_loss': train_metrics['loss'],
                'train_accuracy': train_metrics['accuracy'],
                'val_loss': val_metrics['loss'],
                'val_accuracy': val_metrics['accuracy'],
                'val_macro_f1': val_metrics['macro_f1']
            }
            self.training_history.append(epoch_history)
            
            # ëª©í‘œ ë‹¬ì„± í™•ì¸
            if val_metrics['accuracy'] >= target_accuracy:
                self.logger.success(
                    f"ğŸ¯ ëª©í‘œ ë‹¬ì„±! Accuracy {val_metrics['accuracy']:.1%} >= {target_accuracy:.1%}"
                )
                break
            
            # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
            gpu_memory = self.memory_monitor.get_memory_info()
            if gpu_memory['used_gb'] > 14:
                self.logger.warning(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {gpu_memory['used_gb']:.1f}GB")
        
        # 4. í•™ìŠµ ì™„ë£Œ
        total_time = time.time() - start_time
        
        final_results = {
            'stage': 3,
            'focus': 'classification_only',
            'total_time_hours': total_time / 3600,
            'best_accuracy': self.best_accuracy,
            'best_macro_f1': self.best_macro_f1,
            'target_achieved': self.best_accuracy >= target_accuracy,
            'epochs_completed': len(self.training_history),
            'training_history': self.training_history
        }
        
        self.logger.info("=" * 60)
        self.logger.info("Stage 3 Classification í•™ìŠµ ì™„ë£Œ")
        self.logger.info(f"ìµœê³  ì •í™•ë„: {self.best_accuracy:.2f}%")
        self.logger.info(f"ìµœê³  Macro F1: {self.best_macro_f1:.4f}")
        self.logger.info(f"ëª©í‘œ ë‹¬ì„±: {'âœ…' if final_results['target_achieved'] else 'âŒ'}")
        self.logger.info(f"ì´ í•™ìŠµ ì‹œê°„: {total_time/3600:.1f}ì‹œê°„")
        self.logger.info("=" * 60)
        
        return final_results


def main():
    """CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Stage 3 Classification ì „ìš© í•™ìŠµ")
    parser.add_argument("--config", type=str, default="config.yaml", help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--train-manifest", type=str, default="artifacts/stage3/manifest_train.csv", help="Train manifest ê²½ë¡œ")
    parser.add_argument("--val-manifest", type=str, default="artifacts/stage3/manifest_val.csv", help="Val manifest ê²½ë¡œ")
    parser.add_argument("--device", type=str, default="cuda", help="ë””ë°”ì´ìŠ¤")
    
    args = parser.parse_args()
    
    # í•™ìŠµ ì‹¤í–‰
    trainer = Stage3ClassificationTrainer(
        config_path=args.config,
        manifest_train=args.train_manifest,
        manifest_val=args.val_manifest,
        device=args.device
    )
    
    results = trainer.train()
    
    print("\nğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"  - ìµœê³  ì •í™•ë„: {results['best_accuracy']:.2f}%")
    print(f"  - ìµœê³  Macro F1: {results['best_macro_f1']:.4f}")
    print(f"  - ëª©í‘œ ë‹¬ì„±: {'âœ…' if results['target_achieved'] else 'âŒ'}")
    print(f"  - í•™ìŠµ ì‹œê°„: {results['total_time_hours']:.1f}ì‹œê°„")


if __name__ == "__main__":
    main()