"""
Training State Manager
ν•™μµ μƒνƒ κ΄€λ¦¬ μ‹μ¤ν…

μƒμ—…μ© μμ¤€μ ν•™μµ μƒνƒ κ΄€λ¦¬:
- μ²΄ν¬ν¬μΈνΈ μ €μ¥/λ΅λ“
- ν•™μµ μ¬κ° (Resume Training)
- ν•™μµ νμ¤ν† λ¦¬ μ¶”μ 
- λ°°ν¬μ© λ¨λΈ ν¨ν‚¤μ§•
"""

import os
import time
import json
import shutil
from pathlib import Path
from typing import Dict, Optional, Any, List
from dataclasses import dataclass, asdict

import torch
import torch.nn as nn
from torch.optim import Optimizer
from torch.optim.lr_scheduler import _LRScheduler

from src.utils.core import PillSnapLogger


@dataclass
class TrainingState:
    """ν•™μµ μƒνƒ λ°μ΄ν„° ν΄λμ¤"""
    epoch: int
    best_metric: float
    best_epoch: int
    model_state_dict: Dict[str, Any]
    optimizer_state_dict: Dict[str, Any]
    scheduler_state_dict: Optional[Dict[str, Any]]
    scaler_state_dict: Optional[Dict[str, Any]]
    training_config: Dict[str, Any]
    timestamp: str


@dataclass
class ModelMetadata:
    """λ¨λΈ λ©”νƒ€λ°μ΄ν„°"""
    model_name: str
    stage: int
    num_classes: int
    input_size: tuple
    best_metric_value: float
    best_metric_name: str
    training_samples: int
    validation_samples: int
    training_time_hours: float
    created_timestamp: str


class TrainingStateManager:
    """ν•™μµ μƒνƒ κ΄€λ¦¬μ"""
    
    def __init__(
        self,
        experiment_name: str,
        stage: int = 1,
        checkpoint_dir: str = "artifacts/checkpoints",
        save_every_n_epochs: int = 5,
        keep_last_n_checkpoints: int = 3
    ):
        self.experiment_name = experiment_name
        self.stage = stage
        self.checkpoint_dir = Path(checkpoint_dir)
        self.save_every_n_epochs = save_every_n_epochs
        self.keep_last_n_checkpoints = keep_last_n_checkpoints
        self.logger = PillSnapLogger(__name__)
        
        # μ‹¤ν—λ³„ λ””λ ‰ν† λ¦¬ μƒμ„±
        self.experiment_dir = self.checkpoint_dir / f"stage{stage}" / experiment_name
        self.experiment_dir.mkdir(parents=True, exist_ok=True)
        
        # μƒνƒ μ¶”μ 
        self.training_start_time = None
        self.best_metric = -float('inf')
        self.best_epoch = 0
        self.training_history = []
        
        self.logger.info(f"TrainingStateManager μ΄κΈ°ν™”: {experiment_name} (Stage {stage})")
        self.logger.info(f"μ²΄ν¬ν¬μΈνΈ λ””λ ‰ν† λ¦¬: {self.experiment_dir}")
    
    def save_checkpoint(
        self,
        epoch: int,
        model: nn.Module,
        optimizer: Optimizer,
        scheduler: Optional[_LRScheduler] = None,
        scaler: Optional[Any] = None,
        metric_value: float = 0.0,
        is_best: bool = False,
        training_config: Optional[Dict] = None
    ) -> str:
        """μ²΄ν¬ν¬μΈνΈ μ €μ¥"""
        
        try:
            # μµκ³  μ„±λ¥ μ—…λ°μ΄νΈ (μƒνƒ μƒμ„± μ „μ— μν–‰)
            if metric_value > self.best_metric:
                self.best_metric = metric_value
                self.best_epoch = epoch
                is_best = True
            
            # μƒνƒ μƒμ„±
            state = TrainingState(
                epoch=epoch,
                best_metric=self.best_metric,
                best_epoch=self.best_epoch,
                model_state_dict=model.state_dict(),
                optimizer_state_dict=optimizer.state_dict(),
                scheduler_state_dict=scheduler.state_dict() if scheduler else None,
                scaler_state_dict=scaler.state_dict() if scaler else None,
                training_config=training_config or {},
                timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
            )
            
            # νμΌλ… μƒμ„±
            checkpoint_file = self.experiment_dir / f"checkpoint_epoch_{epoch:04d}.pt"
            best_file = self.experiment_dir / "best_model.pt"
            latest_file = self.experiment_dir / "latest_checkpoint.pt"
            
            # μ²΄ν¬ν¬μΈνΈ μ €μ¥
            torch.save(asdict(state), checkpoint_file)
            torch.save(asdict(state), latest_file)
            
            # μµκ³  μ„±λ¥ λ¨λΈ μ €μ¥
            if is_best:
                torch.save(asdict(state), best_file)
                self.logger.success(f"π† μƒλ΅μ΄ μµκ³  μ„±λ¥ λ¨λΈ μ €μ¥: {metric_value:.4f}")
            
            # λ©”νƒ€λ°μ΄ν„° μ €μ¥
            self._save_metadata(epoch, metric_value)
            
            # μ¤λλ μ²΄ν¬ν¬μΈνΈ μ •λ¦¬
            self._cleanup_old_checkpoints()
            
            self.logger.info(f"μ²΄ν¬ν¬μΈνΈ μ €μ¥: Epoch {epoch}")
            return str(checkpoint_file)
            
        except Exception as e:
            self.logger.error(f"μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ‹¤ν¨: {e}")
            raise
    
    def load_checkpoint(
        self,
        model: nn.Module,
        optimizer: Optional[Optimizer] = None,
        scheduler: Optional[_LRScheduler] = None,
        scaler: Optional[Any] = None,
        checkpoint_path: Optional[str] = None,
        load_best: bool = False
    ) -> Dict[str, Any]:
        """μ²΄ν¬ν¬μΈνΈ λ΅λ“"""
        
        try:
            # μ²΄ν¬ν¬μΈνΈ νμΌ κ²°μ •
            if checkpoint_path:
                ckpt_file = Path(checkpoint_path)
            elif load_best:
                ckpt_file = self.experiment_dir / "best_model.pt"
            else:
                ckpt_file = self.experiment_dir / "latest_checkpoint.pt"
            
            if not ckpt_file.exists():
                raise FileNotFoundError(f"μ²΄ν¬ν¬μΈνΈ νμΌ μ—†μ: {ckpt_file}")
            
            # μ²΄ν¬ν¬μΈνΈ λ΅λ“
            checkpoint = torch.load(ckpt_file, map_location='cpu')
            
            # λ¨λΈ μƒνƒ λ΅λ“
            model.load_state_dict(checkpoint['model_state_dict'])
            
            # μµν‹°λ§μ΄μ € μƒνƒ λ΅λ“
            if optimizer and checkpoint.get('optimizer_state_dict'):
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            # μ¤μΌ€μ¤„λ¬ μƒνƒ λ΅λ“
            if scheduler and checkpoint.get('scheduler_state_dict'):
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            # μ¤μΌ€μΌλ¬ μƒνƒ λ΅λ“
            if scaler and checkpoint.get('scaler_state_dict'):
                scaler.load_state_dict(checkpoint['scaler_state_dict'])
            
            # μƒνƒ λ³µμ›
            self.best_metric = checkpoint.get('best_metric', -float('inf'))
            self.best_epoch = checkpoint.get('best_epoch', 0)
            
            resume_info = {
                'epoch': checkpoint['epoch'],
                'best_metric': checkpoint.get('best_metric', -float('inf')),  # μ²΄ν¬ν¬μΈνΈμ—μ„ μ§μ ‘ κ°€μ Έμ¤κΈ°
                'best_epoch': checkpoint.get('best_epoch', 0),
                'training_config': checkpoint.get('training_config', {}),
                'timestamp': checkpoint.get('timestamp', 'Unknown')
            }
            
            self.logger.success(f"μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ™„λ£: {ckpt_file}")
            self.logger.info(f"  μ—ν¬ν¬: {resume_info['epoch']}")
            self.logger.info(f"  μµκ³  μ„±λ¥: {self.best_metric:.4f}")
            
            return resume_info
            
        except Exception as e:
            self.logger.error(f"μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ‹¤ν¨: {e}")
            raise
    
    def save_final_model_for_deployment(
        self,
        model: nn.Module,
        model_metadata: ModelMetadata,
        include_training_artifacts: bool = False
    ) -> str:
        """λ°°ν¬μ© μµμΆ… λ¨λΈ μ €μ¥"""
        
        try:
            deploy_dir = self.experiment_dir / "deployment"
            deploy_dir.mkdir(exist_ok=True)
            
            # λ¨λΈλ§ μ €μ¥ (μ¶”λ΅ μ©)
            model_file = deploy_dir / "model_inference_only.pt"
            torch.save({
                'model_state_dict': model.state_dict(),
                'metadata': asdict(model_metadata),
                'inference_only': True
            }, model_file)
            
            # λ©”νƒ€λ°μ΄ν„° JSON μ €μ¥
            metadata_file = deploy_dir / "model_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(model_metadata), f, ensure_ascii=False, indent=2)
            
            # ν•™μµ μ•„ν‹°ν©νΈ ν¬ν•¨ λ²„μ „
            if include_training_artifacts:
                full_file = deploy_dir / "model_with_training_artifacts.pt"
                best_model_path = self.experiment_dir / "best_model.pt"
                if best_model_path.exists():
                    best_checkpoint = torch.load(best_model_path)
                    torch.save(best_checkpoint, full_file)
                else:
                    self.logger.warning("best_model.ptκ°€ μ—†μ–΄ ν•™μµ μ•„ν‹°ν©νΈ ν¬ν•¨ λ²„μ „ μƒμ„±μ„ κ±΄λ„λλ‹λ‹¤")
            
            # λ°°ν¬ κ°€μ΄λ“ μƒμ„±
            self._create_deployment_guide(deploy_dir, model_metadata)
            
            self.logger.success(f"λ°°ν¬μ© λ¨λΈ μ €μ¥ μ™„λ£: {deploy_dir}")
            return str(model_file)
            
        except Exception as e:
            self.logger.error(f"λ°°ν¬μ© λ¨λΈ μ €μ¥ μ‹¤ν¨: {e}")
            raise
    
    def get_training_summary(self) -> Dict[str, Any]:
        """ν•™μµ μ”μ•½ μ •λ³΄ λ°ν™"""
        
        try:
            # λ©”νƒ€λ°μ΄ν„° νμΌ ν™•μΈ
            metadata_file = self.experiment_dir / "training_metadata.json"
            
            if metadata_file.exists():
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
            else:
                metadata = {}
            
            # μ²΄ν¬ν¬μΈνΈ νμΌλ“¤ ν™•μΈ
            checkpoint_files = list(self.experiment_dir.glob("checkpoint_*.pt"))
            
            summary = {
                'experiment_name': self.experiment_name,
                'stage': self.stage,
                'best_metric': self.best_metric,
                'best_epoch': self.best_epoch,
                'total_checkpoints': len(checkpoint_files),
                'metadata': metadata,
                'has_best_model': (self.experiment_dir / "best_model.pt").exists(),
                'has_deployment_ready': (self.experiment_dir / "deployment").exists(),
                'experiment_dir': str(self.experiment_dir)
            }
            
            return summary
            
        except Exception as e:
            self.logger.error(f"ν•™μµ μ”μ•½ μƒμ„± μ‹¤ν¨: {e}")
            return {}
    
    def _save_metadata(self, epoch: int, metric_value: float) -> None:
        """λ©”νƒ€λ°μ΄ν„° μ €μ¥"""
        try:
            metadata = {
                'experiment_name': self.experiment_name,
                'stage': self.stage,
                'current_epoch': epoch,
                'best_metric': self.best_metric,
                'best_epoch': self.best_epoch,
                'current_metric': metric_value,
                'last_updated': time.strftime("%Y-%m-%d %H:%M:%S"),
                'total_training_time_hours': self._get_training_time_hours()
            }
            
            metadata_file = self.experiment_dir / "training_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            self.logger.warning(f"λ©”νƒ€λ°μ΄ν„° μ €μ¥ μ‹¤ν¨: {e}")
    
    def _cleanup_old_checkpoints(self) -> None:
        """μ¤λλ μ²΄ν¬ν¬μΈνΈ μ •λ¦¬"""
        try:
            checkpoint_files = sorted(
                self.experiment_dir.glob("checkpoint_*.pt"),
                key=lambda x: x.stat().st_mtime,
                reverse=True
            )
            
            # μµμ‹  Nκ°λ§ μ μ§€
            for old_file in checkpoint_files[self.keep_last_n_checkpoints:]:
                old_file.unlink()
                self.logger.debug(f"μ¤λλ μ²΄ν¬ν¬μΈνΈ μ‚­μ : {old_file.name}")
                
        except Exception as e:
            self.logger.warning(f"μ²΄ν¬ν¬μΈνΈ μ •λ¦¬ μ‹¤ν¨: {e}")
    
    def _get_training_time_hours(self) -> float:
        """ν•™μµ μ‹κ°„ κ³„μ‚°"""
        if self.training_start_time:
            return (time.time() - self.training_start_time) / 3600
        return 0.0
    
    def _create_deployment_guide(self, deploy_dir: Path, metadata: ModelMetadata) -> None:
        """λ°°ν¬ κ°€μ΄λ“ μƒμ„±"""
        try:
            guide_content = f"""# λ¨λΈ λ°°ν¬ κ°€μ΄λ“

## λ¨λΈ μ •λ³΄
- λ¨λΈλ…: {metadata.model_name}
- Stage: {metadata.stage}
- ν΄λμ¤ μ: {metadata.num_classes}
- μ…λ ¥ ν¬κΈ°: {metadata.input_size}
- μµκ³  μ„±λ¥: {metadata.best_metric_value:.4f}

## νμΌ μ„¤λ…
- `model_inference_only.pt`: μ¶”λ΅  μ „μ© λ¨λΈ (κ¶μ¥)
- `model_metadata.json`: λ¨λΈ λ©”νƒ€λ°μ΄ν„°
- `model_with_training_artifacts.pt`: ν•™μµ μ •λ³΄ ν¬ν•¨ (λ””λ²„κΉ…μ©)

## μ‚¬μ© μμ‹
```python
import torch

# λ¨λΈ λ΅λ“
checkpoint = torch.load('model_inference_only.pt')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# μ¶”λ΅ 
with torch.no_grad():
    output = model(input_tensor)
```

## μ”κµ¬μ‚¬ν•­
- PyTorch >= 2.0
- CUDA μ§€μ› (RTX 5080 μµμ ν™”)
- λ©”λ¨λ¦¬: μµμ† 2GB, κ¶μ¥ 4GB

μƒμ„± μ‹κ°„: {time.strftime("%Y-%m-%d %H:%M:%S")}
"""
            
            guide_file = deploy_dir / "deployment_guide.md"
            with open(guide_file, 'w', encoding='utf-8') as f:
                f.write(guide_content)
                
        except Exception as e:
            self.logger.warning(f"λ°°ν¬ κ°€μ΄λ“ μƒμ„± μ‹¤ν¨: {e}")
    
    def start_training_timer(self) -> None:
        """ν•™μµ μ‹μ‘ μ‹κ°„ κΈ°λ΅"""
        self.training_start_time = time.time()
        self.logger.info("ν•™μµ νƒ€μ΄λ¨Έ μ‹μ‘")


def main():
    """TrainingStateManager ν…μ¤νΈ"""
    print("π”§ Training State Manager Test")
    print("=" * 50)
    
    try:
        # λ”λ―Έ λ¨λΈ λ° μµν‹°λ§μ΄μ €
        model = torch.nn.Linear(10, 5)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        
        # μƒνƒ κ΄€λ¦¬μ μƒμ„±
        state_manager = TrainingStateManager(
            experiment_name="test_experiment",
            stage=1,
            checkpoint_dir="test_checkpoints"
        )
        
        state_manager.start_training_timer()
        
        # μ²΄ν¬ν¬μΈνΈ μ €μ¥ ν…μ¤νΈ
        checkpoint_path = state_manager.save_checkpoint(
            epoch=10,
            model=model,
            optimizer=optimizer,
            metric_value=0.85,
            is_best=True,
            training_config={'lr': 0.001, 'batch_size': 32}
        )
        
        print(f"β… μ²΄ν¬ν¬μΈνΈ μ €μ¥: {checkpoint_path}")
        
        # μ²΄ν¬ν¬μΈνΈ λ΅λ“ ν…μ¤νΈ
        resume_info = state_manager.load_checkpoint(model, optimizer, load_best=True)
        print(f"β… μ²΄ν¬ν¬μΈνΈ λ΅λ“: Epoch {resume_info['epoch']}")
        
        # λ°°ν¬μ© λ¨λΈ μ €μ¥
        metadata = ModelMetadata(
            model_name="test_model",
            stage=1,
            num_classes=5,
            input_size=(10,),
            best_metric_value=0.85,
            best_metric_name="accuracy",
            training_samples=1000,
            validation_samples=200,
            training_time_hours=0.1,
            created_timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
        )
        
        deploy_path = state_manager.save_final_model_for_deployment(model, metadata)
        print(f"β… λ°°ν¬μ© λ¨λΈ μ €μ¥: {deploy_path}")
        
        # ν•™μµ μ”μ•½
        summary = state_manager.get_training_summary()
        print(f"β… ν•™μµ μ”μ•½: {summary['total_checkpoints']}κ° μ²΄ν¬ν¬μΈνΈ")
        
        # μ •λ¦¬
        shutil.rmtree("test_checkpoints", ignore_errors=True)
        print("β… ν…μ¤νΈ μ™„λ£")
        
    except Exception as e:
        print(f"β ν…μ¤νΈ μ‹¤ν¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()