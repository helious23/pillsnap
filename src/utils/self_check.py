#!/usr/bin/env python3
"""
PillSnap ML Self-Check System
ì‹œì‘ ì‹œ í™˜ê²½ ë° ì„¤ì • ê²€ì¦ ì‹œìŠ¤í…œ
"""

import os
import sys
import torch
import json
import yaml
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime, timezone, timedelta

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.utils.core import config_provider, KST


class SelfCheckSystem:
    """
    í•™ìŠµ ì‹œì‘ ì „ í™˜ê²½ ë° ì„¤ì • ê²€ì¦
    - GPU ë©”ëª¨ë¦¬ ì¶©ë¶„ì„± í™•ì¸
    - ì˜ì¡´ì„± ë²„ì „ ì²´í¬
    - Manifest íŒŒì¼ ì¡´ì¬ ê²€ì¦
    - ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
    - ì„¤ì • ì¼ê´€ì„± ê²€ì¦
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (Noneì´ë©´ ConfigProviderì—ì„œ ë¡œë“œ)
        """
        if config is None:
            config = config_provider.get_config()
        
        self.config = config
        self.check_results = []
        self.has_errors = False
        self.has_warnings = False
        
    def run_all_checks(self) -> bool:
        """
        ëª¨ë“  ì²´í¬ ì‹¤í–‰
        
        Returns:
            bool: ëª¨ë“  ì²´í¬ í†µê³¼ ì—¬ë¶€
        """
        print("=" * 60)
        print("ğŸ” PillSnap ML Self-Check System")
        print("=" * 60)
        
        # 1. GPU ì²´í¬
        self._check_gpu()
        
        # 2. ì˜ì¡´ì„± ì²´í¬
        self._check_dependencies()
        
        # 3. Manifest íŒŒì¼ ì²´í¬
        self._check_manifest_files()
        
        # 4. ë””ìŠ¤í¬ ê³µê°„ ì²´í¬
        self._check_disk_space()
        
        # 5. ì„¤ì • ì¼ê´€ì„± ì²´í¬
        self._check_config_consistency()
        
        # 6. ì²´í¬í¬ì¸íŠ¸ ì²´í¬
        self._check_checkpoints()
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_results()
        
        return not self.has_errors
    
    def _check_gpu(self) -> None:
        """GPU ì²´í¬"""
        try:
            if not torch.cuda.is_available():
                self._add_error("GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return
            
            device_count = torch.cuda.device_count()
            self._add_success(f"GPU {device_count}ê°œ ê°ì§€ë¨")
            
            for i in range(device_count):
                props = torch.cuda.get_device_properties(i)
                total_memory = props.total_memory / (1024**3)
                
                # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë©”ëª¨ë¦¬
                allocated = torch.cuda.memory_allocated(i) / (1024**3)
                reserved = torch.cuda.memory_reserved(i) / (1024**3)
                
                self._add_info(f"  GPU {i}: {props.name}")
                self._add_info(f"    - ì´ ë©”ëª¨ë¦¬: {total_memory:.1f}GB")
                self._add_info(f"    - í• ë‹¹ëœ ë©”ëª¨ë¦¬: {allocated:.1f}GB")
                self._add_info(f"    - ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: {reserved:.1f}GB")
                
                # ë©”ëª¨ë¦¬ ì¶©ë¶„ì„± ì²´í¬ (ë°°ì¹˜ í¬ê¸° ê¸°ì¤€)
                batch_size = self.config.get('train', {}).get('batch_size', 8)
                required_memory = batch_size * 1.5  # ëŒ€ëµì ì¸ ì¶”ì • (GB)
                
                if total_memory - reserved < required_memory:
                    self._add_warning(f"GPU {i} ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥ì„± (í•„ìš”: {required_memory:.1f}GB)")
                    
        except Exception as e:
            self._add_error(f"GPU ì²´í¬ ì‹¤íŒ¨: {e}")
    
    def _check_dependencies(self) -> None:
        """ì˜ì¡´ì„± ë²„ì „ ì²´í¬"""
        try:
            import torch
            import torchvision
            import timm
            import ultralytics
            
            self._add_success("í•µì‹¬ ì˜ì¡´ì„± ì²´í¬ ì™„ë£Œ")
            self._add_info(f"  - PyTorch: {torch.__version__}")
            self._add_info(f"  - TorchVision: {torchvision.__version__}")
            self._add_info(f"  - Timm: {timm.__version__}")
            self._add_info(f"  - Ultralytics: {ultralytics.__version__}")
            
            # CUDA ë²„ì „ ì²´í¬
            if torch.cuda.is_available():
                self._add_info(f"  - CUDA: {torch.version.cuda}")
                self._add_info(f"  - cuDNN: {torch.backends.cudnn.version()}")
                
        except ImportError as e:
            self._add_error(f"ì˜ì¡´ì„± ëˆ„ë½: {e}")
    
    def _check_manifest_files(self) -> None:
        """Manifest íŒŒì¼ ì¡´ì¬ ë° ìœ íš¨ì„± ì²´í¬"""
        stage = self.config.get('stage', 3)
        
        manifest_paths = {
            'train': f"/home/max16/pillsnap/artifacts/stage{stage}/manifest_train.csv",
            'val': f"/home/max16/pillsnap/artifacts/stage{stage}/manifest_val.csv"
        }
        
        for split, path in manifest_paths.items():
            path_obj = Path(path)
            
            if not path_obj.exists():
                self._add_error(f"Manifest íŒŒì¼ ì—†ìŒ: {path}")
                continue
            
            # íŒŒì¼ í¬ê¸° ì²´í¬
            size_mb = path_obj.stat().st_size / (1024**2)
            
            # ë¼ì¸ ìˆ˜ ì²´í¬ (ìƒ˜í”Œ ìˆ˜)
            try:
                with open(path, 'r') as f:
                    line_count = sum(1 for _ in f) - 1  # í—¤ë” ì œì™¸
                
                self._add_success(f"Manifest {split}: {line_count:,}ê°œ ìƒ˜í”Œ ({size_mb:.1f}MB)")
                
                # Stageë³„ ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ì²´í¬
                min_samples = {1: 4000, 2: 20000, 3: 80000, 4: 400000}
                if stage in min_samples and line_count < min_samples[stage]:
                    self._add_warning(f"Stage {stage} {split} ìƒ˜í”Œ ìˆ˜ ë¶€ì¡±: {line_count:,} < {min_samples[stage]:,}")
                    
            except Exception as e:
                self._add_error(f"Manifest íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    def _check_disk_space(self) -> None:
        """ë””ìŠ¤í¬ ê³µê°„ ì²´í¬"""
        try:
            import shutil
            
            paths_to_check = [
                ("/home/max16/pillsnap", "ì½”ë“œë² ì´ìŠ¤"),
                ("/home/max16/pillsnap_data", "ë°ì´í„°ì…‹"),
                ("/tmp", "ì„ì‹œ íŒŒì¼")
            ]
            
            for path, name in paths_to_check:
                if os.path.exists(path):
                    total, used, free = shutil.disk_usage(path)
                    free_gb = free / (1024**3)
                    used_percent = (used / total) * 100
                    
                    self._add_info(f"  {name} ({path}):")
                    self._add_info(f"    - ì—¬ìœ  ê³µê°„: {free_gb:.1f}GB ({100-used_percent:.1f}%)")
                    
                    # ê²½ê³  ì„ê³„ê°’
                    if free_gb < 10:
                        self._add_warning(f"{name} ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±: {free_gb:.1f}GB")
                    elif free_gb < 50:
                        self._add_info(f"    âš ï¸  ë””ìŠ¤í¬ ê³µê°„ ì£¼ì˜ í•„ìš”")
                        
        except Exception as e:
            self._add_warning(f"ë””ìŠ¤í¬ ê³µê°„ ì²´í¬ ì‹¤íŒ¨: {e}")
    
    def _check_config_consistency(self) -> None:
        """ì„¤ì • ì¼ê´€ì„± ì²´í¬"""
        try:
            # num_classes ì¼ê´€ì„±
            num_classes_config = self.config.get('num_classes', 5000)
            num_classes_model = self.config.get('models', {}).get('classifier', {}).get('num_classes', 5000)
            
            if num_classes_config != num_classes_model:
                self._add_error(f"num_classes ë¶ˆì¼ì¹˜: config={num_classes_config}, model={num_classes_model}")
            else:
                self._add_success(f"num_classes ì¼ê´€ì„± í™•ì¸: {num_classes_config}")
            
            # ë°°ì¹˜ í¬ê¸° vs GPU ë©”ëª¨ë¦¬
            batch_size = self.config.get('train', {}).get('batch_size', 8)
            if batch_size > 16:
                self._add_warning(f"ë°°ì¹˜ í¬ê¸°ê°€ í½ë‹ˆë‹¤: {batch_size} (OOM ìœ„í—˜)")
            
            # Learning rate ë²”ìœ„ ì²´í¬
            lr = self.config.get('train', {}).get('lr', 1e-4)
            if lr > 1e-2:
                self._add_warning(f"Learning rateê°€ ë„ˆë¬´ ë†’ìŠµë‹ˆë‹¤: {lr}")
            elif lr < 1e-6:
                self._add_warning(f"Learning rateê°€ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤: {lr}")
                
        except Exception as e:
            self._add_error(f"ì„¤ì • ì¼ê´€ì„± ì²´í¬ ì‹¤íŒ¨: {e}")
    
    def _check_checkpoints(self) -> None:
        """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì²´í¬"""
        checkpoint_dir = Path("/home/max16/pillsnap_data/exp/exp01/checkpoints")
        
        if not checkpoint_dir.exists():
            self._add_warning("ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ëª©ë¡
        checkpoints = list(checkpoint_dir.glob("*.pt"))
        
        if not checkpoints:
            self._add_info("  ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ (ìƒˆë¡œìš´ í•™ìŠµ)")
        else:
            self._add_success(f"ì²´í¬í¬ì¸íŠ¸ {len(checkpoints)}ê°œ ë°œê²¬")
            
            # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì •ë³´
            latest = max(checkpoints, key=lambda p: p.stat().st_mtime)
            size_mb = latest.stat().st_size / (1024**2)
            
            self._add_info(f"  ìµœì‹ : {latest.name} ({size_mb:.1f}MB)")
            
            # ì²´í¬í¬ì¸íŠ¸ í¬ê¸° ì²´í¬ (ë„ˆë¬´ ì‘ìœ¼ë©´ ë¬¸ì œ)
            if size_mb < 100:
                self._add_warning(f"ì²´í¬í¬ì¸íŠ¸ í¬ê¸°ê°€ ì‘ìŠµë‹ˆë‹¤: {size_mb:.1f}MB (ì†ìƒ ê°€ëŠ¥ì„±)")
    
    def _add_success(self, message: str) -> None:
        """ì„±ê³µ ë©”ì‹œì§€ ì¶”ê°€"""
        self.check_results.append(("SUCCESS", message))
    
    def _add_info(self, message: str) -> None:
        """ì •ë³´ ë©”ì‹œì§€ ì¶”ê°€"""
        self.check_results.append(("INFO", message))
    
    def _add_warning(self, message: str) -> None:
        """ê²½ê³  ë©”ì‹œì§€ ì¶”ê°€"""
        self.check_results.append(("WARNING", message))
        self.has_warnings = True
    
    def _add_error(self, message: str) -> None:
        """ì—ëŸ¬ ë©”ì‹œì§€ ì¶”ê°€"""
        self.check_results.append(("ERROR", message))
        self.has_errors = True
    
    def _print_results(self) -> None:
        """ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ Self-Check ê²°ê³¼")
        print("=" * 60)
        
        # ìƒ‰ìƒ ì½”ë“œ
        colors = {
            "SUCCESS": "\033[92mâœ…",
            "INFO": "\033[94mâ„¹ï¸ ",
            "WARNING": "\033[93mâš ï¸ ",
            "ERROR": "\033[91mâŒ"
        }
        reset = "\033[0m"
        
        for level, message in self.check_results:
            prefix = colors.get(level, "")
            print(f"{prefix} {message}{reset}")
        
        print("=" * 60)
        
        # ìµœì¢… ìƒíƒœ
        if self.has_errors:
            print(f"{colors['ERROR']} Self-Check ì‹¤íŒ¨: ì—ëŸ¬ë¥¼ í•´ê²°í•˜ì„¸ìš”{reset}")
        elif self.has_warnings:
            print(f"{colors['WARNING']} Self-Check ì™„ë£Œ: ê²½ê³  ì‚¬í•­ í™•ì¸ í•„ìš”{reset}")
        else:
            print(f"{colors['SUCCESS']} Self-Check ì™„ë£Œ: ëª¨ë“  ì²´í¬ í†µê³¼{reset}")
        
        print("=" * 60)


def run_self_check(config: Optional[Dict] = None) -> bool:
    """
    Self-check ì‹¤í–‰ í—¬í¼ í•¨ìˆ˜
    
    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Returns:
        bool: ì²´í¬ í†µê³¼ ì—¬ë¶€
    """
    checker = SelfCheckSystem(config)
    return checker.run_all_checks()


if __name__ == "__main__":
    # ë…ë¦½ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
    import argparse
    
    parser = argparse.ArgumentParser(description="PillSnap ML Self-Check System")
    parser.add_argument("--config", type=str, default="config.yaml", help="Config file path")
    parser.add_argument("--stage", type=int, default=3, help="Stage number (1-4)")
    args = parser.parse_args()
    
    # ConfigProvider ì´ˆê¸°í™”
    config_provider.load(args.config)
    
    # Stage ì„¤ì •
    if args.stage:
        config_provider.set("stage", args.stage)
    
    # Self-check ì‹¤í–‰
    success = run_self_check()
    
    # ì¢…ë£Œ ì½”ë“œ
    sys.exit(0 if success else 1)