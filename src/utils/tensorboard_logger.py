#!/usr/bin/env python3
"""
TensorBoard Logger for PillSnap ML Training
ì‹¤ì‹œê°„ í•™ìŠµ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ í†µí•© ë¡œê±°
"""

import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, Union
import torch
from torch.utils.tensorboard import SummaryWriter
import numpy as np


class TensorBoardLogger:
    """TensorBoard í†µí•© ë¡œê±°"""
    
    def __init__(
        self,
        log_dir: str = None,
        experiment_name: str = None,
        comment: str = "",
        flush_secs: int = 30
    ):
        """
        Args:
            log_dir: TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: runs/)
            experiment_name: ì‹¤í—˜ ì´ë¦„
            comment: ì¶”ê°€ ì½”ë©˜íŠ¸
            flush_secs: ë””ìŠ¤í¬ í”ŒëŸ¬ì‹œ ì£¼ê¸° (ì´ˆ)
        """
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        if log_dir is None:
            log_dir = os.environ.get('TENSORBOARD_DIR', 'runs')
        
        # ì‹¤í—˜ ì´ë¦„ ì„¤ì •
        if experiment_name is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            experiment_name = f"stage3_{timestamp}"
        
        # ì „ì²´ ê²½ë¡œ ìƒì„±
        if comment:
            experiment_name = f"{experiment_name}_{comment}"
        
        self.log_dir = Path(log_dir) / experiment_name
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # SummaryWriter ì´ˆê¸°í™”
        self.writer = SummaryWriter(
            log_dir=str(self.log_dir),
            flush_secs=flush_secs
        )
        
        self.global_step = 0
        self.epoch = 0
        print(f"ðŸ“Š TensorBoard ë¡œê±° ì´ˆê¸°í™”: {self.log_dir}")
        print(f"   ì‹¤í–‰: tensorboard --logdir {log_dir}")
    
    def set_epoch(self, epoch: int):
        """í˜„ìž¬ ì—í¬í¬ ì„¤ì •"""
        self.epoch = epoch
    
    def log_scalar(self, tag: str, value: float, step: Optional[int] = None):
        """ìŠ¤ì¹¼ë¼ ê°’ ë¡œê¹…"""
        if step is None:
            step = self.global_step
        self.writer.add_scalar(tag, value, step)
    
    def log_scalars(self, main_tag: str, tag_scalar_dict: Dict[str, float], step: Optional[int] = None):
        """ì—¬ëŸ¬ ìŠ¤ì¹¼ë¼ ê°’ ë™ì‹œ ë¡œê¹…"""
        if step is None:
            step = self.global_step
        self.writer.add_scalars(main_tag, tag_scalar_dict, step)
    
    def log_classification_metrics(
        self,
        loss: float,
        accuracy: float,
        top5_accuracy: Optional[float] = None,
        learning_rate: Optional[float] = None,
        step: Optional[int] = None,
        phase: str = "train"
    ):
        """Classification ë©”íŠ¸ë¦­ ë¡œê¹…"""
        if step is None:
            step = self.global_step
        
        # Lossì™€ Accuracy
        self.writer.add_scalar(f'{phase}/classification/loss', loss, step)
        self.writer.add_scalar(f'{phase}/classification/accuracy', accuracy, step)
        
        # Top-5 Accuracy (ìžˆìœ¼ë©´)
        if top5_accuracy is not None:
            self.writer.add_scalar(f'{phase}/classification/top5_accuracy', top5_accuracy, step)
        
        # Learning Rate (ìžˆìœ¼ë©´)
        if learning_rate is not None:
            self.writer.add_scalar(f'{phase}/learning_rate', learning_rate, step)
        
        # í†µí•© ë©”íŠ¸ë¦­
        metrics = {
            'loss': loss,
            'accuracy': accuracy
        }
        if top5_accuracy is not None:
            metrics['top5_accuracy'] = top5_accuracy
        
        self.writer.add_scalars(f'{phase}/classification/all_metrics', metrics, step)
    
    def log_detection_metrics(
        self,
        box_loss: float,
        cls_loss: float,
        dfl_loss: float,
        map50: float,
        map50_95: Optional[float] = None,
        step: Optional[int] = None,
        phase: str = "train"
    ):
        """Detection ë©”íŠ¸ë¦­ ë¡œê¹…"""
        if step is None:
            step = self.global_step
        
        # ê°œë³„ Loss
        self.writer.add_scalar(f'{phase}/detection/box_loss', box_loss, step)
        self.writer.add_scalar(f'{phase}/detection/cls_loss', cls_loss, step)
        self.writer.add_scalar(f'{phase}/detection/dfl_loss', dfl_loss, step)
        
        # mAP
        self.writer.add_scalar(f'{phase}/detection/mAP50', map50, step)
        if map50_95 is not None:
            self.writer.add_scalar(f'{phase}/detection/mAP50_95', map50_95, step)
        
        # í†µí•© Loss
        total_loss = box_loss + cls_loss + dfl_loss
        self.writer.add_scalar(f'{phase}/detection/total_loss', total_loss, step)
        
        # ëª¨ë“  ë©”íŠ¸ë¦­ í•œë²ˆì—
        metrics = {
            'box_loss': box_loss,
            'cls_loss': cls_loss,
            'dfl_loss': dfl_loss,
            'total_loss': total_loss,
            'mAP50': map50
        }
        if map50_95 is not None:
            metrics['mAP50_95'] = map50_95
        
        self.writer.add_scalars(f'{phase}/detection/all_metrics', metrics, step)
    
    def log_system_metrics(
        self,
        gpu_memory_used: float,
        gpu_memory_peak: float,
        gpu_utilization: Optional[float] = None,
        step: Optional[int] = None
    ):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        if step is None:
            step = self.global_step
        
        self.writer.add_scalar('system/gpu_memory_used_gb', gpu_memory_used, step)
        self.writer.add_scalar('system/gpu_memory_peak_gb', gpu_memory_peak, step)
        
        if gpu_utilization is not None:
            self.writer.add_scalar('system/gpu_utilization', gpu_utilization, step)
    
    def log_batch_metrics(
        self,
        epoch: int,
        batch: int,
        total_batches: int,
        loss: float,
        accuracy: Optional[float] = None,
        learning_rate: Optional[float] = None
    ):
        """ë°°ì¹˜ë³„ ë©”íŠ¸ë¦­ ë¡œê¹… (ë¹ ë¥¸ ì—…ë°ì´íŠ¸)"""
        # Global step ê³„ì‚°
        step = epoch * total_batches + batch
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        self.writer.add_scalar('batch/loss', loss, step)
        
        if accuracy is not None:
            self.writer.add_scalar('batch/accuracy', accuracy, step)
        
        if learning_rate is not None:
            self.writer.add_scalar('batch/learning_rate', learning_rate, step)
        
        # ì§„í–‰ë¥ 
        progress = (batch / total_batches) * 100
        self.writer.add_scalar('batch/progress_percent', progress, step)
    
    def log_histogram(self, tag: str, values: Union[torch.Tensor, np.ndarray], step: Optional[int] = None):
        """ížˆìŠ¤í† ê·¸ëž¨ ë¡œê¹… (ê°€ì¤‘ì¹˜ ë¶„í¬ ë“±)"""
        if step is None:
            step = self.global_step
        self.writer.add_histogram(tag, values, step)
    
    def log_model_graph(self, model: torch.nn.Module, input_sample: torch.Tensor):
        """ëª¨ë¸ êµ¬ì¡° ê·¸ëž˜í”„ ë¡œê¹…"""
        try:
            self.writer.add_graph(model, input_sample)
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ê·¸ëž˜í”„ ë¡œê¹… ì‹¤íŒ¨: {e}")
    
    def log_text(self, tag: str, text: str, step: Optional[int] = None):
        """í…ìŠ¤íŠ¸ ë¡œê¹… (ì„¤ì •, í•˜ì´í¼íŒŒë¼ë¯¸í„° ë“±)"""
        if step is None:
            step = self.global_step
        self.writer.add_text(tag, text, step)
    
    def log_hyperparameters(self, hparams: Dict[str, Any], metrics: Dict[str, float]):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ìµœì¢… ë©”íŠ¸ë¦­ ë¡œê¹…"""
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ í”Œëž«í•œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        flat_hparams = self._flatten_dict(hparams)
        self.writer.add_hparams(flat_hparams, metrics)
    
    def _flatten_dict(self, d: Dict, parent_key: str = '', sep: str = '/') -> Dict:
        """ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ë¥¼ í”Œëž«í•˜ê²Œ ë³€í™˜"""
        items = []
        for k, v in d.items():
            new_key = f"{parent_key}{sep}{k}" if parent_key else k
            if isinstance(v, dict):
                items.extend(self._flatten_dict(v, new_key, sep=sep).items())
            else:
                items.append((new_key, v))
        return dict(items)
    
    def step(self):
        """Global step ì¦ê°€"""
        self.global_step += 1
    
    def flush(self):
        """ë²„í¼ í”ŒëŸ¬ì‹œ (ì¦‰ì‹œ ë””ìŠ¤í¬ ì“°ê¸°)"""
        self.writer.flush()
    
    def close(self):
        """TensorBoard writer ì¢…ë£Œ"""
        self.writer.close()
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()


# ê°„íŽ¸ ì‚¬ìš©ì„ ìœ„í•œ ì „ì—­ ë¡œê±°
_global_logger: Optional[TensorBoardLogger] = None


def get_tensorboard_logger(
    log_dir: str = None,
    experiment_name: str = None,
    reset: bool = False
) -> TensorBoardLogger:
    """ì „ì—­ TensorBoard ë¡œê±° ê°€ì ¸ì˜¤ê¸°"""
    global _global_logger
    
    if _global_logger is None or reset:
        _global_logger = TensorBoardLogger(log_dir, experiment_name)
    
    return _global_logger


def log_metrics(metrics: Dict[str, float], step: Optional[int] = None, prefix: str = ""):
    """ê°„íŽ¸ ë©”íŠ¸ë¦­ ë¡œê¹…"""
    logger = get_tensorboard_logger()
    for key, value in metrics.items():
        tag = f"{prefix}/{key}" if prefix else key
        logger.log_scalar(tag, value, step)