"""
System Optimization Utilities
ì‹œìŠ¤í…œ ìµœì í™” ìœ í‹¸ë¦¬í‹°

í™˜ê²½ë³„ ìµœì  ì„¤ì • ìë™ ê°ì§€:
- WSL vs Native Linux
- CPU/GPU ë¦¬ì†ŒìŠ¤ ê¸°ë°˜ ìµœì í™”
- DataLoader num_workers ìë™ ì¡°ì •
"""

import os
import platform
import multiprocessing as mp
import torch
from pathlib import Path
from typing import Dict, Any, Optional

from src.utils.core import PillSnapLogger


class SystemOptimizer:
    """ì‹œìŠ¤í…œ í™˜ê²½ ìµœì í™” ê´€ë¦¬ì"""
    
    def __init__(self):
        self.logger = PillSnapLogger(__name__)
        self._setup_thread_limiting()
        self._system_info = self._detect_system()
        
    def _setup_thread_limiting(self):
        """OMP/MKL ìŠ¤ë ˆë“œ ì œí•œ ì„¤ì •"""
        # WSLì—ì„œ ì›Œì»¤ ë‚´ë¶€ ìŠ¤ë ˆë“œ ê³¼ì  ë°©ì§€
        os.environ['OMP_NUM_THREADS'] = '1'
        os.environ['MKL_NUM_THREADS'] = '1'
        os.environ['NUMEXPR_NUM_THREADS'] = '1'
        self.logger.info("ğŸ”§ OMP/MKL ìŠ¤ë ˆë“œ 1ë¡œ ì œí•œ ì„¤ì •")
        
    def _detect_system(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í™˜ê²½ ê°ì§€"""
        info = {
            'platform': platform.system(),
            'cpu_count': mp.cpu_count(),
            'is_wsl': False,
            'has_cuda': torch.cuda.is_available(),
            'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0
        }
        
        # WSL ê°ì§€
        if info['platform'] == 'Linux':
            try:
                with open('/proc/version', 'r') as f:
                    version_info = f.read().lower()
                    if 'microsoft' in version_info or 'wsl' in version_info:
                        info['is_wsl'] = True
            except:
                pass
        
        # ì¶”ê°€ í™˜ê²½ ì •ë³´
        info['memory_gb'] = self._get_available_memory_gb()
        info['optimal_workers'] = self._calculate_optimal_workers(info)
        
        return info
    
    def _get_available_memory_gb(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB) ê³„ì‚°"""
        try:
            if torch.cuda.is_available():
                # GPU ë©”ëª¨ë¦¬ ìš°ì„ 
                gpu_memory = torch.cuda.get_device_properties(0).total_memory
                return gpu_memory / (1024**3)
            else:
                # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
                with open('/proc/meminfo', 'r') as f:
                    for line in f:
                        if line.startswith('MemAvailable:'):
                            memory_kb = int(line.split()[1])
                            return memory_kb / (1024**2)
        except:
            return 16.0  # ê¸°ë³¸ê°’
        
        return 16.0
    
    def _calculate_optimal_workers(self, info: Dict[str, Any]) -> int:
        """ìµœì  worker ìˆ˜ ê³„ì‚°"""
        cpu_count = info['cpu_count']
        
        # WSL í™˜ê²½ì—ì„œëŠ” ì•ˆì •ì„± ìš°ì„ 
        if info['is_wsl']:
            # WSLì—ì„œ ë©€í‹°í”„ë¡œì„¸ì‹± ë¬¸ì œë¡œ ì¸í•´ ë¹„í™œì„±í™”
            # batch_sizeê°€ í´ìˆ˜ë¡ ë°ë“œë½ ìœ„í—˜ ì¦ê°€
            return 0
        
        # Native Linux/Windows
        if cpu_count >= 16:
            return 8
        elif cpu_count >= 8:
            return 6
        elif cpu_count >= 4:
            return 4
        else:
            return max(1, cpu_count // 2)  # ìµœì†Œ 1ê°œ ë³´ì¥
    
    def get_dataloader_config(self, stage: int = 1) -> Dict[str, Any]:
        """Stageë³„ DataLoader ìµœì  ì„¤ì •"""
        base_workers = self._system_info['optimal_workers']
        
        # Stageë³„ ì¡°ì •
        stage_multipliers = {
            1: 1.0,   # Stage 1: ê¸°ë³¸
            2: 1.2,   # Stage 2: ì•½ê°„ ì¦ê°€
            3: 1.5,   # Stage 3: ë” ì¦ê°€  
            4: 2.0    # Stage 4: ìµœëŒ€
        }
        
        multiplier = stage_multipliers.get(stage, 1.0)
        workers = min(8, max(1, int(base_workers * multiplier)))
        
        # WSLì—ì„œëŠ” ìµœëŒ€ 4ë¡œ ì œí•œ
        if self._system_info['is_wsl']:
            workers = min(4, workers)
        
        # WSL í™˜ê²½ì—ì„œëŠ” ì•ˆì „ì„± ìš°ì„  ì„¤ì •
        if self._system_info['is_wsl']:
            config = {
                'num_workers': workers,
                'pin_memory': False,  # WSLì—ì„œ ì•ˆì •ì„± ìš°ì„  (data_time ì¦ê°€ íŠ¸ë ˆì´ë“œì˜¤í”„)
                'persistent_workers': False,  # WSLì—ì„œ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥
                'prefetch_factor': None,  # persistent_workers=Falseì¼ ë•ŒëŠ” None
                'drop_last': True,
                'multiprocessing_context': 'spawn'  # WSLì—ì„œ ê°€ì¥ ì•ˆì „
            }
        else:
            # Native í™˜ê²½ì—ì„œëŠ” ì„±ëŠ¥ ìš°ì„ 
            config = {
                'num_workers': workers,
                'pin_memory': self._system_info['has_cuda'],
                'persistent_workers': workers > 0,
                'prefetch_factor': 2 if workers > 0 else None,
                'drop_last': True
            }
        
        return config
    
    def get_training_config(self, batch_size: int) -> Dict[str, Any]:
        """í•™ìŠµ ìµœì í™” ì„¤ì •"""
        config = {
            'mixed_precision': self._system_info['has_cuda'],
            'compile_model': self._system_info['has_cuda'] and not self._system_info['is_wsl'],
            'gradient_clipping': True,
            'channels_last': self._system_info['has_cuda']
        }
        
        return config
    
    def log_system_info(self):
        """ì‹œìŠ¤í…œ ì •ë³´ ë¡œê¹…"""
        info = self._system_info
        
        self.logger.info("ğŸ–¥ï¸  ì‹œìŠ¤í…œ í™˜ê²½ ê°ì§€ ê²°ê³¼:")
        self.logger.info(f"   í”Œë«í¼: {info['platform']}")
        self.logger.info(f"   WSL í™˜ê²½: {'ì˜ˆ' if info['is_wsl'] else 'ì•„ë‹ˆì˜¤'}")
        self.logger.info(f"   CPU ì½”ì–´: {info['cpu_count']}ê°œ")
        self.logger.info(f"   CUDA ì‚¬ìš©: {'ì˜ˆ' if info['has_cuda'] else 'ì•„ë‹ˆì˜¤'}")
        if info['has_cuda']:
            self.logger.info(f"   GPU ê°œìˆ˜: {info['gpu_count']}ê°œ")
        self.logger.info(f"   ë©”ëª¨ë¦¬: {info['memory_gb']:.1f}GB")
        self.logger.info(f"   ìµœì  Workers: {info['optimal_workers']}ê°œ")
        
        # WSL íŠ¹ë³„ ê²½ê³ 
        if info['is_wsl']:
            self.logger.warning("âš ï¸  WSL í™˜ê²½ì—ì„œ ë©€í‹°í”„ë¡œì„¸ì‹± ìµœì í™” ì ìš©")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
system_optimizer = SystemOptimizer()


def get_optimal_num_workers(stage: int = 1) -> int:
    """Stageë³„ ìµœì  num_workers ë°˜í™˜"""
    return system_optimizer.get_dataloader_config(stage)['num_workers']


def get_dataloader_kwargs(stage: int = 1) -> Dict[str, Any]:
    """DataLoader ìƒì„±ìš© ìµœì  kwargs ë°˜í™˜"""
    return system_optimizer.get_dataloader_config(stage)


def log_system_optimization():
    """ì‹œìŠ¤í…œ ìµœì í™” ì •ë³´ ë¡œê¹…"""
    system_optimizer.log_system_info()