"""
PillSnap ML CUDA OOM ê°€ë“œ ì‹œìŠ¤í…œ (1ë‹¨ê³„ í•„ìˆ˜)

CUDA OOM ë³µêµ¬ ë° ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •:
- í•™ìŠµ/ê²€ì¦ CUDAMemoryError ìºì¹˜ â†’ per-GPU batch â†“ â†’ grad_accum â†‘ â†’ LR ì¬ë³´ì •
- torch.compile ì´ˆê¸° ì™„ì¶© ê¸°ê°„ ì§€ì›
- ê²€ì¦ ë°°ì¹˜ í¬ê¸° ê°•ì œ ì œí•œ

RTX 5080 16GB ìµœì í™”
"""

import torch
import time
import gc
from typing import Optional, Dict, Any, Callable, Tuple
from dataclasses import dataclass

from src.utils.core import PillSnapLogger


@dataclass
class OOMGuardConfig:
    """OOM ê°€ë“œ ì„¤ì •"""
    
    # ë°°ì¹˜ í¬ê¸° ì¡°ì • ì„¤ì •
    min_batch_size: int = 1
    batch_reduction_factor: float = 0.5  # 50%ì”© ê°ì†Œ
    max_oom_recoveries: int = 3          # ìµœëŒ€ 3íšŒ ë³µêµ¬ ì‹œë„
    
    # Gradient Accumulation ë³´ì •
    maintain_effective_batch: bool = True
    lr_scale_with_effective: bool = True  # Effective batch ë³€í™”ì‹œ LR ë³´ì •
    
    # torch.compile ì™„ì¶© ì„¤ì •
    compile_warmup_steps: int = 200      # ì´ˆê¸° 200 step ì™„ì¶©
    compile_batch_reduction: int = 1     # ì™„ì¶© ê¸°ê°„ ë°°ì¹˜ -1 ë‹¨ê³„
    
    # ê²€ì¦ ë°°ì¹˜ í¬ê¸° ì œí•œ
    max_validation_batch: int = 4        # ê²€ì¦ ë°°ì¹˜ ìµœëŒ€ 4
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬ ì„¤ì •
    force_gc_on_oom: bool = True
    empty_cache_on_oom: bool = True


class CUDAOOMGuard:
    """CUDA OOM ë³µêµ¬ ë° ë™ì  ë°°ì¹˜ í¬ê¸° ê´€ë¦¬"""
    
    def __init__(self, config: OOMGuardConfig):
        self.config = config
        self.logger = PillSnapLogger(__name__)
        
        # ìƒíƒœ ì¶”ì 
        self.oom_count = 0
        self.current_batch_size = None
        self.original_batch_size = None
        self.current_grad_accum = None
        self.original_grad_accum = None
        self.original_lr = None
        
        # torch.compile ì™„ì¶© ìƒíƒœ
        self.compile_warmup_active = False
        self.compile_step_count = 0
        self.compile_original_batch = None
        
        # ë³µêµ¬ íˆìŠ¤í† ë¦¬
        self.recovery_history = []
        
    def setup_training_params(
        self, 
        batch_size: int, 
        grad_accum_steps: int, 
        learning_rate: float
    ) -> None:
        """í•™ìŠµ íŒŒë¼ë¯¸í„° ì´ˆê¸° ì„¤ì •"""
        self.current_batch_size = batch_size
        self.original_batch_size = batch_size
        self.current_grad_accum = grad_accum_steps
        self.original_grad_accum = grad_accum_steps
        self.original_lr = learning_rate
        
        self.logger.info(f"ğŸ›¡ï¸ OOM Guard ì„¤ì •ì™„ë£Œ - Batch: {batch_size}, GradAccum: {grad_accum_steps}, LR: {learning_rate}")
    
    def enable_compile_warmup(self) -> Tuple[int, int]:
        """
        torch.compile ì™„ì¶© ê¸°ê°„ í™œì„±í™”
        
        Returns:
            Tuple[int, int]: (ì™„ì¶© ë°°ì¹˜ í¬ê¸°, ì™„ì¶© grad_accum)
        """
        if not self.compile_warmup_active:
            self.compile_warmup_active = True
            self.compile_step_count = 0
            self.compile_original_batch = self.current_batch_size
            
            # ì™„ì¶© ê¸°ê°„ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
            warmup_batch = max(
                self.config.min_batch_size, 
                self.current_batch_size - self.config.compile_batch_reduction
            )
            
            # Effective batch ìœ ì§€ë¥¼ ìœ„í•œ grad_accum ì¦ê°€
            if self.config.maintain_effective_batch:
                effective_batch = self.current_batch_size * self.current_grad_accum
                new_grad_accum = max(1, effective_batch // warmup_batch)
            else:
                new_grad_accum = self.current_grad_accum
            
            self.current_batch_size = warmup_batch
            self.current_grad_accum = new_grad_accum
            
            self.logger.warning(
                f"ğŸ”§ torch.compile ì™„ì¶© í™œì„±í™” ({self.config.compile_warmup_steps} steps) - "
                f"Batch: {self.compile_original_batch}â†’{warmup_batch}, "
                f"GradAccum: {self.original_grad_accum}â†’{new_grad_accum}"
            )
            
            return warmup_batch, new_grad_accum
        
        return self.current_batch_size, self.current_grad_accum
    
    def step_compile_warmup(self) -> bool:
        """
        torch.compile ì™„ì¶© step ì—…ë°ì´íŠ¸
        
        Returns:
            bool: ì™„ì¶© ê¸°ê°„ì´ ëë‚¬ëŠ”ì§€ ì—¬ë¶€
        """
        if not self.compile_warmup_active:
            return False
            
        self.compile_step_count += 1
        
        if self.compile_step_count >= self.config.compile_warmup_steps:
            # ì™„ì¶© ê¸°ê°„ ì¢…ë£Œ - ì›ë˜ ì„¤ì •ìœ¼ë¡œ ë³µêµ¬
            self.current_batch_size = self.compile_original_batch
            self.current_grad_accum = self.original_grad_accum
            self.compile_warmup_active = False
            
            self.logger.info(
                f"âœ… torch.compile ì™„ì¶© ì¢…ë£Œ - "
                f"Batch: {self.current_batch_size}, GradAccum: {self.current_grad_accum}"
            )
            return True
        
        return False
    
    def handle_oom_error(
        self, 
        error: Exception, 
        context: str = "training"
    ) -> Tuple[bool, int, int, Optional[float]]:
        """
        CUDA OOM ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬
        
        Args:
            error: CUDA OOM ì—ëŸ¬
            context: ì—ëŸ¬ ë°œìƒ ì»¨í…ìŠ¤íŠ¸ ("training" ë˜ëŠ” "validation")
            
        Returns:
            Tuple[bool, int, int, Optional[float]]: (ë³µêµ¬ ê°€ëŠ¥ ì—¬ë¶€, ìƒˆ ë°°ì¹˜ í¬ê¸°, ìƒˆ grad_accum, ìƒˆ LR)
        """
        self.oom_count += 1
        
        self.logger.error(f"ğŸš¨ CUDA OOM ë°œìƒ ({self.oom_count}/{self.config.max_oom_recoveries}íšŒ) - {context}: {error}")
        
        # ìµœëŒ€ ë³µêµ¬ íšŸìˆ˜ ì´ˆê³¼ ì‹œ ì‹¤íŒ¨
        if self.oom_count > self.config.max_oom_recoveries:
            self.logger.critical(f"âŒ ìµœëŒ€ OOM ë³µêµ¬ íšŸìˆ˜ ì´ˆê³¼ ({self.config.max_oom_recoveries}íšŒ) - í•™ìŠµ ì¤‘ë‹¨")
            return False, self.current_batch_size, self.current_grad_accum, None
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.config.force_gc_on_oom:
            gc.collect()
        
        if self.config.empty_cache_on_oom and torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        time.sleep(1)  # ë©”ëª¨ë¦¬ ì •ë¦¬ ëŒ€ê¸°
        
        # ë°°ì¹˜ í¬ê¸° ê°ì†Œ
        old_batch = self.current_batch_size
        new_batch = max(
            self.config.min_batch_size,
            int(self.current_batch_size * self.config.batch_reduction_factor)
        )
        
        # ìµœì†Œ ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•œ ê²½ìš°
        if new_batch == self.current_batch_size:
            self.logger.critical(f"âŒ ìµœì†Œ ë°°ì¹˜ í¬ê¸°({self.config.min_batch_size})ì— ë„ë‹¬ - ë” ì´ìƒ ë³µêµ¬ ë¶ˆê°€")
            return False, self.current_batch_size, self.current_grad_accum, None
        
        # Effective batch ìœ ì§€ë¥¼ ìœ„í•œ grad_accum ì¦ê°€
        new_grad_accum = self.current_grad_accum
        new_lr = None
        
        if self.config.maintain_effective_batch:
            original_effective = self.original_batch_size * self.original_grad_accum
            new_grad_accum = max(1, original_effective // new_batch)
            
            # LR ì¬ë³´ì • (effective batch ê¸°ì¤€)
            if self.config.lr_scale_with_effective and self.original_lr:
                new_effective = new_batch * new_grad_accum
                lr_scale_factor = new_effective / original_effective
                new_lr = self.original_lr * lr_scale_factor
        
        # ê²€ì¦ ì»¨í…ìŠ¤íŠ¸ì˜ ê²½ìš° ë” ê°•í•œ ì œí•œ ì ìš©
        if context == "validation":
            new_batch = min(new_batch, self.config.max_validation_batch)
            new_grad_accum = 1  # ê²€ì¦ì—ì„œëŠ” grad_accum ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            new_lr = None       # ê²€ì¦ì—ì„œëŠ” LR ì¡°ì • ì—†ìŒ
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        self.current_batch_size = new_batch
        self.current_grad_accum = new_grad_accum
        
        # ë³µêµ¬ íˆìŠ¤í† ë¦¬ ê¸°ë¡
        recovery_record = {
            "timestamp": time.time(),
            "context": context,
            "oom_count": self.oom_count,
            "batch_change": f"{old_batch}â†’{new_batch}",
            "grad_accum": new_grad_accum,
            "new_lr": new_lr
        }
        self.recovery_history.append(recovery_record)
        
        self.logger.warning(
            f"ğŸ”§ OOM ë³µêµ¬ ì‹œë„ - "
            f"Batch: {old_batch}â†’{new_batch}, "
            f"GradAccum: {self.current_grad_accum}, "
            f"ìƒˆ LR: {new_lr}"
        )
        
        return True, new_batch, new_grad_accum, new_lr
    
    def get_current_params(self) -> Tuple[int, int]:
        """í˜„ì¬ ë°°ì¹˜ í¬ê¸°ì™€ grad_accum ë°˜í™˜"""
        return self.current_batch_size, self.current_grad_accum
    
    def get_validation_batch_size(self) -> int:
        """ê²€ì¦ìš© ë°°ì¹˜ í¬ê¸° ë°˜í™˜ (ê°•ì œ ì œí•œ ì ìš©)"""
        return min(self.current_batch_size, self.config.max_validation_batch)
    
    def reset_oom_count(self) -> None:
        """OOM ì¹´ìš´í„° ë¦¬ì…‹ (ì—í¬í¬ ì‹œì‘ ì‹œ í˜¸ì¶œ)"""
        self.oom_count = 0
    
    def get_recovery_summary(self) -> Dict[str, Any]:
        """ë³µêµ¬ íˆìŠ¤í† ë¦¬ ìš”ì•½ ë°˜í™˜"""
        return {
            "total_oom_recoveries": len(self.recovery_history),
            "current_batch_size": self.current_batch_size,
            "current_grad_accum": self.current_grad_accum,
            "batch_size_reduction": f"{self.original_batch_size}â†’{self.current_batch_size}",
            "recovery_history": self.recovery_history[-5:]  # ìµœê·¼ 5ê°œë§Œ
        }


def oom_safe_training_step(
    oom_guard: CUDAOOMGuard,
    training_step_fn: Callable,
    *args,
    context: str = "training",
    **kwargs
) -> Tuple[bool, Any]:
    """
    OOM ì•ˆì „ í•™ìŠµ ìŠ¤í… ë˜í¼
    
    Args:
        oom_guard: OOM ê°€ë“œ ì¸ìŠ¤í„´ìŠ¤
        training_step_fn: í•™ìŠµ ìŠ¤í… í•¨ìˆ˜
        context: ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸
        *args, **kwargs: í•™ìŠµ ìŠ¤í… í•¨ìˆ˜ ì¸ì
        
    Returns:
        Tuple[bool, Any]: (ì„±ê³µ ì—¬ë¶€, ê²°ê³¼)
    """
    max_retries = 3
    
    for attempt in range(max_retries):
        try:
            # torch.compile ì™„ì¶© ì²˜ë¦¬
            if oom_guard.compile_warmup_active:
                oom_guard.step_compile_warmup()
            
            # í•™ìŠµ ìŠ¤í… ì‹¤í–‰
            result = training_step_fn(*args, **kwargs)
            return True, result
            
        except torch.cuda.OutOfMemoryError as e:
            # OOM ë³µêµ¬ ì‹œë„
            can_recover, new_batch, new_grad_accum, new_lr = oom_guard.handle_oom_error(e, context)
            
            if not can_recover:
                return False, None
            
            # ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„°ë¡œ ì¬ì‹œë„ë¥¼ ìœ„í•´ kwargs ì—…ë°ì´íŠ¸
            if 'batch_size' in kwargs:
                kwargs['batch_size'] = new_batch
            if 'grad_accum_steps' in kwargs:
                kwargs['grad_accum_steps'] = new_grad_accum
            if 'learning_rate' in kwargs and new_lr:
                kwargs['learning_rate'] = new_lr
            
            # ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
            time.sleep(2)
            continue
            
        except Exception as e:
            # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
            raise e
    
    # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
    return False, None


if __name__ == "__main__":
    print("ğŸ§ª CUDA OOM Guard ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (1ë‹¨ê³„ í•„ìˆ˜)")
    print("=" * 60)
    
    # ì„¤ì • í…ŒìŠ¤íŠ¸
    config = OOMGuardConfig()
    guard = CUDAOOMGuard(config)
    
    # ì´ˆê¸° ì„¤ì •
    guard.setup_training_params(batch_size=16, grad_accum_steps=4, learning_rate=2e-4)
    
    # torch.compile ì™„ì¶© í…ŒìŠ¤íŠ¸
    warmup_batch, warmup_grad_accum = guard.enable_compile_warmup()
    print(f"âœ… Compile ì™„ì¶©: Batch={warmup_batch}, GradAccum={warmup_grad_accum}")
    
    # ê°€ìƒ OOM ë³µêµ¬ í…ŒìŠ¤íŠ¸
    fake_error = torch.cuda.OutOfMemoryError("CUDA out of memory")
    can_recover, new_batch, new_grad_accum, new_lr = guard.handle_oom_error(fake_error, "training")
    
    if can_recover:
        print(f"âœ… OOM ë³µêµ¬ ì„±ê³µ: Batch={new_batch}, GradAccum={new_grad_accum}, LR={new_lr}")
    
    # ë³µêµ¬ ìš”ì•½
    summary = guard.get_recovery_summary()
    print(f"ğŸ“Š ë³µêµ¬ ìš”ì•½: {summary['total_oom_recoveries']}íšŒ ë³µêµ¬")
    
    print("ğŸ‰ CUDA OOM Guard í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")