"""
Single Pill Training DataLoader
ë‹¨ì¼ ì•½í’ˆ í•™ìŠµìš© ë°ì´í„°ë¡œë”

EfficientNetV2-S ë¶„ë¥˜ìš©:
- 384x384 ì´ë¯¸ì§€ ì „ì²˜ë¦¬
- Progressive Validation ìƒ˜í”Œë§ í†µí•©
- RTX 5080 ìµœì í™” (ë°°ì¹˜ ì²˜ë¦¬, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
- ë°ì´í„° ì¦ê°• ë° ìºì‹± ì§€ì›
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Union, Any
import random

import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from PIL import Image
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.data.progressive_validation_sampler import ProgressiveValidationSampler, Stage1SamplingStrategy
from src.data.image_preprocessing_factory import TwoStageImagePreprocessor, PipelineStage
from src.utils.core import PillSnapLogger, load_config
from src.utils.system_optimization import get_dataloader_kwargs, log_system_optimization


class SinglePillDataset(Dataset):
    """ë‹¨ì¼ ì•½í’ˆ ë°ì´í„°ì…‹"""
    
    def __init__(
        self,
        image_paths: List[Path],
        labels: List[int],
        class_to_idx: Dict[str, int],
        preprocessor: TwoStageImagePreprocessor,
        is_training: bool = True
    ):
        self.image_paths = image_paths
        self.labels = labels
        self.class_to_idx = class_to_idx
        self.idx_to_class = {v: k for k, v in class_to_idx.items()}
        self.preprocessor = preprocessor
        self.is_training = is_training
        self.logger = PillSnapLogger(__name__)
        
        assert len(image_paths) == len(labels), "ì´ë¯¸ì§€ì™€ ë¼ë²¨ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ"
        
        self.logger.info(f"SinglePillDataset ì´ˆê¸°í™”: {len(image_paths)}ê°œ ì´ë¯¸ì§€, {len(class_to_idx)}ê°œ í´ë˜ìŠ¤")
    
    def __len__(self) -> int:
        return len(self.image_paths)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:
        image_path = self.image_paths[idx]
        label = self.labels[idx]
        
        try:
            # ë¶„ë¥˜ìš© ì „ì²˜ë¦¬ (384x384)
            success, processed_tensor, info = self.preprocessor.preprocess_for_classification(
                image_path, 
                is_training=self.is_training
            )
            
            if not success:
                # ì „ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í…ì„œ ë°˜í™˜
                self.logger.warning(f"ì „ì²˜ë¦¬ ì‹¤íŒ¨: {image_path}, ê¸°ë³¸ í…ì„œ ì‚¬ìš©")
                processed_tensor = torch.zeros(3, 384, 384)
            
            return processed_tensor, label
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ {image_path}: {e}")
            # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return torch.zeros(3, 384, 384), label
    
    def get_class_name(self, class_idx: int) -> str:
        """í´ë˜ìŠ¤ ì¸ë±ìŠ¤ë¥¼ í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ë³€í™˜"""
        return self.idx_to_class.get(class_idx, f"Unknown_{class_idx}")
    
    def get_class_distribution(self) -> Dict[str, int]:
        """í´ë˜ìŠ¤ë³„ ë°ì´í„° ë¶„í¬ ë°˜í™˜"""
        distribution = {}
        for label in self.labels:
            class_name = self.get_class_name(label)
            distribution[class_name] = distribution.get(class_name, 0) + 1
        return distribution


class SinglePillTrainingDataLoader:
    """ë‹¨ì¼ ì•½í’ˆ í•™ìŠµìš© ë°ì´í„°ë¡œë” ë§¤ë‹ˆì €"""
    
    def __init__(
        self,
        data_root: Optional[str] = None,
        stage: int = 1,
        batch_size: int = 32,
        num_workers: Optional[int] = None,
        pin_memory: Optional[bool] = None
    ):
        # ì„¤ì •ì—ì„œ ë°ì´í„° ë£¨íŠ¸ ê°€ì ¸ì˜¤ê¸°
        if data_root is None:
            config = load_config()
            data_root = config['data']['root']
        
        self.data_root = Path(data_root)
        self.stage = stage
        self.batch_size = batch_size
        
        # ì‹œìŠ¤í…œ ìµœì í™” ì„¤ì • ì ìš©
        dataloader_config = get_dataloader_kwargs(stage)
        self.num_workers = num_workers if num_workers is not None else dataloader_config['num_workers']
        self.pin_memory = pin_memory if pin_memory is not None else dataloader_config['pin_memory']
        self.persistent_workers = dataloader_config.get('persistent_workers', False)
        self.prefetch_factor = dataloader_config.get('prefetch_factor', 2)
        self.drop_last = dataloader_config.get('drop_last', True)
        self.multiprocessing_context = dataloader_config.get('multiprocessing_context', None)
        
        self.logger = PillSnapLogger(__name__)
        
        # ì‹œìŠ¤í…œ ìµœì í™” ì •ë³´ ë¡œê¹… (ì²« ë²ˆì§¸ ì¸ìŠ¤í„´ìŠ¤ì—ì„œë§Œ)
        if not hasattr(SinglePillTrainingDataLoader, '_optimization_logged'):
            log_system_optimization()
            SinglePillTrainingDataLoader._optimization_logged = True
        
        # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        self.preprocessor = TwoStageImagePreprocessor()
        
        # ìƒ˜í”Œë§ ì „ëµ
        if stage == 1:
            self.strategy = Stage1SamplingStrategy(target_images=5000, target_classes=50)
        elif stage == 2:
            # Stage 2: 25K ìƒ˜í”Œ, 250 í´ë˜ìŠ¤
            from src.data.progressive_validation_sampler import Stage2SamplingStrategy
            self.strategy = Stage2SamplingStrategy(target_images=25000, target_classes=250)
        elif stage == 3:
            # Stage 3: 100K ìƒ˜í”Œ, 1000 í´ë˜ìŠ¤  
            from src.data.progressive_validation_sampler import Stage3SamplingStrategy
            self.strategy = Stage3SamplingStrategy(target_images=100000, target_classes=1000)
        elif stage == 4:
            # Stage 4: 500K ìƒ˜í”Œ, 4523 í´ë˜ìŠ¤
            from src.data.progressive_validation_sampler import Stage4SamplingStrategy
            self.strategy = Stage4SamplingStrategy(target_images=500000, target_classes=4523)
        else:
            raise NotImplementedError(f"Stage {stage} ì§€ì›í•˜ì§€ ì•ŠìŒ (1-4ë§Œ ì§€ì›)")
        
        self.logger.info(f"SinglePillTrainingDataLoader ì´ˆê¸°í™” (Stage {stage})")
        
    def prepare_datasets(
        self,
        validation_split: float = 0.2,
        random_seed: int = 42
    ) -> Tuple[SinglePillDataset, SinglePillDataset]:
        """í•™ìŠµ/ê²€ì¦ ë°ì´í„°ì…‹ ì¤€ë¹„"""
        
        self.logger.step("ë°ì´í„°ì…‹ ì¤€ë¹„", f"Stage {self.stage} ë‹¨ì¼ ì•½í’ˆ ìƒ˜í”Œë§ ë° ë¶„í• ")
        
        try:
            # Progressive Validation ìƒ˜í”ŒëŸ¬ë¡œ ë°ì´í„° ìƒì„±
            sampler = ProgressiveValidationSampler(str(self.data_root), self.strategy)
            stage_sample = sampler.generate_stage1_sample()
            
            # ë‹¨ì¼ ì•½í’ˆë§Œ í•„í„°ë§
            single_samples = []
            for k_code, sample_data in stage_sample['samples'].items():
                single_images = sample_data.get('single_images', [])
                single_samples.extend([(Path(img_path), k_code) for img_path in single_images])
            
            # í´ë˜ìŠ¤ ë§¤í•‘ ìƒì„±
            unique_classes = sorted(list(set([k_code for _, k_code in single_samples])))
            class_to_idx = {k_code: idx for idx, k_code in enumerate(unique_classes)}
            
            # ì´ë¯¸ì§€ ê²½ë¡œì™€ ë¼ë²¨ ë¶„ë¦¬
            image_paths = [img_path for img_path, _ in single_samples]
            labels = [class_to_idx[k_code] for _, k_code in single_samples]
            
            self.logger.info(f"ì´ ì´ë¯¸ì§€: {len(image_paths)}ê°œ, í´ë˜ìŠ¤: {len(unique_classes)}ê°œ")
            
            # í•™ìŠµ/ê²€ì¦ ë¶„í• 
            random.seed(random_seed)
            indices = list(range(len(image_paths)))
            random.shuffle(indices)
            
            split_idx = int(len(indices) * (1 - validation_split))
            train_indices = indices[:split_idx]
            val_indices = indices[split_idx:]
            
            # í•™ìŠµ ë°ì´í„°ì…‹
            train_image_paths = [image_paths[i] for i in train_indices]
            train_labels = [labels[i] for i in train_indices]
            
            train_dataset = SinglePillDataset(
                image_paths=train_image_paths,
                labels=train_labels,
                class_to_idx=class_to_idx,
                preprocessor=self.preprocessor,
                is_training=True
            )
            
            # ê²€ì¦ ë°ì´í„°ì…‹
            val_image_paths = [image_paths[i] for i in val_indices]
            val_labels = [labels[i] for i in val_indices]
            
            val_dataset = SinglePillDataset(
                image_paths=val_image_paths,
                labels=val_labels,
                class_to_idx=class_to_idx,
                preprocessor=self.preprocessor,
                is_training=False
            )
            
            self.logger.info(f"í•™ìŠµ ë°ì´í„°: {len(train_dataset)}ê°œ")
            self.logger.info(f"ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ")
            self.logger.success("ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ")
            
            return train_dataset, val_dataset
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            raise
    
    def create_dataloaders(
        self,
        train_dataset: SinglePillDataset,
        val_dataset: SinglePillDataset,
        shuffle_train: bool = True
    ) -> Tuple[DataLoader, DataLoader]:
        """ë°ì´í„°ë¡œë” ìƒì„±"""
        
        try:
            # í•™ìŠµ ë°ì´í„°ë¡œë” (ì‹œìŠ¤í…œ ìµœì í™” ì ìš©)
            train_kwargs = {
                'dataset': train_dataset,
                'batch_size': self.batch_size,
                'shuffle': shuffle_train,
                'num_workers': self.num_workers,
                'pin_memory': self.pin_memory,
                'drop_last': self.drop_last
            }
            
            # ì¶”ê°€ ìµœì í™” ì˜µì…˜ ì ìš© (num_workers > 0ì¼ ë•Œë§Œ)
            if self.num_workers > 0:
                if self.persistent_workers:
                    train_kwargs['persistent_workers'] = True
                if self.prefetch_factor is not None:
                    train_kwargs['prefetch_factor'] = self.prefetch_factor
                if self.multiprocessing_context is not None:
                    import multiprocessing as mp
                    train_kwargs['multiprocessing_context'] = mp.get_context(self.multiprocessing_context)
            
            train_loader = DataLoader(**train_kwargs)
            
            # ê²€ì¦ ë°ì´í„°ë¡œë” (ë™ì¼í•œ ìµœì í™” ì ìš©, ì…”í”Œ ì œì™¸)
            val_kwargs = train_kwargs.copy()
            val_kwargs.update({
                'dataset': val_dataset,
                'shuffle': False,
                'drop_last': False  # ê²€ì¦ì—ì„œëŠ” ëª¨ë“  ë°ì´í„° ì‚¬ìš©
            })
            
            val_loader = DataLoader(**val_kwargs)
            
            self.logger.info(f"ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ")
            self.logger.info(f"  í•™ìŠµ ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
            self.logger.info(f"  ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader)}")
            
            return train_loader, val_loader
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„°ë¡œë” ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def get_stage_dataloaders(
        self,
        validation_split: float = 0.2,
        shuffle_train: bool = True
    ) -> Tuple[DataLoader, DataLoader, Dict[str, Any]]:
        """Stageë³„ ë°ì´í„°ë¡œë” ì›ìŠ¤í†± ìƒì„±"""
        
        # ë°ì´í„°ì…‹ ì¤€ë¹„
        train_dataset, val_dataset = self.prepare_datasets(validation_split)
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        train_loader, val_loader = self.create_dataloaders(
            train_dataset, val_dataset, shuffle_train
        )
        
        # ë©”íƒ€ì •ë³´
        metadata = {
            'num_classes': len(train_dataset.class_to_idx),
            'class_to_idx': train_dataset.class_to_idx,
            'train_size': len(train_dataset),
            'val_size': len(val_dataset),
            'batch_size': self.batch_size,
            'stage': self.stage
        }
        
        return train_loader, val_loader, metadata


def main():
    """ë‹¨ì¼ ì•½í’ˆ ë°ì´í„°ë¡œë” í…ŒìŠ¤íŠ¸"""
    print("ğŸ“Š Single Pill Training DataLoader Test")
    print("=" * 60)
    
    try:
        # ë°ì´í„°ë¡œë” ë§¤ë‹ˆì € ìƒì„±
        dataloader_manager = SinglePillTrainingDataLoader(
            stage=1,
            batch_size=16,  # í…ŒìŠ¤íŠ¸ìš© ì‘ì€ ë°°ì¹˜
            num_workers=2
        )
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        train_loader, val_loader, metadata = dataloader_manager.get_stage_dataloaders()
        
        print(f"âœ… ë°ì´í„°ë¡œë” ìƒì„± ì„±ê³µ")
        print(f"   í´ë˜ìŠ¤ ìˆ˜: {metadata['num_classes']}")
        print(f"   í•™ìŠµ ë°ì´í„°: {metadata['train_size']}ê°œ")
        print(f"   ê²€ì¦ ë°ì´í„°: {metadata['val_size']}ê°œ")
        print(f"   ë°°ì¹˜ ìˆ˜: í•™ìŠµ {len(train_loader)}, ê²€ì¦ {len(val_loader)}")
        
        # ì²« ë²ˆì§¸ ë°°ì¹˜ í…ŒìŠ¤íŠ¸
        train_batch = next(iter(train_loader))
        images, labels = train_batch
        
        print(f"   ë°°ì¹˜ ëª¨ì–‘: ì´ë¯¸ì§€ {images.shape}, ë¼ë²¨ {labels.shape}")
        print(f"   ì´ë¯¸ì§€ ë²”ìœ„: [{images.min():.3f}, {images.max():.3f}]")
        
        print("\nâœ… ë‹¨ì¼ ì•½í’ˆ ë°ì´í„°ë¡œë” í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()