"""
PillSnap ML ìµœì í™”ëœ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

ê³ ì • í•´ìƒë„ (976x1280) íŠ¹í™” ì „ì²˜ë¦¬:
- ë™ì  í¬ê¸° ê³„ì‚° ì œê±°
- í•˜ë“œì½”ë”©ëœ ë³€í™˜ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
"""

import cv2
import numpy as np
import torch
import albumentations as A
from albumentations.pytorch import ToTensorV2
from pathlib import Path
from typing import Tuple, Optional, Dict, Any
from dataclasses import dataclass
import time

from src.utils.core import PillSnapLogger


@dataclass
class OptimizedPreprocessingConfig:
    """ìµœì í™”ëœ ì „ì²˜ë¦¬ ì„¤ì •"""
    
    # ê³ ì • ì…ë ¥ í•´ìƒë„ (ì‹¤ì œ ë°ì´í„°)
    input_size: Tuple[int, int] = (976, 1280)  # W x H
    
    # ëª©í‘œ í•´ìƒë„
    detection_size: Tuple[int, int] = (640, 640)      # YOLOv11x
    classification_size: Tuple[int, int] = (384, 384)  # EfficientNetV2-L
    
    # ì„±ëŠ¥ ìµœì í™”
    interpolation: int = cv2.INTER_LINEAR  # LANCZOS4 ëŒ€ì‹  LINEAR (ë” ë¹ ë¦„)
    memory_format: str = "channels_last"
    
    # ì •ê·œí™” ìƒìˆ˜ (ë¯¸ë¦¬ ê³„ì‚°)
    normalize_mean: Tuple[float, float, float] = (0.485, 0.456, 0.406)
    normalize_std: Tuple[float, float, float] = (0.229, 0.224, 0.225)


class OptimizedImagePreprocessor:
    """976x1280 ê³ ì • í•´ìƒë„ íŠ¹í™” ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self, config: Optional[OptimizedPreprocessingConfig] = None):
        self.config = config or OptimizedPreprocessingConfig()
        self.logger = PillSnapLogger(__name__)
        
        # ê³ ì • ë³€í™˜ ë§¤íŠ¸ë¦­ìŠ¤ ë¯¸ë¦¬ ê³„ì‚°
        self._precalculate_transforms()
        
        # ì¦ê°• íŒŒì´í”„ë¼ì¸ (ìµœì í™”ëœ ë²„ì „)
        self._setup_augmentation_pipelines()
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            'processed_images': 0,
            'total_time_ms': 0,
            'avg_time_ms': 0
        }
        
        self.logger.info(f"ìµœì í™”ëœ ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (976x1280 íŠ¹í™”)")
        self.logger.info(f"  ê²€ì¶œ ëª©í‘œ: {self.config.detection_size}")
        self.logger.info(f"  ë¶„ë¥˜ ëª©í‘œ: {self.config.classification_size}")
    
    def _precalculate_transforms(self):
        """ê³ ì • í•´ìƒë„ìš© ë³€í™˜ ë§¤íŠ¸ë¦­ìŠ¤ ë¯¸ë¦¬ ê³„ì‚°"""
        
        # ë¶„ë¥˜ìš© ë³€í™˜ (976x1280 â†’ 384x384)
        # ìµœì : ì¤‘ì•™ ì˜ì—­ì„ 384x503ìœ¼ë¡œ í¬ë¡­ í›„ 384x384ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        input_w, input_h = self.config.input_size
        target_w, target_h = self.config.classification_size
        
        # ì¤‘ì•™ í¬ë¡­ ì˜ì—­ ê³„ì‚° (ì¢…íš¡ë¹„ ë§ì¶¤)
        crop_w = min(input_w, int(input_h * target_w / target_h))
        crop_h = min(input_h, int(input_w * target_h / target_w))
        
        self.cls_crop_x = (input_w - crop_w) // 2
        self.cls_crop_y = (input_h - crop_h) // 2
        self.cls_crop_w = crop_w
        self.cls_crop_h = crop_h
        
        # ê²€ì¶œìš© ë³€í™˜ (976x1280 â†’ 640x640)
        # ìµœì : 488x640ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ í›„ ì–‘ìª½ì— 76px íŒ¨ë”©
        det_w, det_h = self.config.detection_size
        scale = min(det_w / input_w, det_h / input_h)
        
        self.det_new_w = int(input_w * scale)
        self.det_new_h = int(input_h * scale)
        self.det_pad_x = (det_w - self.det_new_w) // 2
        self.det_pad_y = (det_h - self.det_new_h) // 2
        
        self.logger.debug(f"ë¶„ë¥˜ìš© í¬ë¡­: ({self.cls_crop_x}, {self.cls_crop_y}, {self.cls_crop_w}, {self.cls_crop_h})")
        self.logger.debug(f"ê²€ì¶œìš© ë¦¬ì‚¬ì´ì¦ˆ: {self.det_new_w}x{self.det_new_h}, íŒ¨ë”©: {self.det_pad_x}x{self.det_pad_y}")
    
    def _setup_augmentation_pipelines(self):
        """ìµœì í™”ëœ ì¦ê°• íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
        
        # ê²€ì¶œìš© (ë¹ ë¥¸ ë²„ì „)
        self.detection_augmentation = A.Compose([
            A.HorizontalFlip(p=0.5),
            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),
            A.HueSaturationValue(hue_shift_limit=5, sat_shift_limit=10, val_shift_limit=5, p=0.3),
            A.Normalize(mean=self.config.normalize_mean, std=self.config.normalize_std),
            ToTensorV2()
        ])
        
        # ë¶„ë¥˜ìš© (ë¹ ë¥¸ ë²„ì „)
        self.classification_augmentation = A.Compose([
            A.HorizontalFlip(p=0.5),
            A.Rotate(limit=10, p=0.3),
            A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.5),
            A.HueSaturationValue(hue_shift_limit=8, sat_shift_limit=15, val_shift_limit=8, p=0.4),
            A.Normalize(mean=self.config.normalize_mean, std=self.config.normalize_std),
            ToTensorV2()
        ])
        
        # ê²€ì¦ìš© (ì¦ê°• ì—†ìŒ)
        self.validation_transform = A.Compose([
            A.Normalize(mean=self.config.normalize_mean, std=self.config.normalize_std),
            ToTensorV2()
        ])
    
    def preprocess_for_classification(
        self, 
        image_path: Path, 
        is_training: bool = True
    ) -> Tuple[bool, Optional[torch.Tensor], Dict]:
        """ìµœì í™”ëœ ë¶„ë¥˜ìš© ì „ì²˜ë¦¬ (976x1280 â†’ 384x384)"""
        start_time = time.time()
        
        try:
            # 1. ì´ë¯¸ì§€ ë¡œë“œ (PIL ëŒ€ì‹  OpenCV ì§ì ‘ ì‚¬ìš©)
            image = cv2.imread(str(image_path))
            if image is None:
                return False, None, {'error': f'ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}'}
            
            # BGR â†’ RGB ë³€í™˜
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # 2. ê³ ì • í¬ë¡­ (ì¤‘ì•™ ì˜ì—­)
            cropped = image[
                self.cls_crop_y:self.cls_crop_y + self.cls_crop_h,
                self.cls_crop_x:self.cls_crop_x + self.cls_crop_w
            ]
            
            # 3. ëª©í‘œ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            resized = cv2.resize(
                cropped, 
                self.config.classification_size, 
                interpolation=self.config.interpolation
            )
            
            # 4. ì¦ê°• ë° í…ì„œ ë³€í™˜
            if is_training:
                transformed = self.classification_augmentation(image=resized)
            else:
                transformed = self.validation_transform(image=resized)
            
            tensor = transformed['image']
            
            # 5. í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = (time.time() - start_time) * 1000
            self.stats['processed_images'] += 1
            self.stats['total_time_ms'] += processing_time
            self.stats['avg_time_ms'] = self.stats['total_time_ms'] / self.stats['processed_images']
            
            return True, tensor, {
                'stage': 'classification',
                'processing_time_ms': processing_time,
                'input_size': f"{self.config.input_size[0]}x{self.config.input_size[1]}",
                'output_size': self.config.classification_size,
                'is_training': is_training
            }
            
        except Exception as e:
            return False, None, {'error': f'ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}'}
    
    def preprocess_for_detection(
        self, 
        image_path: Path, 
        is_training: bool = True
    ) -> Tuple[bool, Optional[torch.Tensor], Dict]:
        """ìµœì í™”ëœ ê²€ì¶œìš© ì „ì²˜ë¦¬ (976x1280 â†’ 640x640)"""
        start_time = time.time()
        
        try:
            # 1. ì´ë¯¸ì§€ ë¡œë“œ
            image = cv2.imread(str(image_path))
            if image is None:
                return False, None, {'error': f'ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}'}
            
            # BGR â†’ RGB ë³€í™˜
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # 2. ê³ ì • ìŠ¤ì¼€ì¼ ë¦¬ì‚¬ì´ì¦ˆ
            resized = cv2.resize(
                image, 
                (self.det_new_w, self.det_new_h), 
                interpolation=self.config.interpolation
            )
            
            # 3. ê³ ì • íŒ¨ë”© ì¶”ê°€
            padded = cv2.copyMakeBorder(
                resized,
                self.det_pad_y, 
                self.config.detection_size[1] - self.det_new_h - self.det_pad_y,
                self.det_pad_x, 
                self.config.detection_size[0] - self.det_new_w - self.det_pad_x,
                cv2.BORDER_CONSTANT,
                value=[0, 0, 0]
            )
            
            # 4. ì¦ê°• ë° í…ì„œ ë³€í™˜
            if is_training:
                transformed = self.detection_augmentation(image=padded)
            else:
                transformed = self.validation_transform(image=padded)
            
            tensor = transformed['image']
            
            # 5. í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = (time.time() - start_time) * 1000
            self.stats['processed_images'] += 1
            self.stats['total_time_ms'] += processing_time
            self.stats['avg_time_ms'] = self.stats['total_time_ms'] / self.stats['processed_images']
            
            return True, tensor, {
                'stage': 'detection',
                'processing_time_ms': processing_time,
                'input_size': f"{self.config.input_size[0]}x{self.config.input_size[1]}",
                'output_size': self.config.detection_size,
                'is_training': is_training
            }
            
        except Exception as e:
            return False, None, {'error': f'ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}'}
    
    def preprocess_pil_for_detection(
        self, 
        pil_image, 
        is_training: bool = True
    ) -> torch.Tensor:
        """PIL Imageë¥¼ Detectionìš©ìœ¼ë¡œ ì „ì²˜ë¦¬ (640x640)"""
        import numpy as np
        
        # PIL â†’ numpy ë³€í™˜
        image_np = np.array(pil_image)
        
        # RGB í™•ì¸ (PILì€ ì´ë¯¸ RGB)
        if len(image_np.shape) != 3 or image_np.shape[2] != 3:
            raise ValueError("RGB ì´ë¯¸ì§€ê°€ ì•„ë‹™ë‹ˆë‹¤")
        
        # 640x640ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        resized = cv2.resize(image_np, (640, 640), interpolation=cv2.INTER_LINEAR)
        
        # ì •ê·œí™” ë° í…ì„œ ë³€í™˜
        tensor = torch.from_numpy(resized).float()
        tensor = tensor / 255.0  # [0, 1] ì •ê·œí™”
        tensor = tensor.permute(2, 0, 1)  # HWC â†’ CHW
        tensor = tensor.unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€: [1, 3, 640, 640]
        
        return tensor
    
    def preprocess_pil_for_classification(
        self, 
        pil_image, 
        is_training: bool = True
    ) -> torch.Tensor:
        """PIL Imageë¥¼ Classificationìš©ìœ¼ë¡œ ì „ì²˜ë¦¬ (384x384)"""
        import numpy as np
        
        # PIL â†’ numpy ë³€í™˜
        image_np = np.array(pil_image)
        
        # RGB í™•ì¸
        if len(image_np.shape) != 3 or image_np.shape[2] != 3:
            raise ValueError("RGB ì´ë¯¸ì§€ê°€ ì•„ë‹™ë‹ˆë‹¤")
        
        # 384x384ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        resized = cv2.resize(image_np, (384, 384), interpolation=cv2.INTER_LINEAR)
        
        # ì •ê·œí™” ë° í…ì„œ ë³€í™˜
        tensor = torch.from_numpy(resized).float()
        tensor = tensor / 255.0  # [0, 1] ì •ê·œí™”
        tensor = tensor.permute(2, 0, 1)  # HWC â†’ CHW
        tensor = tensor.unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€: [1, 3, 384, 384]
        
        return tensor

    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        return self.stats.copy()


def create_optimized_preprocessor_for_stage1() -> OptimizedImagePreprocessor:
    """Stage 1ìš© ìµœì í™”ëœ ì „ì²˜ë¦¬ê¸° ìƒì„±"""
    config = OptimizedPreprocessingConfig(
        detection_size=(640, 640),
        classification_size=(384, 384),
        interpolation=cv2.INTER_LINEAR,  # ì„±ëŠ¥ ìš°ì„ 
    )
    return OptimizedImagePreprocessor(config)


if __name__ == "__main__":
    # ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent.parent))
    
    from src.data.image_preprocessing import TwoStageImagePreprocessor
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
    test_image = Path("/mnt/data/pillsnap_dataset/data/train/images/single/TS_66_single/K-030552/K-030552_0_0_1_0_75_000_200.png")
    
    if test_image.exists():
        print("=== ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ===")
        
        # ê¸°ì¡´ ì „ì²˜ë¦¬ê¸°
        old_preprocessor = TwoStageImagePreprocessor()
        
        # ìµœì í™”ëœ ì „ì²˜ë¦¬ê¸°
        new_preprocessor = create_optimized_preprocessor_for_stage1()
        
        # ì„±ëŠ¥ ì¸¡ì •
        n_tests = 20
        
        print(f"\nğŸ”¬ {n_tests}íšŒ ë°˜ë³µ í…ŒìŠ¤íŠ¸ (ë¶„ë¥˜ìš©)")
        
        # ê¸°ì¡´ ë°©ì‹
        old_times = []
        for _ in range(n_tests):
            start = time.time()
            success, tensor, info = old_preprocessor.preprocess_for_classification(test_image, is_training=False)
            old_times.append((time.time() - start) * 1000)
        
        # ìµœì í™”ëœ ë°©ì‹
        new_times = []
        for _ in range(n_tests):
            start = time.time()
            success, tensor, info = new_preprocessor.preprocess_for_classification(test_image, is_training=False)
            new_times.append((time.time() - start) * 1000)
        
        print(f"ê¸°ì¡´ ë°©ì‹: {sum(old_times)/len(old_times):.2f}ms (í‰ê· )")
        print(f"ìµœì í™” ë°©ì‹: {sum(new_times)/len(new_times):.2f}ms (í‰ê· )")
        print(f"ì„±ëŠ¥ í–¥ìƒ: {((sum(old_times)/len(old_times)) / (sum(new_times)/len(new_times)) - 1) * 100:.1f}%")
        print(f"ì²˜ë¦¬ëŸ‰ í–¥ìƒ: {1000/(sum(new_times)/len(new_times)):.1f} vs {1000/(sum(old_times)/len(old_times)):.1f} images/sec")