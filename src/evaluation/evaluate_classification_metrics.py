"""
Classification Metrics Evaluator
ë¶„ë¥˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­ í‰ê°€ ì‹œìŠ¤í…œ

Stage 1 ëª©í‘œ:
- ë¶„ë¥˜ ì •í™•ë„: 40% (50ê°œ í´ë˜ìŠ¤)
- Top-5 ì •í™•ë„, F1-Score, í˜¼ë™ í–‰ë ¬
- í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
- ROC ê³¡ì„  ë° PR ê³¡ì„ 
"""

import time
import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass

import torch
import torch.nn.functional as F
from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support, 
    confusion_matrix, classification_report,
    roc_auc_score, average_precision_score
)
import matplotlib.pyplot as plt
import seaborn as sns

from src.utils.core import PillSnapLogger


@dataclass
class ClassificationMetrics:
    """ë¶„ë¥˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­
    top1_accuracy: float
    top5_accuracy: float
    
    # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­
    precision_macro: float
    recall_macro: float
    f1_macro: float
    
    # ê°€ì¤‘ í‰ê·  ë©”íŠ¸ë¦­
    precision_weighted: float
    recall_weighted: float
    f1_weighted: float
    
    # ì¶”ê°€ ë©”íŠ¸ë¦­
    auc_macro: Optional[float] = None
    auc_weighted: Optional[float] = None
    
    # ë©”íƒ€ë°ì´í„°
    num_classes: int = 50
    num_samples: int = 0
    evaluation_time_seconds: float = 0.0


class ClassificationMetricsEvaluator:
    """ë¶„ë¥˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­ í‰ê°€ê¸°"""
    
    def __init__(self, num_classes: int = 50, class_names: Optional[List[str]] = None):
        self.num_classes = num_classes
        self.class_names = class_names or [f"Class_{i}" for i in range(num_classes)]
        self.logger = PillSnapLogger(__name__)
        
        self.logger.info(f"ë¶„ë¥˜ ë©”íŠ¸ë¦­ í‰ê°€ê¸° ì´ˆê¸°í™”: {num_classes}ê°œ í´ë˜ìŠ¤")
    
    def evaluate_predictions(
        self, 
        y_true: torch.Tensor, 
        y_pred_logits: torch.Tensor,
        y_pred_probs: Optional[torch.Tensor] = None
    ) -> ClassificationMetrics:
        """ì˜ˆì¸¡ ê²°ê³¼ í‰ê°€"""
        
        start_time = time.time()
        
        try:
            # ì…ë ¥ ê²€ì¦
            if y_true.shape[0] != y_pred_logits.shape[0]:
                raise ValueError(f"ìƒ˜í”Œ ìˆ˜ ë¶ˆì¼ì¹˜: {y_true.shape[0]} vs {y_pred_logits.shape[0]}")
            
            # CPUë¡œ ì´ë™
            y_true_np = y_true.cpu().numpy()
            y_pred_logits_np = y_pred_logits.cpu().numpy()
            
            # í™•ë¥  ê³„ì‚°
            if y_pred_probs is None:
                y_pred_probs = F.softmax(y_pred_logits, dim=1)
            y_pred_probs_np = y_pred_probs.cpu().numpy()
            
            # Top-1 ì˜ˆì¸¡
            y_pred_top1 = np.argmax(y_pred_logits_np, axis=1)
            
            # Top-5 ì˜ˆì¸¡
            top5_indices = np.argsort(y_pred_logits_np, axis=1)[:, -5:]
            
            # ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°
            top1_accuracy = accuracy_score(y_true_np, y_pred_top1)
            top5_accuracy = self._calculate_top_k_accuracy(y_true_np, top5_indices, k=5)
            
            # ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜
            precision, recall, f1, _ = precision_recall_fscore_support(
                y_true_np, y_pred_top1, average=None, zero_division=0
            )
            
            precision_macro = np.mean(precision)
            recall_macro = np.mean(recall)
            f1_macro = np.mean(f1)
            
            # ê°€ì¤‘ í‰ê· 
            precision_weighted = precision_recall_fscore_support(
                y_true_np, y_pred_top1, average='weighted', zero_division=0
            )[0]
            recall_weighted = precision_recall_fscore_support(
                y_true_np, y_pred_top1, average='weighted', zero_division=0
            )[1]
            f1_weighted = precision_recall_fscore_support(
                y_true_np, y_pred_top1, average='weighted', zero_division=0
            )[2]
            
            # AUC ê³„ì‚° (ë‹¤ì¤‘ í´ë˜ìŠ¤)
            try:
                auc_macro = roc_auc_score(y_true_np, y_pred_probs_np, 
                                        multi_class='ovr', average='macro')
                auc_weighted = roc_auc_score(y_true_np, y_pred_probs_np, 
                                           multi_class='ovr', average='weighted')
            except Exception as e:
                self.logger.warning(f"AUC ê³„ì‚° ì‹¤íŒ¨: {e}")
                auc_macro = None
                auc_weighted = None
            
            # ë©”íŠ¸ë¦­ ê°ì²´ ìƒì„±
            metrics = ClassificationMetrics(
                top1_accuracy=top1_accuracy,
                top5_accuracy=top5_accuracy,
                precision_macro=precision_macro,
                recall_macro=recall_macro,
                f1_macro=f1_macro,
                precision_weighted=precision_weighted,
                recall_weighted=recall_weighted,
                f1_weighted=f1_weighted,
                auc_macro=auc_macro,
                auc_weighted=auc_weighted,
                num_classes=self.num_classes,
                num_samples=len(y_true_np),
                evaluation_time_seconds=time.time() - start_time
            )
            
            # ê²°ê³¼ ë¡œê¹…
            self.logger.info(f"ë¶„ë¥˜ í‰ê°€ ì™„ë£Œ ({len(y_true_np)}ê°œ ìƒ˜í”Œ)")
            self.logger.metric("top1_accuracy", top1_accuracy, "%")
            self.logger.metric("top5_accuracy", top5_accuracy, "%")
            self.logger.metric("f1_macro", f1_macro)
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"ë¶„ë¥˜ í‰ê°€ ì‹¤íŒ¨: {e}")
            raise
    
    def _calculate_top_k_accuracy(self, y_true: np.ndarray, top_k_indices: np.ndarray, k: int) -> float:
        """Top-K ì •í™•ë„ ê³„ì‚°"""
        correct = 0
        for i, true_label in enumerate(y_true):
            if true_label in top_k_indices[i]:
                correct += 1
        return correct / len(y_true)
    
    def generate_confusion_matrix(
        self, 
        y_true: torch.Tensor, 
        y_pred: torch.Tensor,
        save_plot: bool = True
    ) -> np.ndarray:
        """í˜¼ë™ í–‰ë ¬ ìƒì„±"""
        
        try:
            y_true_np = y_true.cpu().numpy()
            y_pred_np = y_pred.cpu().numpy()
            
            # í˜¼ë™ í–‰ë ¬ ê³„ì‚°
            cm = confusion_matrix(y_true_np, y_pred_np)
            
            if save_plot:
                self._plot_confusion_matrix(cm)
            
            return cm
            
        except Exception as e:
            self.logger.error(f"í˜¼ë™ í–‰ë ¬ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def _plot_confusion_matrix(self, cm: np.ndarray) -> str:
        """í˜¼ë™ í–‰ë ¬ ì‹œê°í™”"""
        try:
            plt.figure(figsize=(12, 10))
            
            # ì •ê·œí™”ëœ í˜¼ë™ í–‰ë ¬
            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            
            # íˆíŠ¸ë§µ ìƒì„±
            sns.heatmap(cm_normalized, annot=False, cmap='Blues', 
                       xticklabels=False, yticklabels=False)
            
            plt.title(f'Confusion Matrix (Normalized)\n{self.num_classes} Classes')
            plt.xlabel('Predicted Label')
            plt.ylabel('True Label')
            
            # ì €ì¥
            plot_dir = Path("artifacts/reports/validation_results")
            plot_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            plot_file = plot_dir / f"confusion_matrix_{timestamp}.png"
            
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"í˜¼ë™ í–‰ë ¬ ì €ì¥: {plot_file}")
            return str(plot_file)
            
        except Exception as e:
            self.logger.warning(f"í˜¼ë™ í–‰ë ¬ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return ""
    
    def generate_classification_report(
        self, 
        y_true: torch.Tensor, 
        y_pred: torch.Tensor
    ) -> Dict[str, Any]:
        """ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        try:
            y_true_np = y_true.cpu().numpy()
            y_pred_np = y_pred.cpu().numpy()
            
            # í´ë˜ìŠ¤ë³„ ë¦¬í¬íŠ¸
            report_dict = classification_report(
                y_true_np, y_pred_np, 
                target_names=self.class_names[:self.num_classes],
                output_dict=True,
                zero_division=0
            )
            
            # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
            class_performance = []
            for i, class_name in enumerate(self.class_names[:self.num_classes]):
                if str(i) in report_dict or class_name in report_dict:
                    class_stats = report_dict.get(str(i), report_dict.get(class_name, {}))
                    class_performance.append({
                        'class_id': i,
                        'class_name': class_name,
                        'precision': class_stats.get('precision', 0.0),
                        'recall': class_stats.get('recall', 0.0),
                        'f1_score': class_stats.get('f1-score', 0.0),
                        'support': class_stats.get('support', 0)
                    })
            
            # ìµœê³ /ìµœì € ì„±ëŠ¥ í´ë˜ìŠ¤
            if class_performance:
                sorted_by_f1 = sorted(class_performance, key=lambda x: x['f1_score'], reverse=True)
                best_classes = sorted_by_f1[:5]
                worst_classes = sorted_by_f1[-5:]
            else:
                best_classes = []
                worst_classes = []
            
            detailed_report = {
                'classification_report': report_dict,
                'class_performance': class_performance,
                'best_performing_classes': best_classes,
                'worst_performing_classes': worst_classes,
                'overall_stats': {
                    'macro_avg': report_dict.get('macro avg', {}),
                    'weighted_avg': report_dict.get('weighted avg', {}),
                    'accuracy': report_dict.get('accuracy', 0.0)
                }
            }
            
            return detailed_report
            
        except Exception as e:
            self.logger.error(f"ë¶„ë¥˜ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def evaluate_stage1_target_achievement(self, metrics: ClassificationMetrics) -> Dict[str, bool]:
        """Stage 1 ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í‰ê°€"""
        
        # Stage 1 ëª©í‘œ ê¸°ì¤€
        STAGE1_TARGETS = {
            'classification_accuracy': 0.40,  # 40%
            'f1_macro': 0.35,                 # 35% (ë³´ì¡° ì§€í‘œ)
            'top5_accuracy': 0.70             # 70% (ê´€ëŒ€í•œ ê¸°ì¤€)
        }
        
        try:
            achievement = {}
            
            # ë¶„ë¥˜ ì •í™•ë„ ëª©í‘œ
            accuracy_achieved = metrics.top1_accuracy >= STAGE1_TARGETS['classification_accuracy']
            achievement['classification_accuracy_target_met'] = accuracy_achieved
            
            # F1 ì ìˆ˜ ëª©í‘œ
            f1_achieved = metrics.f1_macro >= STAGE1_TARGETS['f1_macro']
            achievement['f1_macro_target_met'] = f1_achieved
            
            # Top-5 ì •í™•ë„ ëª©í‘œ
            top5_achieved = metrics.top5_accuracy >= STAGE1_TARGETS['top5_accuracy']
            achievement['top5_accuracy_target_met'] = top5_achieved
            
            # ì „ì²´ ëª©í‘œ ë‹¬ì„±
            all_achieved = accuracy_achieved and f1_achieved
            achievement['stage1_classification_completed'] = all_achieved
            
            # ê²°ê³¼ ë¡œê¹…
            self.logger.info("ğŸ¯ Stage 1 ë¶„ë¥˜ ëª©í‘œ ë‹¬ì„± í‰ê°€:")
            self.logger.info(f"  ë¶„ë¥˜ ì •í™•ë„: {metrics.top1_accuracy:.1%} "
                           f"(ëª©í‘œ: {STAGE1_TARGETS['classification_accuracy']:.1%}) "
                           f"{'âœ…' if accuracy_achieved else 'âŒ'}")
            self.logger.info(f"  F1 ë§¤í¬ë¡œ: {metrics.f1_macro:.1%} "
                           f"(ëª©í‘œ: {STAGE1_TARGETS['f1_macro']:.1%}) "
                           f"{'âœ…' if f1_achieved else 'âŒ'}")
            self.logger.info(f"  Top-5 ì •í™•ë„: {metrics.top5_accuracy:.1%} "
                           f"(ëª©í‘œ: {STAGE1_TARGETS['top5_accuracy']:.1%}) "
                           f"{'âœ…' if top5_achieved else 'âŒ'}")
            
            if all_achieved:
                self.logger.success("ğŸ‰ Stage 1 ë¶„ë¥˜ ëª©í‘œ ë‹¬ì„±!")
            else:
                self.logger.warning("âš ï¸ Stage 1 ë¶„ë¥˜ ëª©í‘œ ë¯¸ë‹¬ì„± - ì¶”ê°€ í•™ìŠµ í•„ìš”")
            
            return achievement
            
        except Exception as e:
            self.logger.error(f"ëª©í‘œ ë‹¬ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
            return {}
    
    def save_evaluation_report(
        self, 
        metrics: ClassificationMetrics, 
        detailed_report: Optional[Dict] = None
    ) -> str:
        """í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥"""
        
        try:
            report_dir = Path("artifacts/reports/validation_results")
            report_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            report_file = report_dir / f"classification_evaluation_{timestamp}.json"
            
            # ë¦¬í¬íŠ¸ ë°ì´í„° êµ¬ì„±
            report_data = {
                'timestamp': timestamp,
                'evaluation_type': 'classification_metrics',
                'stage': 1,
                'metrics': {
                    'top1_accuracy': metrics.top1_accuracy,
                    'top5_accuracy': metrics.top5_accuracy,
                    'precision_macro': metrics.precision_macro,
                    'recall_macro': metrics.recall_macro,
                    'f1_macro': metrics.f1_macro,
                    'precision_weighted': metrics.precision_weighted,
                    'recall_weighted': metrics.recall_weighted,
                    'f1_weighted': metrics.f1_weighted,
                    'auc_macro': metrics.auc_macro,
                    'auc_weighted': metrics.auc_weighted
                },
                'metadata': {
                    'num_classes': metrics.num_classes,
                    'num_samples': metrics.num_samples,
                    'evaluation_time_seconds': metrics.evaluation_time_seconds
                },
                'target_achievement': self.evaluate_stage1_target_achievement(metrics)
            }
            
            # ìƒì„¸ ë¦¬í¬íŠ¸ ì¶”ê°€
            if detailed_report:
                report_data['detailed_analysis'] = detailed_report
            
            # ì €ì¥
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ë¶„ë¥˜ í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
            return str(report_file)
            
        except Exception as e:
            self.logger.error(f"ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""


def main():
    """ë¶„ë¥˜ ë©”íŠ¸ë¦­ í‰ê°€ í…ŒìŠ¤íŠ¸"""
    print("ğŸ“Š Classification Metrics Evaluator Test")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
    num_classes = 50
    num_samples = 1000
    
    evaluator = ClassificationMetricsEvaluator(num_classes=num_classes)
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    torch.manual_seed(42)
    y_true = torch.randint(0, num_classes, (num_samples,))
    y_pred_logits = torch.randn(num_samples, num_classes)
    
    # ì¼ë¶€ ì˜ˆì¸¡ì„ ì˜ë„ì ìœ¼ë¡œ ë§ì¶¤ (40% ì •í™•ë„ ì‹œë®¬ë ˆì´ì…˜)
    correct_mask = torch.rand(num_samples) < 0.4
    y_pred_logits[correct_mask, y_true[correct_mask]] += 3.0
    
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {num_samples}ê°œ ìƒ˜í”Œ, {num_classes}ê°œ í´ë˜ìŠ¤")
    
    # í‰ê°€ ìˆ˜í–‰
    metrics = evaluator.evaluate_predictions(y_true, y_pred_logits)
    
    print(f"\nğŸ“ˆ í‰ê°€ ê²°ê³¼:")
    print(f"  Top-1 ì •í™•ë„: {metrics.top1_accuracy:.1%}")
    print(f"  Top-5 ì •í™•ë„: {metrics.top5_accuracy:.1%}")
    print(f"  F1 ë§¤í¬ë¡œ: {metrics.f1_macro:.3f}")
    print(f"  F1 ê°€ì¤‘: {metrics.f1_weighted:.3f}")
    
    # ëª©í‘œ ë‹¬ì„± í‰ê°€
    achievement = evaluator.evaluate_stage1_target_achievement(metrics)
    stage1_completed = achievement.get('stage1_classification_completed', False)
    
    print(f"\nğŸ¯ Stage 1 ëª©í‘œ ë‹¬ì„±: {'âœ… ì„±ê³µ' if stage1_completed else 'âŒ ë¯¸ë‹¬ì„±'}")
    
    # ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
    y_pred_classes = torch.argmax(y_pred_logits, dim=1)
    detailed_report = evaluator.generate_classification_report(y_true, y_pred_classes)
    
    # í˜¼ë™ í–‰ë ¬ ìƒì„±
    cm = evaluator.generate_confusion_matrix(y_true, y_pred_classes)
    print(f"í˜¼ë™ í–‰ë ¬ í¬ê¸°: {cm.shape}")
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    report_file = evaluator.save_evaluation_report(metrics, detailed_report)
    if report_file:
        print(f"\nğŸ’¾ í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
    
    print("\nâœ… ë¶„ë¥˜ ë©”íŠ¸ë¦­ í‰ê°€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    main()