"""
End-to-End Pipeline Evaluator
ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ

Two-Stage Conditional Pipeline ì „ì²´ ì„±ëŠ¥ í‰ê°€:
- Single Mode: ì§ì ‘ ë¶„ë¥˜ ì„±ëŠ¥
- Combo Mode: ê²€ì¶œ â†’ ë¶„ë¥˜ ì„±ëŠ¥
- ì „ì²´ ì²˜ë¦¬ëŸ‰ ë° ì§€ì—°ì‹œê°„
- ìƒì—…ì  ì„œë¹„ìŠ¤ ì¤€ë¹„ë„ í‰ê°€
"""

import time
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass

import torch
import numpy as np
from PIL import Image

# from src.models.pipeline_two_stage_conditional import TwoStageConditionalPipeline
from src.evaluation.evaluate_classification_metrics import ClassificationMetricsEvaluator
from src.evaluation.evaluate_detection_metrics import DetectionMetricsEvaluator
from src.training.memory_monitor_gpu_usage import GPUMemoryMonitor
from src.utils.core import PillSnapLogger


@dataclass
class EndToEndMetrics:
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­"""
    
    # ì „ì²´ ì„±ëŠ¥
    overall_accuracy: float
    single_mode_accuracy: float
    combo_mode_accuracy: float
    
    # ê²€ì¶œ ì„±ëŠ¥ (Combo Mode)
    detection_map_50: float
    detection_precision: float
    detection_recall: float
    
    # ì²˜ë¦¬ëŸ‰ ë° ì§€ì—°ì‹œê°„
    avg_inference_time_ms: float
    throughput_images_per_sec: float
    p95_inference_time_ms: float
    
    # ìì› ì‚¬ìš©ëŸ‰
    peak_memory_usage_gb: float
    avg_memory_usage_gb: float
    
    # ëª¨ë“œë³„ ë¶„í¬
    single_mode_ratio: float
    combo_mode_ratio: float
    
    # ì—ëŸ¬ìœ¨
    preprocessing_error_rate: float
    inference_error_rate: float
    total_error_rate: float


@dataclass
class CommercialReadinessScore:
    """ìƒì—…ì  ì¤€ë¹„ë„ ì ìˆ˜"""
    
    accuracy_score: float       # ì •í™•ë„ ì ìˆ˜ (0-100)
    performance_score: float    # ì„±ëŠ¥ ì ìˆ˜ (0-100)
    reliability_score: float    # ì•ˆì •ì„± ì ìˆ˜ (0-100)
    scalability_score: float    # í™•ì¥ì„± ì ìˆ˜ (0-100)
    
    overall_score: float        # ì „ì²´ ì ìˆ˜ (0-100)
    readiness_level: str        # ì¤€ë¹„ ìˆ˜ì¤€ (Alpha/Beta/Production)
    
    recommendations: List[str]  # ê°œì„  ê¶Œì¥ì‚¬í•­


class EndToEndPipelineEvaluator:
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í‰ê°€ê¸°"""
    
    def __init__(self, pipeline=None):
        self.pipeline = pipeline
        self.logger = PillSnapLogger(__name__)
        
        # í‰ê°€ ë„êµ¬ë“¤
        self.memory_monitor = GPUMemoryMonitor()
        self.classification_evaluator = ClassificationMetricsEvaluator()
        self.detection_evaluator = DetectionMetricsEvaluator()
        
        # í‰ê°€ ë°ì´í„° ìˆ˜ì§‘
        self.evaluation_results = []
        self.error_log = []
        
        self.logger.info("EndToEndPipelineEvaluator ì´ˆê¸°í™” ì™„ë£Œ")
    
    def evaluate_complete_pipeline(
        self,
        test_images: List[Path],
        ground_truth_labels: List[int],
        mode_labels: List[str],  # 'single' or 'combo'
        batch_size: int = 16,
        save_detailed_results: bool = True
    ) -> EndToEndMetrics:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ì „ í‰ê°€"""
        
        self.logger.step("ì „ì²´ íŒŒì´í”„ë¼ì¸ í‰ê°€", f"{len(test_images)}ê°œ ì´ë¯¸ì§€ í‰ê°€")
        
        start_time = time.time()
        
        # ê²°ê³¼ ìˆ˜ì§‘ìš©
        single_results = []
        combo_results = []
        inference_times = []
        memory_usages = []
        errors = []
        
        try:
            # ë°°ì¹˜ë³„ í‰ê°€
            for i in range(0, len(test_images), batch_size):
                batch_images = test_images[i:i+batch_size]
                batch_labels = ground_truth_labels[i:i+batch_size]
                batch_modes = mode_labels[i:i+batch_size]
                
                # ë°°ì¹˜ ì²˜ë¦¬
                batch_results = self._evaluate_batch(
                    batch_images, batch_labels, batch_modes
                )
                
                # ê²°ê³¼ ë¶„ë¥˜
                for result in batch_results:
                    if result['mode'] == 'single':
                        single_results.append(result)
                    else:
                        combo_results.append(result)
                    
                    inference_times.append(result['inference_time_ms'])
                    memory_usages.append(result['memory_usage_gb'])
                    
                    if result['error']:
                        errors.append(result)
                
                # ì§„í–‰ ìƒí™© ë¡œê¹…
                progress = min(i + batch_size, len(test_images))
                self.logger.info(f"ì§„í–‰ë¥ : {progress}/{len(test_images)} ({progress/len(test_images):.1%})")
            
            # ì „ì²´ ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = self._calculate_end_to_end_metrics(
                single_results, combo_results, inference_times, memory_usages, errors
            )
            
            total_time = time.time() - start_time
            
            self.logger.success(f"ì „ì²´ íŒŒì´í”„ë¼ì¸ í‰ê°€ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
            self.logger.info(f"ì „ì²´ ì •í™•ë„: {metrics.overall_accuracy:.1%}")
            self.logger.info(f"í‰ê·  ì¶”ë¡  ì‹œê°„: {metrics.avg_inference_time_ms:.1f}ms")
            self.logger.info(f"ì²˜ë¦¬ëŸ‰: {metrics.throughput_images_per_sec:.1f} images/sec")
            
            # ìƒì„¸ ê²°ê³¼ ì €ì¥
            if save_detailed_results:
                self._save_detailed_evaluation_results(metrics, single_results, combo_results)
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"ì „ì²´ íŒŒì´í”„ë¼ì¸ í‰ê°€ ì‹¤íŒ¨: {e}")
            raise
    
    def _evaluate_batch(
        self,
        batch_images: List[Path],
        batch_labels: List[int],
        batch_modes: List[str]
    ) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ í‰ê°€"""
        
        batch_results = []
        
        for img_path, true_label, expected_mode in zip(batch_images, batch_labels, batch_modes):
            result = {
                'image_path': str(img_path),
                'true_label': true_label,
                'expected_mode': expected_mode,
                'predicted_label': -1,
                'predicted_mode': '',
                'confidence': 0.0,
                'inference_time_ms': 0.0,
                'memory_usage_gb': 0.0,
                'error': None,
                'detection_boxes': [],
                'mode': expected_mode
            }
            
            try:
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì‹œì‘
                memory_before = self.memory_monitor.get_current_usage()
                
                # ì¶”ë¡  ì‹œì‘
                inference_start = time.time()
                
                # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)
                inference_result = self._simulate_pipeline_inference(img_path, expected_mode)
                
                inference_time = (time.time() - inference_start) * 1000  # ms
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì¢…ë£Œ
                memory_after = self.memory_monitor.get_current_usage()
                
                # ê²°ê³¼ ì—…ë°ì´íŠ¸
                result.update({
                    'predicted_label': inference_result['predicted_class'],
                    'predicted_mode': inference_result['detected_mode'],
                    'confidence': inference_result['confidence'],
                    'inference_time_ms': inference_time,
                    'memory_usage_gb': memory_after['used_gb'],
                    'detection_boxes': inference_result.get('detection_boxes', [])
                })
                
            except Exception as e:
                result['error'] = str(e)
                self.logger.warning(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ {img_path}: {e}")
            
            batch_results.append(result)
        
        return batch_results
    
    def _simulate_pipeline_inference(self, image_path: Path, expected_mode: str) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì¶”ë¡  ì‹œë®¬ë ˆì´ì…˜"""
        import random
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” self.pipeline.predict(image_path) í˜¸ì¶œ
        
        if expected_mode == 'single':
            # Single Mode ì‹œë®¬ë ˆì´ì…˜
            return {
                'predicted_class': random.randint(0, 49),  # 50ê°œ í´ë˜ìŠ¤
                'detected_mode': 'single',
                'confidence': random.uniform(0.7, 0.95),
                'detection_boxes': []
            }
        else:
            # Combo Mode ì‹œë®¬ë ˆì´ì…˜
            return {
                'predicted_class': random.randint(0, 49),
                'detected_mode': 'combo',
                'confidence': random.uniform(0.6, 0.9),
                'detection_boxes': [
                    {'x': 0.3, 'y': 0.3, 'w': 0.4, 'h': 0.4, 'confidence': 0.8},
                    {'x': 0.6, 'y': 0.5, 'w': 0.3, 'h': 0.3, 'confidence': 0.7}
                ]
            }
    
    def _calculate_end_to_end_metrics(
        self,
        single_results: List[Dict],
        combo_results: List[Dict],
        inference_times: List[float],
        memory_usages: List[float],
        errors: List[Dict]
    ) -> EndToEndMetrics:
        """ì „ì²´ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        total_samples = len(single_results) + len(combo_results)
        
        if total_samples == 0:
            raise ValueError("í‰ê°€ ê²°ê³¼ê°€ ì—†ìŒ")
        
        # ì •í™•ë„ ê³„ì‚°
        single_correct = sum(1 for r in single_results 
                           if r['predicted_label'] == r['true_label'] and not r['error'])
        combo_correct = sum(1 for r in combo_results 
                          if r['predicted_label'] == r['true_label'] and not r['error'])
        
        total_correct = single_correct + combo_correct
        
        single_accuracy = single_correct / len(single_results) if single_results else 0.0
        combo_accuracy = combo_correct / len(combo_results) if combo_results else 0.0
        overall_accuracy = total_correct / total_samples
        
        # ê²€ì¶œ ì„±ëŠ¥ (Combo Modeë§Œ)
        detection_map = 0.32 if combo_results else 0.0  # ì‹œë®¬ë ˆì´ì…˜
        detection_precision = 0.36 if combo_results else 0.0
        detection_recall = 0.30 if combo_results else 0.0
        
        # ì²˜ë¦¬ëŸ‰ ë° ì§€ì—°ì‹œê°„
        if inference_times:
            avg_inference_time = np.mean(inference_times)
            p95_inference_time = np.percentile(inference_times, 95)
            throughput = 1000 / avg_inference_time  # images/sec
        else:
            avg_inference_time = 0.0
            p95_inference_time = 0.0
            throughput = 0.0
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        if memory_usages:
            peak_memory = max(memory_usages)
            avg_memory = np.mean(memory_usages)
        else:
            peak_memory = 0.0
            avg_memory = 0.0
        
        # ì—ëŸ¬ìœ¨
        preprocessing_errors = sum(1 for e in errors if 'preprocessing' in str(e.get('error', '')))
        inference_errors = sum(1 for e in errors if 'inference' in str(e.get('error', '')))
        
        preprocessing_error_rate = preprocessing_errors / total_samples
        inference_error_rate = inference_errors / total_samples
        total_error_rate = len(errors) / total_samples
        
        return EndToEndMetrics(
            overall_accuracy=overall_accuracy,
            single_mode_accuracy=single_accuracy,
            combo_mode_accuracy=combo_accuracy,
            detection_map_50=detection_map,
            detection_precision=detection_precision,
            detection_recall=detection_recall,
            avg_inference_time_ms=avg_inference_time,
            throughput_images_per_sec=throughput,
            p95_inference_time_ms=p95_inference_time,
            peak_memory_usage_gb=peak_memory,
            avg_memory_usage_gb=avg_memory,
            single_mode_ratio=len(single_results) / total_samples,
            combo_mode_ratio=len(combo_results) / total_samples,
            preprocessing_error_rate=preprocessing_error_rate,
            inference_error_rate=inference_error_rate,
            total_error_rate=total_error_rate
        )
    
    def evaluate_commercial_readiness(self, metrics: EndToEndMetrics) -> CommercialReadinessScore:
        """ìƒì—…ì  ì¤€ë¹„ë„ í‰ê°€"""
        
        self.logger.step("ìƒì—…ì  ì¤€ë¹„ë„ í‰ê°€", "ì •í™•ë„, ì„±ëŠ¥, ì•ˆì •ì„±, í™•ì¥ì„± í‰ê°€")
        
        # ì •í™•ë„ ì ìˆ˜ (0-100)
        accuracy_score = min(metrics.overall_accuracy * 100, 100)
        
        # ì„±ëŠ¥ ì ìˆ˜ (50ms ëª©í‘œ ê¸°ì¤€)
        target_latency = 50.0  # ms
        if metrics.avg_inference_time_ms <= target_latency:
            performance_score = 100
        elif metrics.avg_inference_time_ms <= target_latency * 2:
            performance_score = 100 - ((metrics.avg_inference_time_ms - target_latency) / target_latency * 50)
        else:
            performance_score = 25
        
        # ì•ˆì •ì„± ì ìˆ˜ (ì—ëŸ¬ìœ¨ ê¸°ì¤€)
        if metrics.total_error_rate <= 0.01:  # 1% ì´í•˜
            reliability_score = 100
        elif metrics.total_error_rate <= 0.05:  # 5% ì´í•˜
            reliability_score = 80
        elif metrics.total_error_rate <= 0.10:  # 10% ì´í•˜
            reliability_score = 60
        else:
            reliability_score = 30
        
        # í™•ì¥ì„± ì ìˆ˜ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ì²˜ë¦¬ëŸ‰ ê¸°ì¤€)
        if metrics.peak_memory_usage_gb <= 14.0 and metrics.throughput_images_per_sec >= 20:
            scalability_score = 100
        elif metrics.peak_memory_usage_gb <= 16.0 and metrics.throughput_images_per_sec >= 10:
            scalability_score = 80
        else:
            scalability_score = 50
        
        # ì „ì²´ ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
        overall_score = (
            accuracy_score * 0.4 +
            performance_score * 0.25 +
            reliability_score * 0.2 +
            scalability_score * 0.15
        )
        
        # ì¤€ë¹„ ìˆ˜ì¤€ ê²°ì •
        if overall_score >= 85:
            readiness_level = "Production Ready"
        elif overall_score >= 70:
            readiness_level = "Beta Ready"
        elif overall_score >= 50:
            readiness_level = "Alpha Ready"
        else:
            readiness_level = "Development"
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = []
        if accuracy_score < 80:
            recommendations.append("ì •í™•ë„ í–¥ìƒ: ì¶”ê°€ í•™ìŠµ ë°ì´í„° í™•ë³´ ë° ëª¨ë¸ íŠœë‹ í•„ìš”")
        if performance_score < 80:
            recommendations.append("ì„±ëŠ¥ ìµœì í™”: ëª¨ë¸ ê²½ëŸ‰í™” ë˜ëŠ” í•˜ë“œì›¨ì–´ ì—…ê·¸ë ˆì´ë“œ í•„ìš”")
        if reliability_score < 80:
            recommendations.append("ì•ˆì •ì„± ê°œì„ : ì—ëŸ¬ ì²˜ë¦¬ ê°•í™” ë° ì˜ˆì™¸ ìƒí™© ëŒ€ì‘ ë¡œì§ ë³´ì™„")
        if scalability_score < 80:
            recommendations.append("í™•ì¥ì„± ê°œì„ : ë©”ëª¨ë¦¬ ìµœì í™” ë° ë°°ì¹˜ ì²˜ë¦¬ ê°œì„ ")
        
        score = CommercialReadinessScore(
            accuracy_score=accuracy_score,
            performance_score=performance_score,
            reliability_score=reliability_score,
            scalability_score=scalability_score,
            overall_score=overall_score,
            readiness_level=readiness_level,
            recommendations=recommendations
        )
        
        # ê²°ê³¼ ë¡œê¹…
        self.logger.info(f"ğŸ¢ ìƒì—…ì  ì¤€ë¹„ë„ í‰ê°€ ê²°ê³¼:")
        self.logger.info(f"  ì •í™•ë„: {accuracy_score:.1f}/100")
        self.logger.info(f"  ì„±ëŠ¥: {performance_score:.1f}/100")
        self.logger.info(f"  ì•ˆì •ì„±: {reliability_score:.1f}/100")
        self.logger.info(f"  í™•ì¥ì„±: {scalability_score:.1f}/100")
        self.logger.info(f"  ì „ì²´ ì ìˆ˜: {overall_score:.1f}/100")
        self.logger.info(f"  ì¤€ë¹„ ìˆ˜ì¤€: {readiness_level}")
        
        return score
    
    def _save_detailed_evaluation_results(
        self,
        metrics: EndToEndMetrics,
        single_results: List[Dict],
        combo_results: List[Dict]
    ) -> str:
        """ìƒì„¸ í‰ê°€ ê²°ê³¼ ì €ì¥"""
        
        try:
            results_dir = Path("artifacts/reports/validation_results")
            results_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            results_file = results_dir / f"end_to_end_evaluation_{timestamp}.json"
            
            report_data = {
                'timestamp': timestamp,
                'evaluation_type': 'end_to_end_pipeline',
                'overall_metrics': {
                    'overall_accuracy': metrics.overall_accuracy,
                    'single_mode_accuracy': metrics.single_mode_accuracy,
                    'combo_mode_accuracy': metrics.combo_mode_accuracy,
                    'avg_inference_time_ms': metrics.avg_inference_time_ms,
                    'throughput_images_per_sec': metrics.throughput_images_per_sec,
                    'peak_memory_usage_gb': metrics.peak_memory_usage_gb,
                    'total_error_rate': metrics.total_error_rate
                },
                'mode_distribution': {
                    'single_mode_ratio': metrics.single_mode_ratio,
                    'combo_mode_ratio': metrics.combo_mode_ratio,
                    'single_samples': len(single_results),
                    'combo_samples': len(combo_results)
                },
                'detailed_results': {
                    'single_mode_results': single_results[:100],  # ì²˜ìŒ 100ê°œë§Œ ì €ì¥
                    'combo_mode_results': combo_results[:100]
                }
            }
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ìƒì„¸ í‰ê°€ ê²°ê³¼ ì €ì¥: {results_file}")
            return str(results_file)
            
        except Exception as e:
            self.logger.error(f"í‰ê°€ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""


def main():
    """End-to-End Pipeline Evaluator í…ŒìŠ¤íŠ¸"""
    print("ğŸ“Š End-to-End Pipeline Evaluator Test")
    print("=" * 60)
    
    try:
        # ë”ë¯¸ íŒŒì´í”„ë¼ì¸ (ì‹¤ì œë¡œëŠ” TwoStageConditionalPipeline ì¸ìŠ¤í„´ìŠ¤)
        pipeline = None  # ì‹œë®¬ë ˆì´ì…˜ìš©
        
        # í‰ê°€ê¸° ìƒì„±
        evaluator = EndToEndPipelineEvaluator(pipeline)
        
        # ë”ë¯¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°
        test_images = [Path(f"dummy_image_{i}.jpg") for i in range(100)]
        ground_truth_labels = [i % 50 for i in range(100)]  # 50ê°œ í´ë˜ìŠ¤
        mode_labels = ['single' if i % 3 != 0 else 'combo' for i in range(100)]  # 2:1 ë¹„ìœ¨
        
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_images)}ê°œ ì´ë¯¸ì§€")
        print(f"Single/Combo ë¹„ìœ¨: {mode_labels.count('single')}/{mode_labels.count('combo')}")
        
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ í‰ê°€
        metrics = evaluator.evaluate_complete_pipeline(
            test_images=test_images,
            ground_truth_labels=ground_truth_labels,
            mode_labels=mode_labels,
            batch_size=16,
            save_detailed_results=True
        )
        
        print(f"\nğŸ“ˆ ì „ì²´ íŒŒì´í”„ë¼ì¸ í‰ê°€ ê²°ê³¼:")
        print(f"  ì „ì²´ ì •í™•ë„: {metrics.overall_accuracy:.1%}")
        print(f"  Single Mode ì •í™•ë„: {metrics.single_mode_accuracy:.1%}")
        print(f"  Combo Mode ì •í™•ë„: {metrics.combo_mode_accuracy:.1%}")
        print(f"  í‰ê·  ì¶”ë¡  ì‹œê°„: {metrics.avg_inference_time_ms:.1f}ms")
        print(f"  ì²˜ë¦¬ëŸ‰: {metrics.throughput_images_per_sec:.1f} images/sec")
        print(f"  í”¼í¬ ë©”ëª¨ë¦¬: {metrics.peak_memory_usage_gb:.1f}GB")
        print(f"  ì—ëŸ¬ìœ¨: {metrics.total_error_rate:.1%}")
        
        # ìƒì—…ì  ì¤€ë¹„ë„ í‰ê°€
        readiness_score = evaluator.evaluate_commercial_readiness(metrics)
        
        print(f"\nğŸ¢ ìƒì—…ì  ì¤€ë¹„ë„ í‰ê°€:")
        print(f"  ì „ì²´ ì ìˆ˜: {readiness_score.overall_score:.1f}/100")
        print(f"  ì¤€ë¹„ ìˆ˜ì¤€: {readiness_score.readiness_level}")
        print(f"  ê¶Œì¥ì‚¬í•­: {len(readiness_score.recommendations)}ê°œ")
        
        print("\nâœ… End-to-End Pipeline í‰ê°€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()