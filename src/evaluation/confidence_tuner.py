"""
PillSnap ML Confidence ìë™ íŠœë‹ ì‹œìŠ¤í…œ (1ë‹¨ê³„ í•„ìˆ˜)

ìë™ ì„ê³„ê°’ íŠœë‹:
- ê²€ì¦ë§ˆë‹¤ conf âˆˆ [0.20..0.30], step=0.02 ìŠ¤ìœ• â†’ Recall ìš°ì„ , ë™ë¥  ì‹œ F1
- single/combination ë„ë©”ì¸ë³„ ìµœì ê°’ 1ê°œì”© ì„ íƒ
- ì„ íƒê°’ì„ ì¦‰ì‹œ ì¶”ë¡ ì„¤ì •ì— ì£¼ì… + ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ + ìš”ì•½ ë¦¬í¬íŠ¸ì— ê¸°ë¡

RTX 5080 ìµœì í™”
"""

import json
import time
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass, asdict
from pathlib import Path

import torch
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix

from src.utils.core import PillSnapLogger


@dataclass
class ConfidenceTuningConfig:
    """Confidence íŠœë‹ ì„¤ì • (1ë‹¨ê³„ í•„ìˆ˜)"""
    
    # ìŠ¤ìœ• ë²”ìœ„ ì„¤ì •
    conf_min: float = 0.20
    conf_max: float = 0.30
    conf_step: float = 0.02
    
    # ì„ íƒ ê¸°ì¤€
    primary_metric: str = "recall"      # ìš°ì„ ìˆœìœ„: Recall
    secondary_metric: str = "f1"        # ë™ë¥ ì‹œ: F1
    
    # ë„ë©”ì¸ ë¶„ë¦¬
    evaluate_by_domain: bool = True     # ë„ë©”ì¸ë³„ ìµœì ê°’ ì„ íƒ
    domains: List[str] = None           # ê¸°ë³¸ê°’: ["single", "combination"]
    
    # ê²°ê³¼ ì €ì¥
    save_tuning_results: bool = True
    save_to_checkpoint: bool = True     # ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ì— ì €ì¥
    save_to_inference: bool = True      # ì¶”ë¡  ì„¤ì •ì— ì¦‰ì‹œ ë°˜ì˜
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    generate_summary_report: bool = True
    
    def __post_init__(self):
        if self.domains is None:
            self.domains = ["single", "combination"]


@dataclass
class ConfidenceResult:
    """Confidence íŠœë‹ ê²°ê³¼"""
    confidence: float
    domain: str
    metrics: Dict[str, float]
    sample_count: int
    
    def get_primary_score(self, config: ConfidenceTuningConfig) -> float:
        """ì£¼ìš” ë©”íŠ¸ë¦­ ì ìˆ˜ ë°˜í™˜"""
        return self.metrics.get(config.primary_metric, 0.0)
    
    def get_secondary_score(self, config: ConfidenceTuningConfig) -> float:
        """ë³´ì¡° ë©”íŠ¸ë¦­ ì ìˆ˜ ë°˜í™˜"""
        return self.metrics.get(config.secondary_metric, 0.0)


class ConfidenceTuner:
    """Confidence ìë™ íŠœë‹ ì‹œìŠ¤í…œ (1ë‹¨ê³„ í•„ìˆ˜)"""
    
    def __init__(
        self,
        config: ConfidenceTuningConfig,
        model: Optional[torch.nn.Module] = None,
        device: str = "cuda"
    ):
        """
        Args:
            config: Confidence íŠœë‹ ì„¤ì •
            model: í‰ê°€í•  ëª¨ë¸ (ì„ íƒì )
            device: ë””ë°”ì´ìŠ¤
        """
        self.config = config
        self.model = model
        self.device = device
        self.logger = PillSnapLogger(__name__)
        
        # ìŠ¤ìœ• ë²”ìœ„ ìƒì„±
        self.confidence_values = self._generate_confidence_range()
        
        # íŠœë‹ ê²°ê³¼ ì €ì¥
        self.tuning_history = []
        self.best_confidences = {}  # ë„ë©”ì¸ë³„ ìµœì  confidence
        
        self.logger.info(
            f"ğŸ¯ Confidence íŠœë„ˆ ì´ˆê¸°í™” - "
            f"ë²”ìœ„: {self.config.conf_min}-{self.config.conf_max} (step {self.config.conf_step}), "
            f"ë„ë©”ì¸: {self.config.domains}, "
            f"ê¸°ì¤€: {self.config.primary_metric} â†’ {self.config.secondary_metric}"
        )
    
    def _generate_confidence_range(self) -> List[float]:
        """Confidence ìŠ¤ìœ• ë²”ìœ„ ìƒì„±"""
        values = []
        conf = self.config.conf_min
        
        while conf <= self.config.conf_max + 1e-6:  # ë¶€ë™ì†Œìˆ˜ì  ì˜¤ì°¨ ê³ ë ¤
            values.append(round(conf, 3))
            conf += self.config.conf_step
        
        return values
    
    def tune_confidence(
        self,
        predictions: List[Dict[str, Any]],
        ground_truths: List[Dict[str, Any]],
        domain_masks: Optional[Dict[str, torch.Tensor]] = None
    ) -> Dict[str, float]:
        """
        Confidence ì„ê³„ê°’ ìë™ íŠœë‹
        
        Args:
            predictions: ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            ground_truths: ì •ë‹µ ë¦¬ìŠ¤íŠ¸
            domain_masks: ë„ë©”ì¸ë³„ ë§ˆìŠ¤í¬ (ì„ íƒì )
            
        Returns:
            Dict[str, float]: ë„ë©”ì¸ë³„ ìµœì  confidence
        """
        self.logger.info(f"ğŸ” Confidence ìë™ íŠœë‹ ì‹œì‘ - {len(self.confidence_values)}ê°œ ê°’ ìŠ¤ìœ•")
        
        # ë„ë©”ì¸ë³„ íŠœë‹ ìˆ˜í–‰
        domain_results = {}
        
        for domain in self.config.domains:
            domain_preds, domain_gts = self._filter_by_domain(
                predictions, ground_truths, domain, domain_masks
            )
            
            if len(domain_preds) == 0:
                self.logger.warning(f"ë„ë©”ì¸ '{domain}' ìƒ˜í”Œì´ ì—†ìŒ - ê±´ë„ˆëœ€")
                continue
            
            best_result = self._tune_domain_confidence(domain, domain_preds, domain_gts)
            domain_results[domain] = best_result
            
            self.logger.info(
                f"âœ… {domain} ìµœì  confidence: {best_result.confidence} "
                f"({self.config.primary_metric}: {best_result.get_primary_score(self.config):.3f}, "
                f"{self.config.secondary_metric}: {best_result.get_secondary_score(self.config):.3f})"
            )
        
        # ìµœì  confidence ë”•ì…”ë„ˆë¦¬ ìƒì„±
        self.best_confidences = {
            domain: result.confidence 
            for domain, result in domain_results.items()
        }
        
        # ê²°ê³¼ ì €ì¥ ë° ì ìš©
        if self.config.save_tuning_results:
            self._save_tuning_results(domain_results)
        
        if self.config.save_to_inference:
            self._apply_to_inference_config()
            self._persist_to_inference_json()  # ì˜ì†í™” JSON ì €ì¥ ì¶”ê°€
        
        if self.config.generate_summary_report:
            self._generate_summary_report(domain_results)
        
        # ì²´í¬í¬ì¸íŠ¸ ë°˜ì˜ ìë™í™” (ë‚´ë¶€ì—ì„œ ìˆ˜í–‰)
        if self.config.save_to_checkpoint:
            self._auto_apply_to_checkpoint()
        
        self.logger.info(f"ğŸ¯ Confidence íŠœë‹ ì™„ë£Œ: {self.best_confidences}")
        
        return self.best_confidences
    
    def _filter_by_domain(
        self,
        predictions: List[Dict[str, Any]],
        ground_truths: List[Dict[str, Any]],
        domain: str,
        domain_masks: Optional[Dict[str, torch.Tensor]] = None
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """ë„ë©”ì¸ë³„ í•„í„°ë§"""
        if not self.config.evaluate_by_domain or domain_masks is None:
            return predictions, ground_truths
        
        if domain not in domain_masks:
            return [], []
        
        mask = domain_masks[domain].cpu().numpy()
        
        filtered_preds = [pred for i, pred in enumerate(predictions) if i < len(mask) and mask[i]]
        filtered_gts = [gt for i, gt in enumerate(ground_truths) if i < len(mask) and mask[i]]
        
        return filtered_preds, filtered_gts
    
    def _tune_domain_confidence(
        self,
        domain: str,
        predictions: List[Dict[str, Any]],
        ground_truths: List[Dict[str, Any]]
    ) -> ConfidenceResult:
        """íŠ¹ì • ë„ë©”ì¸ì˜ confidence íŠœë‹"""
        best_result = None
        
        for conf in self.confidence_values:
            # í•´ë‹¹ confidenceë¡œ ì˜ˆì¸¡ í•„í„°ë§
            filtered_preds = self._apply_confidence_threshold(predictions, conf)
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = self._calculate_metrics(filtered_preds, ground_truths)
            
            # ê²°ê³¼ ìƒì„±
            result = ConfidenceResult(
                confidence=conf,
                domain=domain,
                metrics=metrics,
                sample_count=len(filtered_preds)
            )
            
            # ìµœì ê°’ ì„ íƒ (Recall ìš°ì„ , F1 ë³´ì¡°)
            if self._is_better_result(result, best_result):
                best_result = result
        
        return best_result
    
    def _apply_confidence_threshold(
        self,
        predictions: List[Dict[str, Any]],
        confidence: float
    ) -> List[Dict[str, Any]]:
        """Confidence ì„ê³„ê°’ ì ìš©"""
        filtered = []
        
        for pred in predictions:
            # ì˜ˆì¸¡ì—ì„œ confidence ì ìˆ˜ ì¶”ì¶œ
            pred_conf = pred.get('confidence', pred.get('score', 1.0))
            
            if pred_conf >= confidence:
                filtered.append(pred)
            else:
                # Confidence ë¯¸ë‹¬ì‹œ negative ì˜ˆì¸¡ìœ¼ë¡œ ì²˜ë¦¬
                negative_pred = pred.copy()
                negative_pred['predicted_class'] = -1  # ë˜ëŠ” background í´ë˜ìŠ¤
                negative_pred['confidence'] = pred_conf
                filtered.append(negative_pred)
        
        return filtered
    
    def _calculate_metrics(
        self,
        predictions: List[Dict[str, Any]],
        ground_truths: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if len(predictions) == 0 or len(ground_truths) == 0:
            return {
                'precision': 0.0,
                'recall': 0.0,
                'f1': 0.0,
                'accuracy': 0.0
            }
        
        # ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µê°’ ì¶”ì¶œ
        y_pred = [p.get('predicted_class', -1) for p in predictions]
        y_true = [gt.get('true_class', gt.get('label', -1)) for gt in ground_truths]
        
        # ê¸¸ì´ ë§ì¶¤
        min_len = min(len(y_pred), len(y_true))
        y_pred = y_pred[:min_len]
        y_true = y_true[:min_len]
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        try:
            precision, recall, f1, _ = precision_recall_fscore_support(
                y_true, y_pred, average='weighted', zero_division=0
            )
            
            accuracy = np.mean(np.array(y_pred) == np.array(y_true))
            
            return {
                'precision': float(precision),
                'recall': float(recall),
                'f1': float(f1),
                'accuracy': float(accuracy)
            }
        
        except Exception as e:
            self.logger.warning(f"ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {
                'precision': 0.0,
                'recall': 0.0,
                'f1': 0.0,
                'accuracy': 0.0
            }
    
    def _is_better_result(
        self,
        current: ConfidenceResult,
        best: Optional[ConfidenceResult]
    ) -> bool:
        """ê²°ê³¼ ë¹„êµ (Recall ìš°ì„ , F1 ë³´ì¡°)"""
        if best is None:
            return True
        
        current_primary = current.get_primary_score(self.config)
        best_primary = best.get_primary_score(self.config)
        
        # Primary ë©”íŠ¸ë¦­ ë¹„êµ
        if current_primary > best_primary:
            return True
        elif current_primary < best_primary:
            return False
        
        # Primaryê°€ ë™ì¼í•˜ë©´ Secondary ë©”íŠ¸ë¦­ ë¹„êµ
        current_secondary = current.get_secondary_score(self.config)
        best_secondary = best.get_secondary_score(self.config)
        
        return current_secondary > best_secondary
    
    def _save_tuning_results(self, domain_results: Dict[str, ConfidenceResult]) -> None:
        """íŠœë‹ ê²°ê³¼ ì €ì¥"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # ê²°ê³¼ ë°ì´í„° ì¤€ë¹„
        results_data = {
            "timestamp": timestamp,
            "config": asdict(self.config),
            "confidence_range": self.confidence_values,
            "domain_results": {
                domain: {
                    "confidence": result.confidence,
                    "metrics": result.metrics,
                    "sample_count": result.sample_count
                }
                for domain, result in domain_results.items()
            },
            "best_confidences": self.best_confidences
        }
        
        # ì €ì¥ ê²½ë¡œ
        save_dir = Path("artifacts/confidence_tuning")
        save_dir.mkdir(parents=True, exist_ok=True)
        save_file = save_dir / f"confidence_tuning_{timestamp}.json"
        
        # JSON ì €ì¥
        with open(save_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ğŸ’¾ Confidence íŠœë‹ ê²°ê³¼ ì €ì¥: {save_file}")
    
    def _apply_to_inference_config(self) -> None:
        """ì¶”ë¡  ì„¤ì •ì— ìµœì  confidence ì ìš© (í”„ë¡œì„¸ìŠ¤ ë‚´ ì£¼ì…)"""
        try:
            # í˜„ì¬ ì„¤ì • ë¡œë“œ
            from src.utils.core import load_config
            config = load_config()
            
            # Inference ì„¹ì…˜ì— ìµœì  confidence ì£¼ì…
            if 'inference' not in config:
                config['inference'] = {}
            
            config['inference']['optimal_confidences'] = self.best_confidences
            config['inference']['confidence_auto_tuned'] = True
            config['inference']['tuning_timestamp'] = time.strftime("%Y-%m-%d %H:%M:%S")
            
            # ê¸°ë³¸ confidenceë„ ì—…ë°ì´íŠ¸ (single ë„ë©”ì¸ ìš°ì„ )
            if 'single' in self.best_confidences:
                config['inference']['confidence'] = self.best_confidences['single']
            
            # í•˜ë“œì½”ë”© ì„ê³„ ë¬´ì‹œ ì¼ê´€í™”
            self._override_hardcoded_thresholds(config)
            
            self.logger.info(f"âš™ï¸ ì¶”ë¡  ì„¤ì •ì— ìµœì  confidence ì ìš©: {self.best_confidences}")
            
        except Exception as e:
            self.logger.error(f"ì¶”ë¡  ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _persist_to_inference_json(self) -> None:
        """ì„ íƒëœ ë„ë©”ì¸ë³„ confidenceë¥¼ JSONì— ì˜ì† ì €ì¥"""
        try:
            import time
            from datetime import datetime
            
            # artifacts/confidence_tuning ë””ë ‰í„°ë¦¬ ìƒì„±
            save_dir = Path("artifacts/confidence_tuning")
            save_dir.mkdir(parents=True, exist_ok=True)
            
            # ì˜ì†í™” ë°ì´í„° ì¤€ë¹„
            persist_data = {
                "single": self.best_confidences.get("single", 0.25),
                "combination": self.best_confidences.get("combination", 0.25),
                "epoch": getattr(self, 'current_epoch', 0),  # í˜„ì¬ ì—í¬í¬
                "timestamp": datetime.now().isoformat(),
                "tuning_config": {
                    "conf_min": self.config.conf_min,
                    "conf_max": self.config.conf_max,
                    "conf_step": self.config.conf_step,
                    "primary_metric": self.config.primary_metric,
                    "secondary_metric": self.config.secondary_metric
                }
            }
            
            # JSON ì €ì¥
            json_file = save_dir / "last_selected.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(persist_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"ğŸ’¾ ì„ íƒëœ confidence ì˜ì† ì €ì¥: {json_file}")
            
        except Exception as e:
            self.logger.error(f"Confidence ì˜ì†í™” ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _override_hardcoded_thresholds(self, config: Dict[str, Any]) -> None:
        """í•˜ë“œì½”ë”© ì„ê³„ ë¬´ì‹œ ì¼ê´€í™”"""
        # í•˜ë“œì½”ë”©ëœ confidence threshold í‚¤ë“¤
        hardcoded_keys = [
            ("train", "detection", "evaluation", "conf_threshold"),
            ("inference", "detection_nms", "confidence_threshold"),
            ("models", "detector", "confidence_threshold")
        ]
        
        overridden_keys = []
        
        for key_path in hardcoded_keys:
            # ì¤‘ì²© í‚¤ ê²½ë¡œ íƒìƒ‰
            current = config
            valid_path = True
            
            for key in key_path[:-1]:
                if key in current and isinstance(current[key], dict):
                    current = current[key]
                else:
                    valid_path = False
                    break
            
            # ë§ˆì§€ë§‰ í‚¤ ì²˜ë¦¬
            if valid_path and key_path[-1] in current:
                old_value = current[key_path[-1]]
                
                # single ë„ë©”ì¸ ê°’ìœ¼ë¡œ ì˜¤ë²„ë¼ì´ë“œ
                if 'single' in self.best_confidences:
                    current[key_path[-1]] = self.best_confidences['single']
                    overridden_keys.append(f"{.".join(key_path)}: {old_value} â†’ {self.best_confidences['single']}")
        
        if overridden_keys:
            self.logger.info(
                f"ğŸ”„ Using tuned confidence; overriding hard-coded thresholds:\n  " +
                "\n  ".join(overridden_keys)
            )
    
    def _auto_apply_to_checkpoint(self) -> None:
        """ì²´í¬í¬ì¸íŠ¸ ë°˜ì˜ ìë™í™” (ë‚´ë¶€ì—ì„œ ìˆ˜í–‰)"""
        # ì´ ë©”ì„œë“œëŠ” ì²´í¬í¬ì¸íŠ¸ê°€ ì œê³µë˜ì—ˆì„ ë•Œë§Œ ì‘ë™í•˜ë„ë¡ ì„¤ê³„
        # ì™¸ë¶€ í˜¸ì¶œ ì˜ì¡´ ì œê±°ë¥¼ ìœ„í•´ ë‚´ë¶€ì—ì„œ ì²´í¬í¬ì¸íŠ¸ ì²˜ë¦¬ ì‹œë„
        try:
            # í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ì— ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
            if hasattr(self, '_current_checkpoint') and self._current_checkpoint:
                self.apply_to_checkpoint(self._current_checkpoint)
            else:
                # ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ì•ˆì „í•˜ê²Œ ìŠ¤í‚µ
                self.logger.debug("ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ - ìë™ ì ìš© ìŠ¤í‚µ")
        
        except Exception as e:
            self.logger.warning(f"ì²´í¬í¬ì¸íŠ¸ ìë™ ë°˜ì˜ ì‹¤íŒ¨: {e}")
    
    def set_current_checkpoint(self, checkpoint: Dict[str, Any]) -> None:
        """í˜„ì¬ ì²´í¬í¬ì¸íŠ¸ ì„¤ì • (ë‚´ë¶€ ìë™ ì ìš©ìš©)"""
        self._current_checkpoint = checkpoint
    
    def load_persisted_confidences(self) -> Optional[Dict[str, float]]:
        """ì˜ì†í™”ëœ confidence ê°’ ë¡œë“œ (ì¶”ë¡  ì´ˆê¸°í™”ì‹œ ì‚¬ìš©)"""
        try:
            json_file = Path("artifacts/confidence_tuning/last_selected.json")
            
            if not json_file.exists():
                return None
            
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ë„ë©”ì¸ë³„ confidence ê°’ ì¶”ì¶œ
            confidences = {
                "single": data.get("single", 0.25),
                "combination": data.get("combination", 0.25)
            }
            
            self.logger.info(f"ğŸ’¼ ì˜ì†í™”ëœ confidence ë¡œë“œ: {confidences}")
            return confidences
        
        except Exception as e:
            self.logger.warning(f"ì˜ì†í™”ëœ confidence ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_summary_report(self, domain_results: Dict[str, ConfidenceResult]) -> None:
        """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_lines = [
            "# Confidence ìë™ íŠœë‹ ë¦¬í¬íŠ¸",
            "",
            f"**íŠœë‹ ì‹œê°„**: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"**ìŠ¤ìœ• ë²”ìœ„**: {self.config.conf_min} - {self.config.conf_max} (step {self.config.conf_step})",
            f"**ì„ íƒ ê¸°ì¤€**: {self.config.primary_metric} (ìš°ì„ ), {self.config.secondary_metric} (ë³´ì¡°)",
            "",
            "## ë„ë©”ì¸ë³„ ìµœì  Confidence",
            ""
        ]
        
        for domain, result in domain_results.items():
            report_lines.extend([
                f"### {domain.upper()}",
                f"- **ìµœì  Confidence**: {result.confidence}",
                f"- **ìƒ˜í”Œ ìˆ˜**: {result.sample_count}",
                f"- **Precision**: {result.metrics.get('precision', 0):.3f}",
                f"- **Recall**: {result.metrics.get('recall', 0):.3f}",
                f"- **F1 Score**: {result.metrics.get('f1', 0):.3f}",
                f"- **Accuracy**: {result.metrics.get('accuracy', 0):.3f}",
                ""
            ])
        
        # ì¶”ê°€ ë¦¬í¬íŠ¸ ì •ë³´
        report_lines.extend([
            "## íŠœë‹ ì„¸ë¶€ ì„¤ì •",
            f"- **ìŠ¤ìœ™ ë²”ìœ„**: {self.config.conf_min} - {self.config.conf_max}",
            f"- **ìŠ¤ìœ™ ìŠ¤í…**: {self.config.conf_step}",
            f"- **ì„ ì • ê¸°ì¤€**: {self.config.primary_metric} (ìš°ì„ ) â†’ {self.config.secondary_metric} (ë³´ì¡°)",
            f"- **ë„ë©”ì¸**: {', '.join(self.config.domains)}",
            "",
            "## ì ìš© ìƒíƒœ",
            "- âœ… **ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€**: ìë™ ë°˜ì˜",
            "- âœ… **ì¶”ë¡  ì„¤ì •**: ì˜ì† JSON ì €ì¥",
            "- âœ… **ë¦¬í¬íŠ¸**: ë„ë©”ì¸ë³„ ì„ íƒê°’ ë° ìŠ¤ìœ™ ë‚´ì—­ ëª…ì‹œ",
            ""
        ])
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_dir = Path("artifacts/reports")
        report_dir.mkdir(parents=True, exist_ok=True)
        report_file = report_dir / f"confidence_tuning_{time.strftime('%Y%m%d_%H%M%S')}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        self.logger.info(f"ğŸ“‹ Confidence íŠœë‹ ë¦¬í¬íŠ¸ ìƒì„±: {report_file}")
    
    def get_best_confidences(self) -> Dict[str, float]:
        """ìµœì  confidence ë°˜í™˜"""
        return self.best_confidences.copy()
    
    def apply_to_checkpoint(self, checkpoint: Dict[str, Any]) -> Dict[str, Any]:
        """ì²´í¬í¬ì¸íŠ¸ì— ìµœì  confidence ë©”íƒ€ ì¶”ê°€"""
        if checkpoint is None:
            self.logger.warning("ì²´í¬í¬ì¸íŠ¸ê°€ Noneì„ - ì•ˆì „í•˜ê²Œ ìŠ¤í‚µ")
            return {}
        
        if not self.config.save_to_checkpoint:
            return checkpoint
        
        if 'meta' not in checkpoint:
            checkpoint['meta'] = {}
        
        checkpoint['meta']['optimal_confidences'] = self.best_confidences
        checkpoint['meta']['confidence_tuning_timestamp'] = time.time()
        checkpoint['meta']['confidence_tuning_config'] = asdict(self.config)
        
        # í•˜ë“œì½”ë”© ì„ê³„ ì˜¤ë²„ë¼ì´ë“œ ì •ë³´ ì¶”ê°€
        checkpoint['meta']['hardcoded_thresholds_overridden'] = True
        checkpoint['meta']['override_keys'] = [
            "train.detection.evaluation.conf_threshold",
            "inference.detection_nms.confidence_threshold",
            "models.detector.confidence_threshold"
        ]
        
        self.logger.info("ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ì— ìµœì  confidence ë©”íƒ€ ì¶”ê°€ (ìë™ ìˆ˜í–‰)")
        
        return checkpoint


if __name__ == "__main__":
    print("ğŸ§ª Confidence ìë™ íŠœë‹ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (1ë‹¨ê³„ í•„ìˆ˜)")
    print("=" * 60)
    
    # ì„¤ì • í…ŒìŠ¤íŠ¸
    config = ConfidenceTuningConfig(
        conf_min=0.20,
        conf_max=0.30,
        conf_step=0.02,
        domains=["single", "combination"]
    )
    
    tuner = ConfidenceTuner(config)
    print(f"âœ… Confidence ë²”ìœ„: {tuner.confidence_values}")
    
    # Mock ë°ì´í„° ìƒì„±
    import random
    
    mock_predictions = []
    mock_ground_truths = []
    
    for i in range(100):
        # Mock prediction
        conf = random.uniform(0.1, 0.9)
        pred_class = random.randint(0, 10) if conf > 0.25 else -1
        
        mock_predictions.append({
            'confidence': conf,
            'predicted_class': pred_class
        })
        
        # Mock ground truth
        true_class = random.randint(0, 10)
        mock_ground_truths.append({
            'true_class': true_class
        })
    
    # Mock ë„ë©”ì¸ ë§ˆìŠ¤í¬
    mock_domain_masks = {
        'single': torch.tensor([i < 75 for i in range(100)]),  # 75% single
        'combination': torch.tensor([i >= 75 for i in range(100)])  # 25% combination
    }
    
    # íŠœë‹ ì‹¤í–‰
    best_confidences = tuner.tune_confidence(
        mock_predictions,
        mock_ground_truths,
        mock_domain_masks
    )
    
    print(f"âœ… ìµœì  confidence: {best_confidences}")
    
    # ì²´í¬í¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
    mock_checkpoint = {'model_state_dict': {}, 'epoch': 10}
    updated_checkpoint = tuner.apply_to_checkpoint(mock_checkpoint)
    print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ ì¶”ê°€: {'optimal_confidences' in updated_checkpoint.get('meta', {})}")
    
    print("ğŸ‰ Confidence ìë™ íŠœë‹ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")