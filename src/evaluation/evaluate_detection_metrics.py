"""
Detection Metrics Evaluator
ê²€ì¶œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ í‰ê°€ ì‹œìŠ¤í…œ

YOLOv11m ê²€ì¶œ ì„±ëŠ¥ í‰ê°€:
- mAP@0.5, mAP@0.5:0.95
- Precision, Recall, F1-Score
- í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
- Stageë³„ ëª©í‘œ ë‹¬ì„± ê²€ì¦
"""

import time
import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass

import torch
import matplotlib.pyplot as plt
import seaborn as sns

from src.utils.core import PillSnapLogger


@dataclass
class DetectionMetrics:
    """ê²€ì¶œ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    
    # ì£¼ìš” mAP ë©”íŠ¸ë¦­
    map_50: float              # mAP@0.5
    map_50_95: float           # mAP@0.5:0.95
    
    # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­
    precision: float           # í‰ê·  ì •ë°€ë„
    recall: float              # í‰ê·  ì¬í˜„ìœ¨
    f1_score: float            # F1 ì ìˆ˜
    
    # ì„¸ë¶€ ë©”íŠ¸ë¦­
    ap_per_class: List[float]  # í´ë˜ìŠ¤ë³„ AP
    precision_per_class: List[float]
    recall_per_class: List[float]
    
    # ë©”íƒ€ë°ì´í„°
    num_classes: int = 1
    num_images: int = 0
    evaluation_time_seconds: float = 0.0


class DetectionMetricsEvaluator:
    """ê²€ì¶œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ í‰ê°€ê¸°"""
    
    def __init__(self, num_classes: int = 1, class_names: Optional[List[str]] = None):
        self.num_classes = num_classes
        self.class_names = class_names or [f"Class_{i}" for i in range(num_classes)]
        self.logger = PillSnapLogger(__name__)
        
        self.logger.info(f"ê²€ì¶œ ë©”íŠ¸ë¦­ í‰ê°€ê¸° ì´ˆê¸°í™”: {num_classes}ê°œ í´ë˜ìŠ¤")
    
    def evaluate_yolo_results(
        self, 
        results_dict: Dict[str, Any]
    ) -> DetectionMetrics:
        """YOLO ê²°ê³¼ í‰ê°€"""
        
        start_time = time.time()
        
        try:
            # YOLO ê²°ê³¼ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ
            metrics_data = results_dict.get('metrics', {})
            
            # ì£¼ìš” ë©”íŠ¸ë¦­
            map_50 = metrics_data.get('mAP_0.5', 0.0)
            map_50_95 = metrics_data.get('mAP_0.5:0.95', 0.0)
            precision = metrics_data.get('precision', 0.0)
            recall = metrics_data.get('recall', 0.0)
            
            # F1 ì ìˆ˜ ê³„ì‚°
            if precision + recall > 0:
                f1_score = 2 * (precision * recall) / (precision + recall)
            else:
                f1_score = 0.0
            
            # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­ (ë‹¨ì¼ í´ë˜ìŠ¤ì˜ ê²½ìš°)
            ap_per_class = [map_50] * self.num_classes
            precision_per_class = [precision] * self.num_classes
            recall_per_class = [recall] * self.num_classes
            
            # ë©”íŠ¸ë¦­ ê°ì²´ ìƒì„±
            detection_metrics = DetectionMetrics(
                map_50=map_50,
                map_50_95=map_50_95,
                precision=precision,
                recall=recall,
                f1_score=f1_score,
                ap_per_class=ap_per_class,
                precision_per_class=precision_per_class,
                recall_per_class=recall_per_class,
                num_classes=self.num_classes,
                num_images=results_dict.get('num_images', 0),
                evaluation_time_seconds=time.time() - start_time
            )
            
            # ê²°ê³¼ ë¡œê¹…
            self.logger.info(f"ê²€ì¶œ í‰ê°€ ì™„ë£Œ ({detection_metrics.num_images}ê°œ ì´ë¯¸ì§€)")
            self.logger.metric("mAP_0.5", map_50)
            self.logger.metric("mAP_0.5:0.95", map_50_95)
            self.logger.metric("precision", precision)
            self.logger.metric("recall", recall)
            self.logger.metric("f1_score", f1_score)
            
            return detection_metrics
            
        except Exception as e:
            self.logger.error(f"ê²€ì¶œ í‰ê°€ ì‹¤íŒ¨: {e}")
            raise
    
    def evaluate_stage_target_achievement(
        self, 
        metrics: DetectionMetrics,
        stage: int = 1
    ) -> Dict[str, bool]:
        """Stageë³„ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í‰ê°€"""
        
        # Stageë³„ ëª©í‘œ ê¸°ì¤€
        STAGE_TARGETS = {
            1: {'map_50': 0.30, 'precision': 0.35, 'recall': 0.30},
            2: {'map_50': 0.50, 'precision': 0.55, 'recall': 0.50},
            3: {'map_50': 0.70, 'precision': 0.75, 'recall': 0.70},
            4: {'map_50': 0.85, 'precision': 0.90, 'recall': 0.85}
        }
        
        targets = STAGE_TARGETS.get(stage, STAGE_TARGETS[1])
        
        try:
            achievement = {}
            
            # mAP@0.5 ëª©í‘œ
            map_achieved = metrics.map_50 >= targets['map_50']
            achievement['map_50_target_met'] = map_achieved
            
            # ì •ë°€ë„ ëª©í‘œ
            precision_achieved = metrics.precision >= targets['precision']
            achievement['precision_target_met'] = precision_achieved
            
            # ì¬í˜„ìœ¨ ëª©í‘œ
            recall_achieved = metrics.recall >= targets['recall']
            achievement['recall_target_met'] = recall_achieved
            
            # ì „ì²´ ëª©í‘œ ë‹¬ì„±
            all_achieved = map_achieved and precision_achieved and recall_achieved
            achievement[f'stage{stage}_detection_completed'] = all_achieved
            
            # ê²°ê³¼ ë¡œê¹…
            self.logger.info(f"ğŸ¯ Stage {stage} ê²€ì¶œ ëª©í‘œ ë‹¬ì„± í‰ê°€:")
            self.logger.info(f"  mAP@0.5: {metrics.map_50:.3f} "
                           f"(ëª©í‘œ: {targets['map_50']:.3f}) "
                           f"{'âœ…' if map_achieved else 'âŒ'}")
            self.logger.info(f"  ì •ë°€ë„: {metrics.precision:.3f} "
                           f"(ëª©í‘œ: {targets['precision']:.3f}) "
                           f"{'âœ…' if precision_achieved else 'âŒ'}")
            self.logger.info(f"  ì¬í˜„ìœ¨: {metrics.recall:.3f} "
                           f"(ëª©í‘œ: {targets['recall']:.3f}) "
                           f"{'âœ…' if recall_achieved else 'âŒ'}")
            
            if all_achieved:
                self.logger.success(f"ğŸ‰ Stage {stage} ê²€ì¶œ ëª©í‘œ ë‹¬ì„±!")
            else:
                self.logger.warning(f"âš ï¸ Stage {stage} ê²€ì¶œ ëª©í‘œ ë¯¸ë‹¬ì„± - ì¶”ê°€ í•™ìŠµ í•„ìš”")
            
            return achievement
            
        except Exception as e:
            self.logger.error(f"ëª©í‘œ ë‹¬ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
            return {}
    
    def generate_detection_report(
        self, 
        metrics: DetectionMetrics,
        save_plots: bool = True
    ) -> Dict[str, Any]:
        """ìƒì„¸ ê²€ì¶œ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        try:
            # ì„±ëŠ¥ ë¶„ì„
            performance_analysis = {
                'overall_performance': {
                    'map_50': metrics.map_50,
                    'map_50_95': metrics.map_50_95,
                    'precision': metrics.precision,
                    'recall': metrics.recall,
                    'f1_score': metrics.f1_score
                },
                'class_performance': []
            }
            
            # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ (ë‹¨ì¼ í´ë˜ìŠ¤ì˜ ê²½ìš° ë‹¨ìˆœí™”)
            for i in range(self.num_classes):
                class_perf = {
                    'class_id': i,
                    'class_name': self.class_names[i],
                    'ap': metrics.ap_per_class[i] if i < len(metrics.ap_per_class) else 0.0,
                    'precision': metrics.precision_per_class[i] if i < len(metrics.precision_per_class) else 0.0,
                    'recall': metrics.recall_per_class[i] if i < len(metrics.recall_per_class) else 0.0
                }
                performance_analysis['class_performance'].append(class_perf)
            
            # ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€
            if metrics.map_50 >= 0.80:
                performance_grade = "ìš°ìˆ˜ (Excellent)"
            elif metrics.map_50 >= 0.60:
                performance_grade = "ì–‘í˜¸ (Good)"
            elif metrics.map_50 >= 0.40:
                performance_grade = "ë³´í†µ (Average)"
            elif metrics.map_50 >= 0.20:
                performance_grade = "ë¯¸í¡ (Below Average)"
            else:
                performance_grade = "ë¶€ì¡± (Poor)"
            
            # ê°œì„  ê¶Œì¥ì‚¬í•­
            recommendations = []
            if metrics.precision < 0.70:
                recommendations.append("False Positive ê°ì†Œë¥¼ ìœ„í•œ NMS ì„ê³„ê°’ ì¡°ì •")
            if metrics.recall < 0.70:
                recommendations.append("False Negative ê°ì†Œë¥¼ ìœ„í•œ Confidence ì„ê³„ê°’ í•˜í–¥ ì¡°ì •")
            if metrics.map_50 < 0.50:
                recommendations.append("ë°ì´í„° ì¦ê°• ë° ì¶”ê°€ í•™ìŠµ ì—í¬í¬ ê¶Œì¥")
            
            detailed_report = {
                'performance_analysis': performance_analysis,
                'performance_grade': performance_grade,
                'recommendations': recommendations,
                'metadata': {
                    'num_classes': metrics.num_classes,
                    'num_images': metrics.num_images,
                    'evaluation_time_seconds': metrics.evaluation_time_seconds
                }
            }
            
            # ì‹œê°í™” ìƒì„±
            if save_plots:
                plot_files = self._create_detection_plots(metrics)
                detailed_report['plot_files'] = plot_files
            
            return detailed_report
            
        except Exception as e:
            self.logger.error(f"ê²€ì¶œ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def _create_detection_plots(self, metrics: DetectionMetrics) -> List[str]:
        """ê²€ì¶œ ì„±ëŠ¥ ì‹œê°í™”"""
        plot_files = []
        
        try:
            plot_dir = Path("artifacts/reports/validation_results")
            plot_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            
            # 1. ë©”íŠ¸ë¦­ ë§‰ëŒ€ ê·¸ë˜í”„
            plt.figure(figsize=(10, 6))
            
            metrics_names = ['mAP@0.5', 'mAP@0.5:0.95', 'Precision', 'Recall', 'F1-Score']
            metrics_values = [
                metrics.map_50, metrics.map_50_95, 
                metrics.precision, metrics.recall, metrics.f1_score
            ]
            
            bars = plt.bar(metrics_names, metrics_values, 
                          color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])
            
            # ê°’ í‘œì‹œ
            for bar, value in zip(bars, metrics_values):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{value:.3f}', ha='center', va='bottom')
            
            plt.title('Detection Performance Metrics')
            plt.ylabel('Score')
            plt.ylim(0, 1.0)
            plt.grid(axis='y', alpha=0.3)
            
            plot_file = plot_dir / f"detection_metrics_bar_{timestamp}.png"
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            plot_files.append(str(plot_file))
            self.logger.info(f"ê²€ì¶œ ë©”íŠ¸ë¦­ ê·¸ë˜í”„ ì €ì¥: {plot_file}")
            
        except Exception as e:
            self.logger.warning(f"ê²€ì¶œ ì‹œê°í™” ì‹¤íŒ¨: {e}")
        
        return plot_files
    
    def save_evaluation_report(
        self, 
        metrics: DetectionMetrics,
        detailed_report: Optional[Dict] = None,
        stage: int = 1
    ) -> str:
        """í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥"""
        
        try:
            report_dir = Path("artifacts/reports/validation_results")
            report_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            report_file = report_dir / f"detection_evaluation_stage{stage}_{timestamp}.json"
            
            # ë¦¬í¬íŠ¸ ë°ì´í„° êµ¬ì„±
            report_data = {
                'timestamp': timestamp,
                'evaluation_type': 'detection_metrics',
                'stage': stage,
                'metrics': {
                    'map_50': metrics.map_50,
                    'map_50_95': metrics.map_50_95,
                    'precision': metrics.precision,
                    'recall': metrics.recall,
                    'f1_score': metrics.f1_score,
                    'ap_per_class': metrics.ap_per_class,
                    'precision_per_class': metrics.precision_per_class,
                    'recall_per_class': metrics.recall_per_class
                },
                'metadata': {
                    'num_classes': metrics.num_classes,
                    'num_images': metrics.num_images,
                    'evaluation_time_seconds': metrics.evaluation_time_seconds
                },
                'target_achievement': self.evaluate_stage_target_achievement(metrics, stage)
            }
            
            # ìƒì„¸ ë¦¬í¬íŠ¸ ì¶”ê°€
            if detailed_report:
                report_data['detailed_analysis'] = detailed_report
            
            # ì €ì¥
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ê²€ì¶œ í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
            return str(report_file)
            
        except Exception as e:
            self.logger.error(f"ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""


def main():
    """ê²€ì¶œ ë©”íŠ¸ë¦­ í‰ê°€ í…ŒìŠ¤íŠ¸"""
    print("ğŸ“Š Detection Metrics Evaluator Test")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
    evaluator = DetectionMetricsEvaluator(num_classes=1, class_names=["pill"])
    
    # ë”ë¯¸ YOLO ê²°ê³¼
    dummy_results = {
        'metrics': {
            'mAP_0.5': 0.32,
            'mAP_0.5:0.95': 0.22,
            'precision': 0.36,
            'recall': 0.30
        },
        'num_images': 1000
    }
    
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: 1000ê°œ ì´ë¯¸ì§€, 1ê°œ í´ë˜ìŠ¤")
    
    # í‰ê°€ ìˆ˜í–‰
    metrics = evaluator.evaluate_yolo_results(dummy_results)
    
    print(f"\nğŸ“ˆ í‰ê°€ ê²°ê³¼:")
    print(f"  mAP@0.5: {metrics.map_50:.3f}")
    print(f"  mAP@0.5:0.95: {metrics.map_50_95:.3f}")
    print(f"  ì •ë°€ë„: {metrics.precision:.3f}")
    print(f"  ì¬í˜„ìœ¨: {metrics.recall:.3f}")
    print(f"  F1 ì ìˆ˜: {metrics.f1_score:.3f}")
    
    # Stage 1 ëª©í‘œ ë‹¬ì„± í‰ê°€
    achievement = evaluator.evaluate_stage_target_achievement(metrics, stage=1)
    stage1_completed = achievement.get('stage1_detection_completed', False)
    
    print(f"\nğŸ¯ Stage 1 ëª©í‘œ ë‹¬ì„±: {'âœ… ì„±ê³µ' if stage1_completed else 'âŒ ë¯¸ë‹¬ì„±'}")
    
    # ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
    detailed_report = evaluator.generate_detection_report(metrics)
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    report_file = evaluator.save_evaluation_report(metrics, detailed_report, stage=1)
    if report_file:
        print(f"\nğŸ’¾ í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
    
    print("\nâœ… ê²€ì¶œ ë©”íŠ¸ë¦­ í‰ê°€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    main()